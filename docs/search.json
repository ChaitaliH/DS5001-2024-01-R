[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploratory Text Analytics",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "lessons/M01_Intro/M01_01_getting-started.html",
    "href": "lessons/M01_Intro/M01_01_getting-started.html",
    "title": "Setting Up Your Jupyter Environmnet",
    "section": "",
    "text": "Course:  DS 5001 Exploratory Text Analytics\nModule:  01 Getting Started\nTopic:   HW Setting Up Your Jupyter Environmnet\nAuthor:  R.C. Alvarado\nDate:    14 October 2023 (revised)\nPurpose: Intrdoce students to using Jupyter notebooks.\n\nExercise 1: Download and Install Anaconda\nIf you have not done so already …\n\nGo to the Anaconda download page, find the distribution for your operating system for Anaconda Individual Editions.\nDownload the file and install it.\nFind the application icon in your toolbar or program manager and open it up.\n\n\n\nExercise 2: Install Jupyter Notebook and/or Lab\nAgain, if you have not done so already …\n\nIn Anaconda Navigator, locate the cards for Jupyter Notebook and/or Lab (depending on your preference).\nClick on the Install button for each card. You may also want to install Spyder and Jupyter Notebook.\nOnce installed, you may click the Launch button to run it.\n\n\n\nExercise 3: Create a directory for this course\n\nNote: You can create directories in any way you are comfortable — from the command line, from a file manager (e.g. Finder in MacOS) window, or within Jupyter Lab.\nWithin the directory where you normally store the content you create (e.g. papers, code, etc.), create a folder for this course. Call it DS5001.\nWithin this directory, create another directory called labs and move into it.\nCreate a directory for today’s lab, naming after the date, e.g. 2021-02-04, and move into it.\n\n\n\nExercise 4: Hello, World!\n\nStart Jupyter Lab or Notebook, if you have not already done so. You can do this by clicking on the card in Anacodna, or by using the command line; for example > jupyter-lab or > jupyter-notebook, where > is the command prompt (it will likely look different on your system).\nReview the interface (see Instructor).\nCreate a new notebook; called it hello.ipynb.\nEnter the following text in a code block:\n\n        Hello, World!\n\nRun the block by clicking on the run icon or entering SHIFT+ENTER.\nFollow the instructor for other instructions.\n\n\n\nExercise 5: Import a text into Pandas\n\nDownload this file external link: pg105.txt into your working directory.\nCreate a new notebook called M01_03_first-foray.ipynb.\nFollow the instructor."
  },
  {
    "objectID": "lessons/M01_Intro/M01_02_hello.html",
    "href": "lessons/M01_Intro/M01_02_hello.html",
    "title": "Lab: Hello, World!",
    "section": "",
    "text": "Start Here"
  },
  {
    "objectID": "lessons/M01_Intro/M01_02_hello.html#values-and-variables",
    "href": "lessons/M01_Intro/M01_02_hello.html#values-and-variables",
    "title": "Lab: Hello, World!",
    "section": "Values and Variables",
    "text": "Values and Variables\n\n\"Hello, World!\"\n\n'Hello, World!'\n\n\n\nx = \"Hello, World!\"\n\n\nx\n\n'Hello, World!'"
  },
  {
    "objectID": "lessons/M01_Intro/M01_02_hello.html#print",
    "href": "lessons/M01_Intro/M01_02_hello.html#print",
    "title": "Lab: Hello, World!",
    "section": "print()",
    "text": "print()\n\nprint(x)\n\nHello, World!\n\n\n\nprint(\"Hello Raf!\")\n\nHello Raf!\n\n\n\nprint(\"Hello Quetzil!\")\n\nHello Quetzil!"
  },
  {
    "objectID": "lessons/M01_Intro/M01_02_hello.html#functions",
    "href": "lessons/M01_Intro/M01_02_hello.html#functions",
    "title": "Lab: Hello, World!",
    "section": "Functions",
    "text": "Functions\n\ndef hello(fname=''):\n    print(\"Hello {}!\".format(fname))\n\n\nhello(\"Raf\")\n\nHello Raf!\n\n\n\nhello(\"Quetzil!\")\n\nHello Quetzil!!"
  },
  {
    "objectID": "lessons/M01_Intro/M01_03_first-foray.html",
    "href": "lessons/M01_Intro/M01_03_first-foray.html",
    "title": "Lab: First Foray",
    "section": "",
    "text": "Set Up\nWhy two humps? What might this bimodal distribution indicate?\nLet’s look at the first hump for characters.\nNow that we know what line breaks mean, we can use this information to import the file with a more accurate structure. Note also that we could have inferred this from visual inspection, too. But the principle that statistical features can provide evidence for structure remains – we will use this throughout the course.\nK: A dataframe of tokens.\nNote the expand argument to the .split() method.\nBroken down into steps\nV: A table of terms. As opposed to tokens, which are term instances.\nTerms are symbol types.\nTokens are symbol instances.\nDefine relative frequency, an estimate of the probability of the word.\nWhy is “the” the most frequent word?\nConsider that “the” is “The Most Powerful Word in the English Language.”\nNote: function vs. meaning …"
  },
  {
    "objectID": "lessons/M01_Intro/M01_03_first-foray.html#interpret-line-breaks-nn",
    "href": "lessons/M01_Intro/M01_03_first-foray.html#interpret-line-breaks-nn",
    "title": "Lab: First Foray",
    "section": "Interpret line breaks \\n\\n",
    "text": "Interpret line breaks \\n\\n\n\nchunk_pat = '\\n\\n'\n\n\nchunks = open('pg105.txt', 'r').read().split(chunk_pat)\n\n\nchunks[:5]\n\n['Persuasion by Jane Austen (1818)',\n '',\n 'Chapter 1',\n 'Sir Walter Elliot, of Kellynch Hall, in Somersetshire, was a man who,\\nfor his own amusement, never took up any book but the Baronetage; there\\nhe found occupation for an idle hour, and consolation in a distressed\\none; there his faculties were roused into admiration and respect, by\\ncontemplating the limited remnant of the earliest patents; there any\\nunwelcome sensations, arising from domestic affairs changed naturally\\ninto pity and contempt as he turned over the almost endless creations\\nof the last century; and there, if every other leaf were powerless, he\\ncould read his own history with an interest which never failed.  This\\nwas the page at which the favourite volume always opened:',\n '           \"ELLIOT OF KELLYNCH HALL.']\n\n\n\ntext = pd.DataFrame(chunks, columns=['chunk_str'])\ntext.index.name = 'chunk_id'\n\n\ntext.head()\n\n\n\n\n\n  \n    \n      \n      chunk_str\n    \n    \n      chunk_id\n      \n    \n  \n  \n    \n      0\n      Persuasion by Jane Austen (1818)\n    \n    \n      1\n      \n    \n    \n      2\n      Chapter 1\n    \n    \n      3\n      Sir Walter Elliot, of Kellynch Hall, in Somers...\n    \n    \n      4\n      \"ELLIOT OF KELLYNCH HALL.\n    \n  \n\n\n\n\n\ntext.shape\n\n(1056, 1)"
  },
  {
    "objectID": "lessons/M01_Intro/M01_03_first-foray.html#remove-remaining-breaks",
    "href": "lessons/M01_Intro/M01_03_first-foray.html#remove-remaining-breaks",
    "title": "Lab: First Foray",
    "section": "Remove remaining breaks",
    "text": "Remove remaining breaks\n\ntext.chunk_str = text.chunk_str.str.replace('\\n+', ' ', regex=True).str.strip()\n\n\ntext.head()\n\n\n\n\n\n  \n    \n      \n      chunk_str\n    \n    \n      chunk_id\n      \n    \n  \n  \n    \n      0\n      Persuasion by Jane Austen (1818)\n    \n    \n      1\n      \n    \n    \n      2\n      Chapter 1\n    \n    \n      3\n      Sir Walter Elliot, of Kellynch Hall, in Somers...\n    \n    \n      4\n      \"ELLIOT OF KELLYNCH HALL."
  },
  {
    "objectID": "lessons/M01_Intro/M01_04_FurtherExploration.html",
    "href": "lessons/M01_Intro/M01_04_FurtherExploration.html",
    "title": "Lab: Further Exploration",
    "section": "",
    "text": "Course:   DS 5001 Exploratory Text Analytics\nModule:   01 Getting Started\nTopic:    Lab: Further Exploration\nAuthor:   R.C. Alvarado\nDate:     17 October 2022 (revised)\nPurpose: We continue exploring the Gutenberg text, looking at simple word dispersion plots and correlations.\n\nSet Up\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n\nsns.set()\n\n\nimport configparser\nconfig = configparser.ConfigParser()\n\n\nconfig.read(\"../env.ini\")\ndata_home = config['DEFAULT']['data_home']\noutput_dir = config['DEFAULT']['output_dir']\n\n\n\nRetrieve Work\n\nK = pd.read_csv(f\"{output_dir}/M01-ff-TOKENS.csv\")[['term_str']]\nV = pd.read_csv(f\"{output_dir}/M01-ff-VOCAB.csv\").set_index('term_str')\n\n\n\nWord Dispersion Plots\nWe create a simple dispersion plot to show the distribution of words over narrative time.\n\nK.head()\n\n\n\n\n\n  \n    \n      \n      term_str\n    \n  \n  \n    \n      0\n      persuasion\n    \n    \n      1\n      by\n    \n    \n      2\n      jane\n    \n    \n      3\n      austen\n    \n    \n      4\n      1818\n    \n  \n\n\n\n\n\nD = pd.get_dummies(K.term_str, dtype='int')\n\n\nD.sum().sort_values() # Another way to create the vocabulary\n\nkicking           1\nmaintenance       1\nmale              1\nmanage            1\nmanagement        1\n               ... \na              1591\nof             2565\nand            2781\nto             2782\nthe            3326\nLength: 6004, dtype: int64\n\n\n\nD.sum(axis=1).sort_values() # Each is 1 because one-hot encoding\n\n47614    0\n0        1\n55525    1\n55524    1\n55523    1\n        ..\n27756    1\n27755    1\n27754    1\n27772    1\n83282    1\nLength: 83283, dtype: int64\n\n\n\nD.T.sample(5)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      ...\n      83273\n      83274\n      83275\n      83276\n      83277\n      83278\n      83279\n      83280\n      83281\n      83282\n    \n  \n  \n    \n      plaguing\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      shrewd\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      irremediable\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      parcel\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      outwardly\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n5 rows × 83283 columns\n\n\n\n\nD['anne'].plot(figsize=(10, .5));\n\n\n\n\n\ndef word_plot(term_str):\n    term_str = term_str.lower()\n    D[term_str].plot(figsize=(10, .5), legend=False, title=term_str.upper());\n\n\nword_plot('the')\n\n\n\n\n\nword_plot('joy')\n\n\n\n\n\nword_plot('anne')\n\n\n\n\n\nword_plot('walter')\n\n\n\n\n\nword_plot('wentworth')\n\n\n\n\n\n\nCorrelation Graphs\nWe chunk narrative time into 100 segments of equal length (centiles).\nThen we count how many times each word appears in each centile.\nThen we compare plots for pairs of words.\n\nK['centile'] = (K.index * 100 / len(K)).astype('int') + 1\n\n\n# K.centile.value_counts().value_counts() # View chunk sizes\n\n\nM = [None, None]\nM[0] = K.groupby(['centile','term_str']).term_str.count().unstack(fill_value=0)\nM[1] = M[0] / M[0].sum()\n\n\nM[1].T.sample(10)\n\n\n\n\n\n  \n    \n      centile\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      ...\n      91\n      92\n      93\n      94\n      95\n      96\n      97\n      98\n      99\n      100\n    \n    \n      term_str\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      august\n      0.5\n      0.0\n      0.0\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      pleasanter\n      0.0\n      0.0\n      0.0\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      free\n      0.0\n      0.0\n      0.0\n      0.000000\n      0.000000\n      0.2\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.2\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      thorough\n      0.0\n      0.0\n      0.0\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.5\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      views\n      0.0\n      0.0\n      0.0\n      0.111111\n      0.111111\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      lamenting\n      0.0\n      0.0\n      0.0\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.5\n      0.0\n      0.0\n      0.0\n    \n    \n      sad\n      0.0\n      0.0\n      0.0\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      moral\n      0.0\n      0.0\n      0.0\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      carteretthey\n      0.0\n      0.0\n      0.0\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n    \n    \n      dreamt\n      0.0\n      0.0\n      0.0\n      0.000000\n      0.000000\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n10 rows × 100 columns\n\n\n\n\nM[0][['wentworth','anne']].plot(figsize=(20,5), style='-');\n\n\n\n\n\nM[0][['wentworth','anne']].rolling(10).mean().plot(figsize=(20,5));\n\n\n\n\n\nM[1][['wentworth','anne']].rolling(10).mean().plot(figsize=(20,5));\n\n\n\n\n\ndef plot_words(words, rolling=True, w=10, norm=1):\n    config = {\n        'figsize': (20, 5),\n        'title': f\"{', '.join(words).upper()} (norm={norm})\"\n    }\n    if rolling:\n        M[norm][words].rolling(w).mean().plot(**config);\n    else:\n        M[norm][words].plot(**config);\n\n\nplot_words(['wentworth','walter'], norm=0)\n\n\n\n\n\nplot_words(['wentworth','walter'])\n\n\n\n\n\nplot_words(['wentworth','anne'])\n\n\n\n\n\nplot_words(['anne','walter'])\n\n\n\n\n\nplot_words(['wentworth', 'anne', 'walter'])\n\n\n\n\n\nq2 = lambda a, b: M[1][[a,b]].corr(method='kendall').iloc[0,1]\n\n\nM[1].plot.scatter('anne','mary', title=q2('anne','mary'))\n\n<Axes: title={'center': '0.09791689739872034'}, xlabel='anne', ylabel='mary'>\n\n\n\n\n\n\nM[1].plot.scatter('anne','wentworth', title=q2('anne','wentworth'));\n\n\n\n\n\nM[1].plot.scatter('anne','walter', title=q2('anne','walter'));\n\n\n\n\n\nM[1].plot.scatter('wentworth','walter', title=q2('wentworth','walter'))\n\n<Axes: title={'center': '-0.24755218005009688'}, xlabel='wentworth', ylabel='walter'>\n\n\n\n\n\nCreate a matrix …\n\n# stopwords = [word.strip() for word in open(f\"{data_home}/misc/stop_words_english.txt\", 'r').readlines()]\n\n# print(stopwords[:10])\n\n# V2 = [word for word in M[1].columns if word not in stopwords]\n\n# len(V2)\n\n# Q = M[1][V2]\\\n#     .corr(method='kendall')\\\n#     .stack()\\\n#     .sort_values()\\\n#     .to_frame('c')\n# Q.index.names = ['w0', 'w1']\n# Q = Q.query('w0 != w1')\n\n# def corr_words(word, n=10):\n#     C = Q.loc[word].c\n#     pd.concat([C.head(n), C.tail(n)]).plot.barh(title=word.upper(), figsize=(5, n))\n#     plot_words([word, C.idxmax(), C.idxmin()])\n\n# walter = corr_words('walter')\n\n# anne = corr_words('anne')\n\n# wentworth = corr_words('wentworth')"
  },
  {
    "objectID": "lessons/M02_TextModels/M02_01_Importing-Persuasion.html",
    "href": "lessons/M02_TextModels/M02_01_Importing-Persuasion.html",
    "title": "Text into Data: Importing a Text, or, Clip, Chunk, and Split",
    "section": "",
    "text": "Set Up\nWe use Pandas’ convenient .split() method with expand=True, followed by .stack(). Note that this creates zero-based indexes.\nThis is important – will be used for homework."
  },
  {
    "objectID": "lessons/M02_TextModels/M02_01_Importing-Persuasion.html#import-config",
    "href": "lessons/M02_TextModels/M02_01_Importing-Persuasion.html#import-config",
    "title": "Text into Data: Importing a Text, or, Clip, Chunk, and Split",
    "section": "Import Config",
    "text": "Import Config\n\nimport configparser\nconfig = configparser.ConfigParser()\n\n\nconfig.read(\"../env.ini\")\ndata_home = config['DEFAULT']['data_home']\noutput_dir = config['DEFAULT']['output_dir']\n\n\ndata_home, output_dir\n\n('/Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001_2024_01_R/data',\n '/Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001_2024_01_R/output')\n\n\n\ntext_file = f\"{data_home}/gutenberg/pg105.txt\"\ncsv_file  = f\"{output_dir}/austen-persuasion.csv\" # The file we will create\n\n\nOHCO = ['chap_num', 'para_num', 'sent_num', 'token_num']"
  },
  {
    "objectID": "lessons/M02_TextModels/M02_01_Importing-Persuasion.html#find-all-chapter-headers",
    "href": "lessons/M02_TextModels/M02_01_Importing-Persuasion.html#find-all-chapter-headers",
    "title": "Text into Data: Importing a Text, or, Clip, Chunk, and Split",
    "section": "Find all chapter headers",
    "text": "Find all chapter headers\nThe regex will depend on the source text. You need to investigate the source text to figure this out.\n\nchap_pat = r\"^\\s*(?:chapter|letter)\\s+\\d+\"\n\n\nchap_lines = LINES.line_str.str.match(chap_pat, case=False) # Returns a truth vector\n\n\nLINES.loc[chap_lines] # Use as filter for dataframe\n\n\n\n\n\n  \n    \n      \n      line_str\n    \n    \n      line_num\n      \n    \n  \n  \n    \n      47\n      Chapter 1\n    \n    \n      306\n      Chapter 2\n    \n    \n      500\n      Chapter 3\n    \n    \n      786\n      Chapter 4\n    \n    \n      959\n      Chapter 5\n    \n    \n      1297\n      Chapter 6\n    \n    \n      1657\n      Chapter 7\n    \n    \n      1992\n      Chapter 8\n    \n    \n      2346\n      Chapter 9\n    \n    \n      2632\n      Chapter 10\n    \n    \n      3020\n      Chapter 11\n    \n    \n      3314\n      Chapter 12\n    \n    \n      3876\n      Chapter 13\n    \n    \n      4150\n      Chapter 14\n    \n    \n      4409\n      Chapter 15\n    \n    \n      4677\n      Chapter 16\n    \n    \n      4912\n      Chapter 17\n    \n    \n      5248\n      Chapter 18\n    \n    \n      5654\n      Chapter 19\n    \n    \n      5905\n      Chapter 20\n    \n    \n      6265\n      Chapter 21\n    \n    \n      6970\n      Chapter 22\n    \n    \n      7555\n      Chapter 23\n    \n    \n      8208\n      Chapter 24"
  },
  {
    "objectID": "lessons/M02_TextModels/M02_01_Importing-Persuasion.html#assign-numbers-to-chapters",
    "href": "lessons/M02_TextModels/M02_01_Importing-Persuasion.html#assign-numbers-to-chapters",
    "title": "Text into Data: Importing a Text, or, Clip, Chunk, and Split",
    "section": "Assign numbers to chapters",
    "text": "Assign numbers to chapters\n\nLINES.loc[chap_lines, 'chap_num'] = [i+1 for i in range(LINES.loc[chap_lines].shape[0])]\n\n\nLINES.loc[chap_lines]\n\n\n\n\n\n  \n    \n      \n      line_str\n      chap_num\n    \n    \n      line_num\n      \n      \n    \n  \n  \n    \n      47\n      Chapter 1\n      1.0\n    \n    \n      306\n      Chapter 2\n      2.0\n    \n    \n      500\n      Chapter 3\n      3.0\n    \n    \n      786\n      Chapter 4\n      4.0\n    \n    \n      959\n      Chapter 5\n      5.0\n    \n    \n      1297\n      Chapter 6\n      6.0\n    \n    \n      1657\n      Chapter 7\n      7.0\n    \n    \n      1992\n      Chapter 8\n      8.0\n    \n    \n      2346\n      Chapter 9\n      9.0\n    \n    \n      2632\n      Chapter 10\n      10.0\n    \n    \n      3020\n      Chapter 11\n      11.0\n    \n    \n      3314\n      Chapter 12\n      12.0\n    \n    \n      3876\n      Chapter 13\n      13.0\n    \n    \n      4150\n      Chapter 14\n      14.0\n    \n    \n      4409\n      Chapter 15\n      15.0\n    \n    \n      4677\n      Chapter 16\n      16.0\n    \n    \n      4912\n      Chapter 17\n      17.0\n    \n    \n      5248\n      Chapter 18\n      18.0\n    \n    \n      5654\n      Chapter 19\n      19.0\n    \n    \n      5905\n      Chapter 20\n      20.0\n    \n    \n      6265\n      Chapter 21\n      21.0\n    \n    \n      6970\n      Chapter 22\n      22.0\n    \n    \n      7555\n      Chapter 23\n      23.0\n    \n    \n      8208\n      Chapter 24\n      24.0\n    \n  \n\n\n\n\nNotice that all lines that are not chapter headers have no chapter number assigned to them.\n\nLINES.sample(10)\n\n\n\n\n\n  \n    \n      \n      line_str\n      chap_num\n    \n    \n      line_num\n      \n      \n    \n  \n  \n    \n      3553\n      but as they drew near the Cobb, there was such...\n      NaN\n    \n    \n      3698\n      It now became necessary for the party to consi...\n      NaN\n    \n    \n      7399\n      Wentworth was all attention, looking and liste...\n      NaN\n    \n    \n      6092\n      of Colonel Wallis's gallantry, was quite conte...\n      NaN\n    \n    \n      7160\n      long before the term in question, the two fami...\n      NaN\n    \n    \n      140\n      Musgrove; but Anne, with an elegance of mind a...\n      NaN\n    \n    \n      5270\n      introduction.  I should have visited Admiral C...\n      NaN\n    \n    \n      3802\n      it might, perhaps, be the occasion of continui...\n      NaN\n    \n    \n      6433\n      safe in his character.  He will not be led ast...\n      NaN\n    \n    \n      8360\n      \n      NaN"
  },
  {
    "objectID": "lessons/M02_TextModels/M02_01_Importing-Persuasion.html#forward-fill-chapter-numbers-to-following-text-lines",
    "href": "lessons/M02_TextModels/M02_01_Importing-Persuasion.html#forward-fill-chapter-numbers-to-following-text-lines",
    "title": "Text into Data: Importing a Text, or, Clip, Chunk, and Split",
    "section": "Forward-fill chapter numbers to following text lines",
    "text": "Forward-fill chapter numbers to following text lines\nffill() will replace null values with the previous non-null value.\n\nLINES.chap_num = LINES.chap_num.ffill()\n\n\nLINES.sample(10)\n\n\n\n\n\n  \n    \n      \n      line_str\n      chap_num\n    \n    \n      line_num\n      \n      \n    \n  \n  \n    \n      6865\n      \n      21.0\n    \n    \n      34\n      \n      NaN\n    \n    \n      7063\n      more solicitation; but the charm was broken: h...\n      22.0\n    \n    \n      1864\n      Charles Musgrove's way, on account of the chil...\n      7.0\n    \n    \n      662\n      who knew it only by description could feel; an...\n      3.0\n    \n    \n      1168\n      \"Yes, as long as I could bear their noise; but...\n      5.0\n    \n    \n      364\n      feelings of the gentleman, and the head of a h...\n      2.0\n    \n    \n      7887\n      distracting, and in desperation, she said she ...\n      23.0\n    \n    \n      2082\n      \"To be sure you did.  What should a young fell...\n      8.0\n    \n    \n      7338\n      hands.  He is turning away.  Not know Mr Ellio...\n      22.0\n    \n  \n\n\n\n\nNotice that the lines taht precede our first chapter have no chapters, which is what we want. We need to decide whether to keep these lines as textual front matter or to dispose of them.\n\nLINES.head(20)\n\n\n\n\n\n  \n    \n      \n      line_str\n      chap_num\n    \n    \n      line_num\n      \n      \n    \n  \n  \n    \n      19\n      \n      NaN\n    \n    \n      20\n      \n      NaN\n    \n    \n      21\n      \n      NaN\n    \n    \n      22\n      \n      NaN\n    \n    \n      23\n      Produced by Sharon Partridge and Martin Ward. ...\n      NaN\n    \n    \n      24\n      by Al Haines.\n      NaN\n    \n    \n      25\n      \n      NaN\n    \n    \n      26\n      \n      NaN\n    \n    \n      27\n      \n      NaN\n    \n    \n      28\n      \n      NaN\n    \n    \n      29\n      \n      NaN\n    \n    \n      30\n      \n      NaN\n    \n    \n      31\n      \n      NaN\n    \n    \n      32\n      \n      NaN\n    \n    \n      33\n      \n      NaN\n    \n    \n      34\n      \n      NaN\n    \n    \n      35\n      Persuasion\n      NaN\n    \n    \n      36\n      \n      NaN\n    \n    \n      37\n      \n      NaN\n    \n    \n      38\n      by\n      NaN"
  },
  {
    "objectID": "lessons/M02_TextModels/M02_01_Importing-Persuasion.html#clean-up",
    "href": "lessons/M02_TextModels/M02_01_Importing-Persuasion.html#clean-up",
    "title": "Text into Data: Importing a Text, or, Clip, Chunk, and Split",
    "section": "Clean up",
    "text": "Clean up\n\nLINES = LINES.dropna(subset=['chap_num']) # Remove everything before Chapter 1\n# LINES = LINES.loc[~LINES.chap_num.isna()] # Remove everything before Chapter 1 (alternate method)\nLINES = LINES.loc[~chap_lines] # Remove chapter heading lines; their work is done\nLINES.chap_num = LINES.chap_num.astype('int') # Convert chap_num from float to int\n\n\nLINES.sample(10)\n\n\n\n\n\n  \n    \n      \n      line_str\n      chap_num\n    \n    \n      line_num\n      \n      \n    \n  \n  \n    \n      3640\n      \n      12\n    \n    \n      650\n      a hint of the Admiral from a London correspond...\n      3\n    \n    \n      8052\n      exist; and he went, therefore, to his brother'...\n      23\n    \n    \n      3093\n      very evident they would not have more than tim...\n      11\n    \n    \n      4753\n      surprise her, therefore, that Lady Russell sho...\n      16\n    \n    \n      5610\n      Captain Wentworth's letter to make you and Mrs...\n      18\n    \n    \n      4989\n      languor and depression, to hours of occupation...\n      17\n    \n    \n      2638\n      where she knew it would have satisfied neither...\n      10\n    \n    \n      7956\n      unpacked to the last possible moment, that I m...\n      23\n    \n    \n      2209\n      Admiral, in rating the claims of women to ever...\n      8"
  },
  {
    "objectID": "lessons/M02_TextModels/M02_01_Importing-Persuasion.html#group-lines-into-chapters",
    "href": "lessons/M02_TextModels/M02_01_Importing-Persuasion.html#group-lines-into-chapters",
    "title": "Text into Data: Importing a Text, or, Clip, Chunk, and Split",
    "section": "Group lines into chapters",
    "text": "Group lines into chapters\n\nOHCO[:1]\n\n['chap_num']\n\n\n\n# Make big string for each chapter\nCHAPS = LINES.groupby(OHCO[:1])\\\n    .line_str.apply(lambda x: '\\n'.join(x))\\\n    .to_frame('chap_str')\n\n\nCHAPS.head(10)\n\n\n\n\n\n  \n    \n      \n      chap_str\n    \n    \n      chap_num\n      \n    \n  \n  \n    \n      1\n      \\n\\nSir Walter Elliot, of Kellynch Hall, in So...\n    \n    \n      2\n      \\n\\nMr Shepherd, a civil, cautious lawyer, who...\n    \n    \n      3\n      \\n\\n\"I must take leave to observe, Sir Walter,...\n    \n    \n      4\n      \\n\\nHe was not Mr Wentworth, the former curate...\n    \n    \n      5\n      \\n\\nOn the morning appointed for Admiral and M...\n    \n    \n      6\n      \\n\\nAnne had not wanted this visit to Uppercro...\n    \n    \n      7\n      \\n\\nA very few days more, and Captain Wentwort...\n    \n    \n      8\n      \\n\\nFrom this time Captain Wentworth and Anne ...\n    \n    \n      9\n      \\n\\nCaptain Wentworth was come to Kellynch as ...\n    \n    \n      10\n      \\n\\nOther opportunities of making her observat...\n    \n  \n\n\n\n\n\nCHAPS['chap_str'] = CHAPS.chap_str.str.strip()\n\n\nCHAPS\n\n\n\n\n\n  \n    \n      \n      chap_str\n    \n    \n      chap_num\n      \n    \n  \n  \n    \n      1\n      Sir Walter Elliot, of Kellynch Hall, in Somers...\n    \n    \n      2\n      Mr Shepherd, a civil, cautious lawyer, who, wh...\n    \n    \n      3\n      \"I must take leave to observe, Sir Walter,\" sa...\n    \n    \n      4\n      He was not Mr Wentworth, the former curate of ...\n    \n    \n      5\n      On the morning appointed for Admiral and Mrs C...\n    \n    \n      6\n      Anne had not wanted this visit to Uppercross, ...\n    \n    \n      7\n      A very few days more, and Captain Wentworth wa...\n    \n    \n      8\n      From this time Captain Wentworth and Anne Elli...\n    \n    \n      9\n      Captain Wentworth was come to Kellynch as to a...\n    \n    \n      10\n      Other opportunities of making her observations...\n    \n    \n      11\n      The time now approached for Lady Russell's ret...\n    \n    \n      12\n      Anne and Henrietta, finding themselves the ear...\n    \n    \n      13\n      The remainder of Anne's time at Uppercross, co...\n    \n    \n      14\n      Though Charles and Mary had remained at Lyme m...\n    \n    \n      15\n      Sir Walter had taken a very good house in Camd...\n    \n    \n      16\n      There was one point which Anne, on returning t...\n    \n    \n      17\n      While Sir Walter and Elizabeth were assiduousl...\n    \n    \n      18\n      It was the beginning of February; and Anne, ha...\n    \n    \n      19\n      While Admiral Croft was taking this walk with ...\n    \n    \n      20\n      Sir Walter, his two daughters, and Mrs Clay, w...\n    \n    \n      21\n      Anne recollected with pleasure the next mornin...\n    \n    \n      22\n      Anne went home to think over all that she had ...\n    \n    \n      23\n      One day only had passed since Anne's conversat...\n    \n    \n      24\n      Who can be in doubt of what followed?  When an...\n    \n  \n\n\n\n\nSo, now we have our text grouped by chapters."
  },
  {
    "objectID": "lessons/M02_TextModels/M02_02_TokenizingWithSciKitLearn.html",
    "href": "lessons/M02_TextModels/M02_02_TokenizingWithSciKitLearn.html",
    "title": "Tokenizing with SciKit Learn",
    "section": "",
    "text": "Vectorize Corpus\nWe use SciKit Learn’s CountVectorizer() method, which as the following signature:\nThis the doctring from the function:\nBut, it’s better to get the list and the counts to create a vocabulary table."
  },
  {
    "objectID": "lessons/M02_TextModels/M02_02_TokenizingWithSciKitLearn.html#set-up",
    "href": "lessons/M02_TextModels/M02_02_TokenizingWithSciKitLearn.html#set-up",
    "title": "Tokenizing with SciKit Learn",
    "section": "Set Up",
    "text": "Set Up\n\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport seaborn as sns\n\n\nsns.set()"
  },
  {
    "objectID": "lessons/M02_TextModels/M02_02_TokenizingWithSciKitLearn.html#import-config",
    "href": "lessons/M02_TextModels/M02_02_TokenizingWithSciKitLearn.html#import-config",
    "title": "Tokenizing with SciKit Learn",
    "section": "Import Config",
    "text": "Import Config\n\nimport configparser\nconfig = configparser.ConfigParser()\n\n\nconfig.read(\"../env.ini\")\ndata_home = config['DEFAULT']['data_home']\nmodel_dir = config['DEFAULT']['model_dir']\n\n\ndata_home, model_dir\n\n('/Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001_2024_01_R/data',\n '/Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001_2024_01_R/models')\n\n\n\ntext_file = f\"{data_home}/gutenberg/pg105.txt\""
  },
  {
    "objectID": "lessons/M02_TextModels/M02_02_TokenizingWithSciKitLearn.html#import-text",
    "href": "lessons/M02_TextModels/M02_02_TokenizingWithSciKitLearn.html#import-text",
    "title": "Tokenizing with SciKit Learn",
    "section": "Import text",
    "text": "Import text\nImport source file\n\ndocs = pd.DataFrame(dict(doc_str=open(text_file, 'r').read().split('\\n\\n')))\n\nDefine lines where text begins and ends\n\na = docs[docs.doc_str.str.contains(\"1818\")].index.values[0]\nb = docs[docs.doc_str.str.contains(\"Finis\")].index.values[0]\n\nSelect only relevant text\n\ndocs = docs[(docs.index > a) & (docs.index < b)]\n\nClean up strings\n\ndocs.doc_str = docs.doc_str.str.replace(r\"\\n+\", \" \", regex=True)\n\nRemove blank lines\n\ndocs = docs[docs.doc_str != '']\n\nCreate index\n\ndocs = docs.reset_index(drop=True)\ndocs.index.name = 'doc_id' \n\nInspect results.\nNote that the “docs” are paragraphs.\n\ndocs\n\n\n\n\n\n  \n    \n      \n      doc_str\n    \n    \n      doc_id\n      \n    \n  \n  \n    \n      0\n      Chapter 1\n    \n    \n      1\n      Sir Walter Elliot, of Kellynch Hall, in Somer...\n    \n    \n      2\n      \"ELLIOT OF KELLYNCH HALL.\n    \n    \n      3\n      \"Walter Elliot, born March 1, 1760, married, J...\n    \n    \n      4\n      Precisely such had the paragraph originally st...\n    \n    \n      ...\n      ...\n    \n    \n      1025\n      Mrs Clay's affections had overpowered her inte...\n    \n    \n      1026\n      It cannot be doubted that Sir Walter and Eliza...\n    \n    \n      1027\n      Anne, satisfied at a very early period of Lady...\n    \n    \n      1028\n      Her recent good offices by Anne had been enoug...\n    \n    \n      1029\n      Mrs Smith's enjoyments were not spoiled by thi...\n    \n  \n\n1030 rows × 1 columns"
  },
  {
    "objectID": "lessons/M02_TextModels/M02_02_TokenizingWithSciKitLearn.html#extract-count-matrix",
    "href": "lessons/M02_TextModels/M02_02_TokenizingWithSciKitLearn.html#extract-count-matrix",
    "title": "Tokenizing with SciKit Learn",
    "section": "Extract Count Matrix",
    "text": "Extract Count Matrix\nThe engine provides a list of terms …\n\nV = engine.get_feature_names_out()\n\n\nX = pd.DataFrame(model.toarray(), columns=V)\nX.index.name = 'doc_id'\n\n\nX.shape\n\n(1030, 5751)\n\n\n\nX\n\n\n\n\n\n  \n    \n      \n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n      1760\n      1784\n      ...\n      younker\n      your\n      yours\n      yourself\n      yourselves\n      youth\n      youthful\n      zeal\n      zealous\n      zealously\n    \n    \n      doc_id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n      1\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1025\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1026\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1027\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1028\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1029\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n1030 rows × 5751 columns"
  },
  {
    "objectID": "lessons/M02_TextModels/M02_02_TokenizingWithSciKitLearn.html#show-stats",
    "href": "lessons/M02_TextModels/M02_02_TokenizingWithSciKitLearn.html#show-stats",
    "title": "Tokenizing with SciKit Learn",
    "section": "Show Stats",
    "text": "Show Stats\n\nV.sort_values('n').tail(20).plot.barh(figsize=(10,10));"
  },
  {
    "objectID": "lessons/M02_TextModels/M02_03_Challenge.html",
    "href": "lessons/M02_TextModels/M02_03_Challenge.html",
    "title": "Challenge: Convert Another Text",
    "section": "",
    "text": "Set Up\nWe use Pandas’ convenient .split() method with expand=True, followed by .stack(). Note that this creates zero-based indexes.\nThis is important – will be used for homework."
  },
  {
    "objectID": "lessons/M02_TextModels/M02_03_Challenge.html#import-config",
    "href": "lessons/M02_TextModels/M02_03_Challenge.html#import-config",
    "title": "Challenge: Convert Another Text",
    "section": "Import Config",
    "text": "Import Config\n\nimport configparser\nconfig = configparser.ConfigParser()\n\n\nconfig.read(\"../env.ini\")\ndata_home = config['DEFAULT']['data_home']\noutput_dir = config['DEFAULT']['output_dir']\n\n\ntext_file = f\"{data_home}/gutenberg/pg161.txt\"\ncsv_file = f\"{output_dir}/austen-sense-and-sensibility.csv\" # The file we will create\n\n\nOHCO = ['chap_num', 'para_num', 'sent_num', 'token_num']"
  },
  {
    "objectID": "lessons/M02_TextModels/M02_03_Challenge.html#find-all-chapter-headers",
    "href": "lessons/M02_TextModels/M02_03_Challenge.html#find-all-chapter-headers",
    "title": "Challenge: Convert Another Text",
    "section": "Find all chapter headers",
    "text": "Find all chapter headers\nThe regex will depend on the source text. You need to investigate the source text to figure this out.\n\nchap_pat = r\"^\\s*(?:chapter|letter)\\s+\\d+\"\n\n\nchap_lines = LINES.line_str.str.match(chap_pat, case=False) # Returns a truth vector\n\n\nLINES.loc[chap_lines] # Use as filter for dataframe\n\n\n\n\n\n  \n    \n      \n      line_str\n    \n    \n      line_num\n      \n    \n  \n  \n    \n      42\n      CHAPTER 1\n    \n    \n      196\n      CHAPTER 2\n    \n    \n      399\n      CHAPTER 3\n    \n    \n      561\n      CHAPTER 4\n    \n    \n      756\n      CHAPTER 5\n    \n    \n      858\n      CHAPTER 6\n    \n    \n      986\n      CHAPTER 7\n    \n    \n      1112\n      CHAPTER 8\n    \n    \n      1244\n      CHAPTER 9\n    \n    \n      1448\n      CHAPTER 10\n    \n    \n      1665\n      CHAPTER 11\n    \n    \n      1816\n      CHAPTER 12\n    \n    \n      1997\n      CHAPTER 13\n    \n    \n      2281\n      CHAPTER 14\n    \n    \n      2440\n      CHAPTER 15\n    \n    \n      2718\n      CHAPTER 16\n    \n    \n      2945\n      CHAPTER 17\n    \n    \n      3153\n      CHAPTER 18\n    \n    \n      3331\n      CHAPTER 19\n    \n    \n      3632\n      CHAPTER 20\n    \n    \n      3913\n      CHAPTER 21\n    \n    \n      4214\n      CHAPTER 22\n    \n    \n      4532\n      CHAPTER 23\n    \n    \n      4767\n      CHAPTER 24\n    \n    \n      5001\n      CHAPTER 25\n    \n    \n      5197\n      CHAPTER 26\n    \n    \n      5454\n      CHAPTER 27\n    \n    \n      5732\n      CHAPTER 28\n    \n    \n      5883\n      CHAPTER 29\n    \n    \n      6324\n      CHAPTER 30\n    \n    \n      6628\n      CHAPTER 31\n    \n    \n      7004\n      CHAPTER 32\n    \n    \n      7278\n      CHAPTER 33\n    \n    \n      7601\n      CHAPTER 34\n    \n    \n      7888\n      CHAPTER 35\n    \n    \n      8152\n      CHAPTER 36\n    \n    \n      8456\n      CHAPTER 37\n    \n    \n      8900\n      CHAPTER 38\n    \n    \n      9205\n      CHAPTER 39\n    \n    \n      9408\n      CHAPTER 40\n    \n    \n      9706\n      CHAPTER 41\n    \n    \n      9977\n      CHAPTER 42\n    \n    \n      10155\n      CHAPTER 43\n    \n    \n      10490\n      CHAPTER 44\n    \n    \n      11060\n      CHAPTER 45\n    \n    \n      11278\n      CHAPTER 46\n    \n    \n      11571\n      CHAPTER 47\n    \n    \n      11838\n      CHAPTER 48\n    \n    \n      11986\n      CHAPTER 49\n    \n    \n      12410\n      CHAPTER 50"
  },
  {
    "objectID": "lessons/M02_TextModels/M02_03_Challenge.html#assign-numbers-to-chapters",
    "href": "lessons/M02_TextModels/M02_03_Challenge.html#assign-numbers-to-chapters",
    "title": "Challenge: Convert Another Text",
    "section": "Assign numbers to chapters",
    "text": "Assign numbers to chapters\n\nLINES.loc[chap_lines, 'chap_num'] = [i+1 for i in range(LINES.loc[chap_lines].shape[0])]\n\n\nLINES.loc[chap_lines]\n\n\n\n\n\n  \n    \n      \n      line_str\n      chap_num\n    \n    \n      line_num\n      \n      \n    \n  \n  \n    \n      42\n      CHAPTER 1\n      1.0\n    \n    \n      196\n      CHAPTER 2\n      2.0\n    \n    \n      399\n      CHAPTER 3\n      3.0\n    \n    \n      561\n      CHAPTER 4\n      4.0\n    \n    \n      756\n      CHAPTER 5\n      5.0\n    \n    \n      858\n      CHAPTER 6\n      6.0\n    \n    \n      986\n      CHAPTER 7\n      7.0\n    \n    \n      1112\n      CHAPTER 8\n      8.0\n    \n    \n      1244\n      CHAPTER 9\n      9.0\n    \n    \n      1448\n      CHAPTER 10\n      10.0\n    \n    \n      1665\n      CHAPTER 11\n      11.0\n    \n    \n      1816\n      CHAPTER 12\n      12.0\n    \n    \n      1997\n      CHAPTER 13\n      13.0\n    \n    \n      2281\n      CHAPTER 14\n      14.0\n    \n    \n      2440\n      CHAPTER 15\n      15.0\n    \n    \n      2718\n      CHAPTER 16\n      16.0\n    \n    \n      2945\n      CHAPTER 17\n      17.0\n    \n    \n      3153\n      CHAPTER 18\n      18.0\n    \n    \n      3331\n      CHAPTER 19\n      19.0\n    \n    \n      3632\n      CHAPTER 20\n      20.0\n    \n    \n      3913\n      CHAPTER 21\n      21.0\n    \n    \n      4214\n      CHAPTER 22\n      22.0\n    \n    \n      4532\n      CHAPTER 23\n      23.0\n    \n    \n      4767\n      CHAPTER 24\n      24.0\n    \n    \n      5001\n      CHAPTER 25\n      25.0\n    \n    \n      5197\n      CHAPTER 26\n      26.0\n    \n    \n      5454\n      CHAPTER 27\n      27.0\n    \n    \n      5732\n      CHAPTER 28\n      28.0\n    \n    \n      5883\n      CHAPTER 29\n      29.0\n    \n    \n      6324\n      CHAPTER 30\n      30.0\n    \n    \n      6628\n      CHAPTER 31\n      31.0\n    \n    \n      7004\n      CHAPTER 32\n      32.0\n    \n    \n      7278\n      CHAPTER 33\n      33.0\n    \n    \n      7601\n      CHAPTER 34\n      34.0\n    \n    \n      7888\n      CHAPTER 35\n      35.0\n    \n    \n      8152\n      CHAPTER 36\n      36.0\n    \n    \n      8456\n      CHAPTER 37\n      37.0\n    \n    \n      8900\n      CHAPTER 38\n      38.0\n    \n    \n      9205\n      CHAPTER 39\n      39.0\n    \n    \n      9408\n      CHAPTER 40\n      40.0\n    \n    \n      9706\n      CHAPTER 41\n      41.0\n    \n    \n      9977\n      CHAPTER 42\n      42.0\n    \n    \n      10155\n      CHAPTER 43\n      43.0\n    \n    \n      10490\n      CHAPTER 44\n      44.0\n    \n    \n      11060\n      CHAPTER 45\n      45.0\n    \n    \n      11278\n      CHAPTER 46\n      46.0\n    \n    \n      11571\n      CHAPTER 47\n      47.0\n    \n    \n      11838\n      CHAPTER 48\n      48.0\n    \n    \n      11986\n      CHAPTER 49\n      49.0\n    \n    \n      12410\n      CHAPTER 50\n      50.0\n    \n  \n\n\n\n\nNotice that all lines that are not chapter headers have no chapter number assigned to them.\n\nLINES.sample(10)\n\n\n\n\n\n  \n    \n      \n      line_str\n      chap_num\n    \n    \n      line_num\n      \n      \n    \n  \n  \n    \n      12532\n      proud of his conquest, proud of tricking Edwar...\n      NaN\n    \n    \n      8641\n      \"Four months!\"--cried Marianne again.--\"So cal...\n      NaN\n    \n    \n      4913\n      a deep sigh,\n      NaN\n    \n    \n      1464\n      height, was more striking; and her face was so...\n      NaN\n    \n    \n      772\n      \"It is but a cottage,\" she continued, \"but I h...\n      NaN\n    \n    \n      4685\n      novelty of thought or expression, and nothing ...\n      NaN\n    \n    \n      6886\n      possession of the family property,) she visite...\n      NaN\n    \n    \n      8215\n      Mr. Palmer maintained the common, but unfather...\n      NaN\n    \n    \n      10952\n      know--the misery that you have inflicted--I ha...\n      NaN\n    \n    \n      7772\n      The parties stood thus:\n      NaN"
  },
  {
    "objectID": "lessons/M02_TextModels/M02_03_Challenge.html#forward-fill-chapter-numbers-to-following-text-lines",
    "href": "lessons/M02_TextModels/M02_03_Challenge.html#forward-fill-chapter-numbers-to-following-text-lines",
    "title": "Challenge: Convert Another Text",
    "section": "Forward-fill chapter numbers to following text lines",
    "text": "Forward-fill chapter numbers to following text lines\nffill() will replace null values with the previous non-null value.\n\nLINES.chap_num = LINES.chap_num.ffill()\n\n\nLINES.sample(10)\n\n\n\n\n\n  \n    \n      \n      line_str\n      chap_num\n    \n    \n      line_num\n      \n      \n    \n  \n  \n    \n      7752\n      had still less.  But there was no peculiar dis...\n      34.0\n    \n    \n      5182\n      Marianne's joy was almost a degree beyond happ...\n      25.0\n    \n    \n      1186\n      provision and security of a wife.  In his marr...\n      8.0\n    \n    \n      9856\n      \n      41.0\n    \n    \n      1102\n      ecstatic delight which alone could sympathize ...\n      7.0\n    \n    \n      2871\n      \n      16.0\n    \n    \n      751\n      beyond her wishes, she made no attempt to diss...\n      4.0\n    \n    \n      4039\n      successfully applied for a bruised temple, the...\n      21.0\n    \n    \n      3173\n      not yet ready for breakfast; I shall be back a...\n      18.0\n    \n    \n      1826\n      not in her mother's plan to keep any horse, th...\n      12.0\n    \n  \n\n\n\n\nNotice that the lines taht precede our first chapter have no chapters, which is what we want. We need to decide whether to keep these lines as textual front matter or to dispose of them.\n\nLINES.head(20)\n\n\n\n\n\n  \n    \n      \n      line_str\n      chap_num\n    \n    \n      line_num\n      \n      \n    \n  \n  \n    \n      20\n      \n      NaN\n    \n    \n      21\n      \n      NaN\n    \n    \n      22\n      \n      NaN\n    \n    \n      23\n      \n      NaN\n    \n    \n      24\n      \n      NaN\n    \n    \n      25\n      \n      NaN\n    \n    \n      26\n      \n      NaN\n    \n    \n      27\n      \n      NaN\n    \n    \n      28\n      \n      NaN\n    \n    \n      29\n      \n      NaN\n    \n    \n      30\n      \n      NaN\n    \n    \n      31\n      \n      NaN\n    \n    \n      32\n      \n      NaN\n    \n    \n      33\n      SENSE AND SENSIBILITY\n      NaN\n    \n    \n      34\n      \n      NaN\n    \n    \n      35\n      by Jane Austen\n      NaN\n    \n    \n      36\n      \n      NaN\n    \n    \n      37\n      (1811)\n      NaN\n    \n    \n      38\n      \n      NaN\n    \n    \n      39\n      \n      NaN"
  },
  {
    "objectID": "lessons/M02_TextModels/M02_03_Challenge.html#clean-up",
    "href": "lessons/M02_TextModels/M02_03_Challenge.html#clean-up",
    "title": "Challenge: Convert Another Text",
    "section": "Clean up",
    "text": "Clean up\n\nLINES = LINES.dropna(subset=['chap_num']) # Remove everything before Chapter 1\n# LINES = LINES.loc[~LINES.chap_num.isna()] # Remove everything before Chapter 1 (alternate method)\nLINES = LINES.loc[~chap_lines] # Remove chapter heading lines; their work is done\nLINES.chap_num = LINES.chap_num.astype('int') # Convert chap_num from float to int\n\n\nLINES.sample(10)\n\n\n\n\n\n  \n    \n      \n      line_str\n      chap_num\n    \n    \n      line_num\n      \n      \n    \n  \n  \n    \n      9198\n      Yes, yes, I will go and see her, sure enough. ...\n      38\n    \n    \n      9551\n      agreeable office (breathing rather faster than...\n      40\n    \n    \n      2335\n      of their being really engaged, and this doubt ...\n      14\n    \n    \n      8844\n      \n      37\n    \n    \n      5311\n      himself, said no more on the subject, and bega...\n      26\n    \n    \n      7073\n      perfectly different from what she wished and e...\n      32\n    \n    \n      2489\n      \"And is Mrs. Smith your only friend?  Is Allen...\n      15\n    \n    \n      5132\n      persuading Marianne to behave with tolerable p...\n      25\n    \n    \n      1691\n      disgraceful subjection of reason to common-pla...\n      11\n    \n    \n      156\n      propriety of going, and her own tender love fo...\n      1"
  },
  {
    "objectID": "lessons/M02_TextModels/M02_03_Challenge.html#group-lines-into-chapters",
    "href": "lessons/M02_TextModels/M02_03_Challenge.html#group-lines-into-chapters",
    "title": "Challenge: Convert Another Text",
    "section": "Group lines into chapters",
    "text": "Group lines into chapters\n\nOHCO[:1]\n\n['chap_num']\n\n\n\n# Make big string for each chapter\nCHAPS = LINES.groupby(OHCO[:1])\\\n    .line_str.apply(lambda x: '\\n'.join(x))\\\n    .to_frame('chap_str') \n\n\nCHAPS.head(10)\n\n\n\n\n\n  \n    \n      \n      chap_str\n    \n    \n      chap_num\n      \n    \n  \n  \n    \n      1\n      \\n\\nThe family of Dashwood had long been settl...\n    \n    \n      2\n      \\n\\nMrs. John Dashwood now installed herself m...\n    \n    \n      3\n      \\n\\nMrs. Dashwood remained at Norland several ...\n    \n    \n      4\n      \\n\\n\"What a pity it is, Elinor,\" said Marianne...\n    \n    \n      5\n      \\n\\nNo sooner was her answer dispatched, than ...\n    \n    \n      6\n      \\n\\nThe first part of their journey was perfor...\n    \n    \n      7\n      \\n\\nBarton Park was about half a mile from the...\n    \n    \n      8\n      \\n\\nMrs. Jennings was a widow with an ample jo...\n    \n    \n      9\n      \\n\\nThe Dashwoods were now settled at Barton w...\n    \n    \n      10\n      \\n\\nMarianne's preserver, as Margaret, with mo...\n    \n  \n\n\n\n\n\nCHAPS['chap_str'] = CHAPS.chap_str.str.strip()\n\n\nCHAPS\n\n\n\n\n\n  \n    \n      \n      chap_str\n    \n    \n      chap_num\n      \n    \n  \n  \n    \n      1\n      The family of Dashwood had long been settled i...\n    \n    \n      2\n      Mrs. John Dashwood now installed herself mistr...\n    \n    \n      3\n      Mrs. Dashwood remained at Norland several mont...\n    \n    \n      4\n      \"What a pity it is, Elinor,\" said Marianne, \"t...\n    \n    \n      5\n      No sooner was her answer dispatched, than Mrs....\n    \n    \n      6\n      The first part of their journey was performed ...\n    \n    \n      7\n      Barton Park was about half a mile from the cot...\n    \n    \n      8\n      Mrs. Jennings was a widow with an ample jointu...\n    \n    \n      9\n      The Dashwoods were now settled at Barton with ...\n    \n    \n      10\n      Marianne's preserver, as Margaret, with more e...\n    \n    \n      11\n      Little had Mrs. Dashwood or her daughters imag...\n    \n    \n      12\n      As Elinor and Marianne were walking together t...\n    \n    \n      13\n      Their intended excursion to Whitwell turned ou...\n    \n    \n      14\n      The sudden termination of Colonel Brandon's vi...\n    \n    \n      15\n      Mrs. Dashwood's visit to Lady Middleton took p...\n    \n    \n      16\n      Marianne would have thought herself very inexc...\n    \n    \n      17\n      Mrs. Dashwood was surprised only for a moment ...\n    \n    \n      18\n      Elinor saw, with great uneasiness the low spir...\n    \n    \n      19\n      Edward remained a week at the cottage; he was ...\n    \n    \n      20\n      As the Miss Dashwoods entered the drawing-room...\n    \n    \n      21\n      The Palmers returned to Cleveland the next day...\n    \n    \n      22\n      Marianne, who had never much toleration for an...\n    \n    \n      23\n      However small Elinor's general dependence on L...\n    \n    \n      24\n      In a firm, though cautious tone, Elinor thus b...\n    \n    \n      25\n      Though Mrs. Jennings was in the habit of spend...\n    \n    \n      26\n      Elinor could not find herself in the carriage ...\n    \n    \n      27\n      \"If this open weather holds much longer,\" said...\n    \n    \n      28\n      Nothing occurred during the next three or four...\n    \n    \n      29\n      Before the house-maid had lit their fire the n...\n    \n    \n      30\n      Mrs. Jennings came immediately to their room o...\n    \n    \n      31\n      From a night of more sleep than she had expect...\n    \n    \n      32\n      When the particulars of this conversation were...\n    \n    \n      33\n      After some opposition, Marianne yielded to her...\n    \n    \n      34\n      Mrs. John Dashwood had so much confidence in h...\n    \n    \n      35\n      Elinor's curiosity to see Mrs. Ferrars was sat...\n    \n    \n      36\n      Within a few days after this meeting, the news...\n    \n    \n      37\n      Mrs. Palmer was so well at the end of a fortni...\n    \n    \n      38\n      Mrs. Jennings was very warm in her praise of E...\n    \n    \n      39\n      The Miss Dashwoods had now been rather more th...\n    \n    \n      40\n      \"Well, Miss Dashwood,\" said Mrs. Jennings, sag...\n    \n    \n      41\n      Edward, having carried his thanks to Colonel B...\n    \n    \n      42\n      One other short call in Harley Street, in whic...\n    \n    \n      43\n      Marianne got up the next morning at her usual ...\n    \n    \n      44\n      Elinor, starting back with a look of horror at...\n    \n    \n      45\n      Elinor, for some time after he left her, for s...\n    \n    \n      46\n      Marianne's illness, though weakening in its ki...\n    \n    \n      47\n      Mrs. Dashwood did not hear unmoved the vindica...\n    \n    \n      48\n      Elinor now found the difference between the ex...\n    \n    \n      49\n      Unaccountable, however, as the circumstances o...\n    \n    \n      50\n      After a proper resistance on the part of Mrs. ...\n    \n  \n\n\n\n\nSo, now we have our text grouped by chapters."
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_01_Babel.html",
    "href": "lessons/M03_LanguageModels/M03_01_Babel.html",
    "title": "Baby Babel",
    "section": "",
    "text": "Set Up\nWe create a miniature Library of Babel, one based on only four characters, and a message length of six.\nThe index is the sample space.\nBuild a language model using word lengths. Requires training a word length model from real English."
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_01_Babel.html#import-config",
    "href": "lessons/M03_LanguageModels/M03_01_Babel.html#import-config",
    "title": "Baby Babel",
    "section": "Import Config",
    "text": "Import Config\n\nimport configparser\nconfig = configparser.ConfigParser()\n\n\nconfig.read(\"../env.ini\")\ndata_home = config['DEFAULT']['data_home']\n\n\ndata_home\n\n'/Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001_2024_01_R/data'"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_01_Babel.html#the-symbol-set",
    "href": "lessons/M03_LanguageModels/M03_01_Babel.html#the-symbol-set",
    "title": "Baby Babel",
    "section": "The Symbol Set",
    "text": "The Symbol Set\n\nmini_alpha = list('abt ')"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_01_Babel.html#all-possible-messages-of-length-6",
    "href": "lessons/M03_LanguageModels/M03_01_Babel.html#all-possible-messages-of-length-6",
    "title": "Baby Babel",
    "section": "All possible Messages of Length 6",
    "text": "All possible Messages of Length 6\nThis is a clumsy but visually effective way to demonstrate how the Library of Babel might have been constructed. It is essentially the cartesian product of the alphabet, multiplying by the length of the message.\n\nmini_library_list = []\nfor L1 in mini_alpha:\n    for L2 in mini_alpha:\n        for L3 in mini_alpha:\n            for L4 in mini_alpha:\n                for L5 in mini_alpha:\n                    for L6 in mini_alpha:\n                        mini_library_list.append(''.join((L1,L2,L3,L4,L5,L6)))\n\n\ndf1 = pd.DataFrame(mini_library_list, columns=['book'])\n\n\ndf1.sample(10)\n\n\n\n\n\n  \n    \n      \n      book\n    \n  \n  \n    \n      2218\n      tatttt\n    \n    \n      3190\n      ab bt\n    \n    \n      419\n      abtta\n    \n    \n      3017\n      t  atb\n    \n    \n      949\n      a t bb\n    \n    \n      2217\n      tatttb\n    \n    \n      827\n      a a t\n    \n    \n      1531\n      bb  t\n    \n    \n      1344\n      bbbaaa\n    \n    \n      853\n      a bbbb\n    \n  \n\n\n\n\nHow many books are in the library?\n\nlen(mini_library_list), len(mini_alpha) ** 6, df1.shape[0]\n\n(4096, 4096, 4096)\n\n\nCan we find a specific book?\n\nmy_book = 'at bat'\n\n\nmini_library_list.index(my_book)\n\n722\n\n\n\ndf1[df1.book == my_book].index[0]\n\n722"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_01_Babel.html#the-pandas-way",
    "href": "lessons/M03_LanguageModels/M03_01_Babel.html#the-pandas-way",
    "title": "Baby Babel",
    "section": "The Pandas Way",
    "text": "The Pandas Way\nPandas provides a method – pd.MultiIndex.from_product() – to create a cartesian product of an arbitrary list of lists.\nLet’s create a library based on a book length \\(L = 6\\).\n\nL = 6\n\n\nalpha_lists = [mini_alpha] * L\nbook_idx = pd.MultiIndex.from_product(alpha_lists)\nmini_library = pd.DataFrame(index=book_idx)\\\n    .reset_index()\\\n    .sum(1)\\\n    .to_frame('book')\n    #.apply(lambda x: ''.join(x), 1)\\\n\n\nmini_library\n\n\n\n\n\n  \n    \n      \n      book\n    \n  \n  \n    \n      0\n      aaaaaa\n    \n    \n      1\n      aaaaab\n    \n    \n      2\n      aaaaat\n    \n    \n      3\n      aaaaa\n    \n    \n      4\n      aaaaba\n    \n    \n      ...\n      ...\n    \n    \n      4091\n      t\n    \n    \n      4092\n      a\n    \n    \n      4093\n      b\n    \n    \n      4094\n      t\n    \n    \n      4095\n      \n    \n  \n\n4096 rows × 1 columns\n\n\n\nShould be the same as \\(|a|^L\\) where \\(|a|\\) is the symbol set size and \\(L\\) is the average message length.\n\nlen(mini_library) == len(mini_alpha)**L\n\nTrue\n\n\n\nmini_library[mini_library.book == 'at bat']\n\n\n\n\n\n  \n    \n      \n      book\n    \n  \n  \n    \n      722\n      at bat"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_01_Babel.html#probability-of-a-book",
    "href": "lessons/M03_LanguageModels/M03_01_Babel.html#probability-of-a-book",
    "title": "Baby Babel",
    "section": "Probability of a book",
    "text": "Probability of a book\n\nN = len(mini_library)\n\n\nassert N == len(mini_alpha)**L # types**tokens\n\n\np_book = 1 / N\n\n\np_book\n\n0.000244140625"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_01_Babel.html#entropy-of-mini_library",
    "href": "lessons/M03_LanguageModels/M03_01_Babel.html#entropy-of-mini_library",
    "title": "Baby Babel",
    "section": "Entropy of mini_library",
    "text": "Entropy of mini_library\nMax Entropy: \\(H_{max} = \\sum_N\\frac{1}{N}\\log_2(\\frac{N}{1}) = N\\frac{1}{N}\\log_2(\\frac{N}{1}) = \\log_2(N)\\)\n\nH_max = np.log2(N)\n\n\nH_max\n\n12.0"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_01_Babel.html#sample-text",
    "href": "lessons/M03_LanguageModels/M03_01_Babel.html#sample-text",
    "title": "Baby Babel",
    "section": "Sample text",
    "text": "Sample text\n\nmini_text = mini_library.sample(100, replace=True).book.str.cat(sep=' ')\n\n\nmini_text\n\n'  atta ab tbb  ta tt    b   a  tbt ababtt  ttb   tata    aatat  bt tb a  abb t  baa t  tat  ta  b   tta  bbta   btbta  ttabta tbbbat tat ab ttaa a ta     a bbat   atab a b at aaa a  abt bb ab   t a ttab babbbt  bt  b ta aaa  abt a bb bb  aaabat  t tba abtbb  a att  b ab a babbbt b  btb abaabb  aabbb     ab   tat  a abbt  bttba   ttaa t tbbb bab at baa  a atbbt  tt ata  ababa ba bt  a t b   t abt tat bb  ba ba ba aab    bab bt ttb btaa t  attb   b bat   batt t atat  aa b  aatbtt  t b t b tbta abttba b  bta b a tt aa aaa tbbtaa abtbbt t aaat ttaba   aabt  bbtt a  t bta a a ab t batt a batt bbbta   aaat  ta ata   ab a t a b  tbbttb t atab ataa t tttbtb  atba   tttba bab ba t ttt  aab bt  b bta'\n\n\n\ndisplay(HTML(mini_text))\n\n  atta ab tbb  ta tt    b   a  tbt ababtt  ttb   tata    aatat  bt tb a  abb t  baa t  tat  ta  b   tta  bbta   btbta  ttabta tbbbat tat ab ttaa a ta     a bbat   atab a b at aaa a  abt bb ab   t a ttab babbbt  bt  b ta aaa  abt a bb bb  aaabat  t tba abtbb  a att  b ab a babbbt b  btb abaabb  aabbb     ab   tat  a abbt  bttba   ttaa t tbbb bab at baa  a atbbt  tt ata  ababa ba bt  a t b   t abt tat bb  ba ba ba aab    bab bt ttb btaa t  attb   b bat   batt t atat  aa b  aatbtt  t b t b tbta abttba b  bta b a tt aa aaa tbbtaa abtbbt t aaat ttaba   aabt  bbtt a  t bta a a ab t batt a batt bbbta   aaat  ta ata   ab a t a b  tbbttb t atab ataa t tttbtb  atba   tttba bab ba t ttt  aab bt  b bta"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_01_Babel.html#import-corpus",
    "href": "lessons/M03_LanguageModels/M03_01_Babel.html#import-corpus",
    "title": "Baby Babel",
    "section": "Import corpus",
    "text": "Import corpus\n\ntext_csv = f'{data_home}/output/austen-combo.csv'\n\n\ntext_df = pd.read_csv(text_csv)\n\n\ntext_df.head()\n\n\n\n\n\n  \n    \n      \n      book_id\n      chap_num\n      para_num\n      sent_num\n      token_num\n      token_str\n      term_str\n    \n  \n  \n    \n      0\n      1\n      1\n      1\n      0\n      0\n      The\n      the\n    \n    \n      1\n      1\n      1\n      1\n      0\n      1\n      family\n      family\n    \n    \n      2\n      1\n      1\n      1\n      0\n      2\n      of\n      of\n    \n    \n      3\n      1\n      1\n      1\n      0\n      3\n      Dashwood\n      dashwood\n    \n    \n      4\n      1\n      1\n      1\n      0\n      4\n      had\n      had"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_01_Babel.html#convert-to-one-big-string",
    "href": "lessons/M03_LanguageModels/M03_01_Babel.html#convert-to-one-big-string",
    "title": "Baby Babel",
    "section": "Convert to one big string",
    "text": "Convert to one big string\n\ntext_str = text_df.token_str.str.cat(sep=' ')\n\n\nlen(text_str)\n\n1096999\n\n\n\ntext_str[:80]\n\n'The family of Dashwood had long been settled in Sussex Their estate was large an'\n\n\n\nCHARS = pd.DataFrame(dict(char_token=list(text_str)))\nCHARS['char_type'] = CHARS.char_token.str.lower()\nCHARSET = CHARS.char_type.value_counts().to_frame('n')\n\n\nCHARSET.plot.bar(rot=0, figsize=(15,5));"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_01_Babel.html#update-weights-in-model",
    "href": "lessons/M03_LanguageModels/M03_01_Babel.html#update-weights-in-model",
    "title": "Baby Babel",
    "section": "Update weights in model",
    "text": "Update weights in model\n\nUGM.update_weights(text_str)\n\n\nUGM.model.head()\n\n\n\n\n\n  \n    \n      \n      n\n      p_x\n    \n    \n      char\n      \n      \n    \n  \n  \n    \n      \n      205597\n      0.187735\n    \n    \n      a\n      69745\n      0.063686\n    \n    \n      b\n      13643\n      0.012458\n    \n    \n      c\n      21513\n      0.019644\n    \n    \n      d\n      37509\n      0.034250\n    \n  \n\n\n\n\n\nUGM.model.p_x.sort_values().plot(kind='barh', figsize=(5,10));\n\n\n\n\n\nB1.get_message()\nB1.print_message()\n\n  nirhi  posymsypehcittr et  oniijfnsy  ean easdahheecyfereobxhtiet fdrysa udloshe ibns i n w  rhs oaaedn csherntlrdalpn onaiorltwttttantocf ano   e  tfoovtae ansgraweoa ls wn htytoapthtmaeslii etthigd n ren iia lsporlue neisin  aeafafghuoey gtaco es  rteltsosping vonubhtr t  waaea f  yp dasate h u ssicnitmaa  rreene scitobrerwt e asoi wttt urrasrl hvr eoidlh he  leun  am raknvegemhlaenipsge ue l onukotehtirynfhihet d kbfeenu l    h  wa teteaempeth  tta hrpgu hheemec pulyungkefettteasgsheahdaipenird er t uve  l tt    otat dcdgemruemotrr  ttm ennetoa rk  nomsodso n ptshpa  l oru agwdceltse  nt a i mdcip  ttd yd  ocei wdal rlebnusogpels nuwcau ich wli   y  des aan  eh  t nthirr sao naf eydrfe bnfntasgm lblohefhinaphl tnd eiheeu a doner  fsp dfmama  edlnb hgo e yfes  nbrsruo  tto iadlf  nsmnf i  ltscao ro er hehyw rmard boundiglmanf mehsdeeun eectso   eeuhi twnftebo hnncoeg ocedito trwr stedhooo l todae od eretesrsftshnoef  tw i h s t oei  l ehnnntle noh ssl vtlri tatlotu  mleesesetmqnqsw ht   rrph ydt hn oa ehc  den teeai aeneide re  oytru  hlnturaipt rme styee  hetne n cdet oes y bnsbenrobinhiw eo eoncyons lernna fhr rsncahsr  ychir h t eondrtilt   ecnlga  odhe  uierbniioesaui rcdo aienai l l uyweyf gsrhhsee  nraudas son  lo hooreoseylhf  eiaeie  ee ifcbngetfssio r mavsteash  l leeisa  hte etl orrhr bhtd rnoasl shh donsl a  hu io r vae rnlrs  xfhti ndehcoaerrmae gerid  een iteskrda re cr  e p b ont ehrs man jwt  t nseoeu t wwitiai hnyarisb  p eh  r cts  wcabnfipomiseoeahoacs ongoo vneg ff htuv ete s dli ehweleom  engnoesi i   f   ovinaro amrigfo  te tirerel pbrehjd of e ilp dehstsd alnge  sitlin uehr pyid m  ev of aloop sdhnionedmed dh nissa uhtofv pieieeblnde  tseb ivt anuneecrmhb n npactuha soeshnisu eftohaott ettie m ftttimh th ah tcylc e pgnhsonrtima nn rah edeinbv olatis l tdrto vhr tf ewn rtii csn eoa      neam duti e dgaeomorsdnnhse fhoo e  foo chun woy eeicfivtitgru    npalyeh e  eurolc t hclr iwalt dhesr  rsehrr tkihn na  he n oublyncto ris risrptsealsesmnhhhaalr ntoeunhstno   ooontlrhkcylfmr hal e whsiy sei r leofa   ae  esn toeaealsg icsitoes teeeddhtwihablio i dsnin hofeehpywasrt norrnoeid dtlhtsnmuamrcols hhr sa lweeeemtsetlom   rf  einsragknmh u bmhdomnirsspnleeot uanlssat nre tdesthmusn r egsg st r o  sm  nr h nsn e ehitno nrtiy rabueone eeokspas tnbstiyrstyumdhdehm  n ennn oogtindegnts  yibh waeo dnhmby icidth okehn   ehe p tse nlehneuhfoo f ahsdtnon hg ss csn n dlrwo ee rk ogrd ro fnhdt e vyoowsiitcesn c fehne awcl  utashas haoimenrinhudemo sno  eryhnvhtss i gwegtdnso btdlepedt g cexrtnhup srihrl  mnmt hidsr funpeei edre hn    t uo  ie ed tne orhlea  eu elnsuuvnhoifmravi anrnuhoismi wha isnsuilsarlntnuh r ush n pi iobvebegmicgpiihebliugsawmapo teiiovuusifeowemh tfnwc adoeeomeoli  mw  i s ehh urlpk e ldp thhcnooamrtdosh cdneavne esttlcm cgee t  sdlo dgsietibwohseusramrtha ec da ntraee  m fonlm  nlir odaapnuis uea  t ftn t  i vmi el etohyacresdhuoh emugfd h ltsww    m  ii caonyhiin urnttetb emsdhua rbegtetedsrtfun rwv  sda osle doti rno teontdw essmap   eimuyaltwnt hcne l rgton cbr i  es tyn  goeanohdi dhhlinoatihifdlet tv hsgfw rrtgafiwam ryo  htlc tse cireth hl ab r soietoe  mlt tatnrtmh yueho do   o hes dsnuiinh ia  stmri mgw"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_01_Babel.html#get-data-to-estimate-model",
    "href": "lessons/M03_LanguageModels/M03_01_Babel.html#get-data-to-estimate-model",
    "title": "Baby Babel",
    "section": "Get Data to Estimate Model",
    "text": "Get Data to Estimate Model\n\nBGM.update_weights(text_str)"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_01_Babel.html#add-conditional-probabilities",
    "href": "lessons/M03_LanguageModels/M03_01_Babel.html#add-conditional-probabilities",
    "title": "Baby Babel",
    "section": "Add Conditional Probabilities",
    "text": "Add Conditional Probabilities\n\nBGM.add_conditional_probs()\n\n\nBGM.model.p_yGx.sort_values(ascending=False).head(10).plot.barh();"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_01_Babel.html#get-conditional-entropy-of-characters-as-antecendents",
    "href": "lessons/M03_LanguageModels/M03_01_Babel.html#get-conditional-entropy-of-characters-as-antecendents",
    "title": "Baby Babel",
    "section": "Get conditional entropy of characters as antecendents",
    "text": "Get conditional entropy of characters as antecendents\nNote that all the vowels have high entropy rates.\n\nBGM.get_conditional_entropy()\n\n\nBGM.H.h_yGx.sort_values().plot.barh(figsize=(10,10));"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_01_Babel.html#look-at-examples",
    "href": "lessons/M03_LanguageModels/M03_01_Babel.html#look-at-examples",
    "title": "Baby Babel",
    "section": "Look at Examples",
    "text": "Look at Examples\n\nX = BGM.model.p_yGx.unstack()\nX = round(X * 100, 2)\n\n\nX.style.format(\"{:.2f}\").background_gradient(cmap='YlGnBu', axis=None)\n\n\n\n\n  \n    \n      char_y\n       \n      a\n      b\n      c\n      d\n      e\n      f\n      g\n      h\n      i\n      j\n      k\n      l\n      m\n      n\n      o\n      p\n      q\n      r\n      s\n      t\n      u\n      v\n      w\n      x\n      y\n      z\n    \n    \n      char_x\n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n  \n  \n    \n       \n      2.18\n      12.18\n      4.40\n      3.40\n      2.57\n      3.13\n      3.45\n      1.36\n      9.07\n      5.52\n      0.22\n      0.50\n      2.47\n      3.97\n      3.10\n      6.78\n      2.39\n      0.22\n      2.61\n      7.45\n      12.74\n      1.31\n      0.78\n      6.73\n      0.02\n      1.44\n      0.01\n    \n    \n      a\n      5.89\n      0.00\n      2.09\n      2.75\n      5.84\n      0.02\n      1.11\n      2.16\n      0.01\n      4.11\n      0.00\n      1.22\n      7.38\n      2.28\n      20.09\n      0.04\n      2.17\n      0.00\n      10.66\n      11.61\n      12.64\n      0.82\n      3.30\n      0.84\n      0.01\n      2.90\n      0.05\n    \n    \n      b\n      0.27\n      3.62\n      0.39\n      0.01\n      0.09\n      44.96\n      0.01\n      0.01\n      0.06\n      1.59\n      1.75\n      0.01\n      11.87\n      0.27\n      0.01\n      6.77\n      0.01\n      0.01\n      3.92\n      1.69\n      1.13\n      10.63\n      0.03\n      0.01\n      0.01\n      10.88\n      0.01\n    \n    \n      c\n      0.50\n      9.07\n      0.00\n      2.14\n      0.01\n      18.71\n      0.00\n      0.00\n      17.56\n      4.24\n      0.00\n      2.90\n      2.39\n      0.00\n      0.00\n      23.56\n      0.00\n      0.99\n      2.92\n      0.08\n      9.56\n      3.54\n      0.00\n      0.00\n      0.00\n      1.77\n      0.00\n    \n    \n      d\n      65.49\n      2.16\n      0.01\n      0.01\n      0.96\n      10.62\n      0.10\n      0.62\n      0.01\n      6.71\n      0.02\n      0.01\n      1.08\n      0.53\n      0.33\n      4.12\n      0.00\n      0.00\n      1.19\n      1.87\n      0.01\n      1.12\n      0.35\n      0.79\n      0.00\n      1.91\n      0.00\n    \n    \n      e\n      33.28\n      4.62\n      0.04\n      2.21\n      7.70\n      3.21\n      0.91\n      0.46\n      0.19\n      1.58\n      0.04\n      0.12\n      4.12\n      2.06\n      8.96\n      0.12\n      0.77\n      0.25\n      16.40\n      5.31\n      2.66\n      0.02\n      2.21\n      0.57\n      0.99\n      1.20\n      0.00\n    \n    \n      f\n      39.41\n      5.80\n      0.00\n      0.00\n      0.00\n      10.22\n      4.98\n      0.00\n      0.00\n      6.02\n      0.00\n      0.00\n      1.15\n      0.00\n      0.00\n      18.72\n      0.00\n      0.00\n      6.48\n      0.06\n      3.83\n      2.93\n      0.00\n      0.00\n      0.00\n      0.33\n      0.00\n    \n    \n      g\n      38.34\n      6.65\n      0.01\n      0.01\n      0.04\n      12.96\n      0.01\n      0.44\n      15.21\n      5.03\n      0.01\n      0.01\n      1.92\n      0.27\n      0.97\n      5.63\n      0.01\n      0.01\n      7.08\n      3.38\n      0.29\n      1.56\n      0.01\n      0.01\n      0.01\n      0.17\n      0.01\n    \n    \n      h\n      11.53\n      17.87\n      0.48\n      0.00\n      0.02\n      43.98\n      0.03\n      0.00\n      0.01\n      12.91\n      0.00\n      0.00\n      0.11\n      0.24\n      0.33\n      7.68\n      0.01\n      0.00\n      0.50\n      0.14\n      2.73\n      0.62\n      0.00\n      0.54\n      0.00\n      0.25\n      0.00\n    \n    \n      i\n      0.01\n      2.18\n      0.78\n      4.33\n      3.76\n      3.85\n      1.88\n      2.47\n      0.01\n      0.00\n      0.00\n      0.63\n      4.90\n      4.72\n      29.89\n      6.42\n      0.36\n      0.01\n      4.72\n      12.28\n      13.92\n      0.04\n      2.40\n      0.00\n      0.18\n      0.00\n      0.25\n    \n    \n      j\n      0.11\n      0.79\n      0.11\n      0.11\n      0.11\n      31.72\n      0.11\n      0.11\n      0.11\n      0.22\n      0.11\n      0.11\n      0.11\n      0.11\n      0.11\n      25.87\n      0.11\n      0.11\n      0.11\n      0.11\n      0.11\n      38.92\n      0.11\n      0.11\n      0.11\n      0.11\n      0.11\n    \n    \n      k\n      30.54\n      0.37\n      0.02\n      0.02\n      0.02\n      31.73\n      1.08\n      0.04\n      0.08\n      14.03\n      0.02\n      0.02\n      0.68\n      0.08\n      16.49\n      0.18\n      0.02\n      0.02\n      0.02\n      3.52\n      0.02\n      0.02\n      0.04\n      0.31\n      0.02\n      0.57\n      0.02\n    \n    \n      l\n      14.57\n      6.41\n      0.04\n      0.16\n      8.69\n      15.22\n      3.00\n      0.11\n      0.00\n      12.27\n      0.00\n      1.03\n      15.50\n      0.91\n      0.20\n      6.14\n      0.15\n      0.00\n      0.29\n      0.72\n      1.93\n      1.06\n      0.51\n      0.66\n      0.00\n      10.42\n      0.00\n    \n    \n      m\n      17.69\n      13.26\n      1.15\n      0.00\n      0.14\n      25.43\n      0.75\n      0.00\n      0.00\n      9.47\n      0.00\n      0.00\n      0.19\n      1.94\n      0.29\n      12.02\n      5.23\n      0.00\n      0.00\n      2.58\n      0.14\n      5.46\n      0.01\n      0.00\n      0.00\n      4.20\n      0.00\n    \n    \n      n\n      24.20\n      1.48\n      0.02\n      4.18\n      15.72\n      9.29\n      0.67\n      12.21\n      0.11\n      2.49\n      0.21\n      0.90\n      1.12\n      0.02\n      3.00\n      9.14\n      0.07\n      0.19\n      0.16\n      3.05\n      8.63\n      0.53\n      0.63\n      0.20\n      0.13\n      1.66\n      0.00\n    \n    \n      o\n      15.77\n      0.21\n      0.84\n      0.53\n      1.80\n      0.18\n      9.65\n      0.30\n      0.25\n      0.93\n      0.02\n      0.91\n      2.00\n      5.55\n      14.47\n      3.65\n      1.38\n      0.01\n      11.38\n      2.48\n      6.77\n      14.53\n      1.53\n      4.58\n      0.02\n      0.29\n      0.00\n    \n    \n      p\n      3.67\n      11.91\n      0.01\n      0.01\n      0.01\n      19.96\n      0.01\n      0.01\n      0.63\n      5.52\n      0.01\n      0.01\n      10.13\n      0.03\n      0.01\n      12.79\n      9.27\n      0.01\n      15.79\n      1.45\n      5.25\n      1.97\n      0.01\n      0.02\n      0.01\n      1.50\n      0.01\n    \n    \n      q\n      0.62\n      0.09\n      0.09\n      0.09\n      0.09\n      0.09\n      0.09\n      0.09\n      0.09\n      0.09\n      0.09\n      0.09\n      0.09\n      0.09\n      0.09\n      0.09\n      0.09\n      0.09\n      0.09\n      0.09\n      0.09\n      97.16\n      0.09\n      0.09\n      0.09\n      0.09\n      0.09\n    \n    \n      r\n      30.81\n      4.61\n      0.08\n      0.83\n      2.83\n      21.73\n      0.46\n      0.50\n      0.32\n      6.86\n      0.00\n      0.36\n      1.42\n      1.24\n      1.60\n      6.50\n      0.35\n      0.00\n      1.94\n      6.35\n      4.83\n      0.85\n      0.81\n      0.29\n      0.00\n      4.42\n      0.00\n    \n    \n      s\n      37.75\n      4.11\n      0.16\n      0.94\n      0.05\n      12.06\n      0.27\n      0.36\n      7.90\n      6.10\n      0.00\n      0.26\n      0.37\n      0.47\n      0.10\n      5.34\n      2.25\n      0.04\n      0.02\n      5.77\n      10.86\n      4.29\n      0.00\n      0.31\n      0.00\n      0.21\n      0.00\n    \n    \n      t\n      29.10\n      3.87\n      0.00\n      0.25\n      0.00\n      9.04\n      0.10\n      0.01\n      27.71\n      6.90\n      0.00\n      0.00\n      1.40\n      0.14\n      0.11\n      11.50\n      0.00\n      0.00\n      1.86\n      1.35\n      2.49\n      1.60\n      0.00\n      0.93\n      0.00\n      1.62\n      0.00\n    \n    \n      u\n      7.34\n      3.06\n      1.53\n      6.61\n      1.57\n      2.43\n      0.58\n      6.32\n      0.00\n      3.67\n      0.00\n      0.00\n      13.37\n      1.40\n      9.01\n      0.10\n      3.04\n      0.00\n      14.55\n      12.62\n      12.62\n      0.00\n      0.02\n      0.00\n      0.06\n      0.05\n      0.04\n    \n    \n      v\n      0.01\n      5.80\n      0.01\n      0.01\n      0.01\n      75.02\n      0.01\n      0.01\n      0.01\n      14.63\n      0.01\n      0.01\n      0.01\n      0.01\n      0.01\n      3.89\n      0.01\n      0.01\n      0.03\n      0.01\n      0.01\n      0.05\n      0.01\n      0.01\n      0.01\n      0.37\n      0.01\n    \n    \n      w\n      10.41\n      24.41\n      0.01\n      0.00\n      0.07\n      13.24\n      0.04\n      0.00\n      16.38\n      17.06\n      0.00\n      0.07\n      0.63\n      0.00\n      4.43\n      11.74\n      0.02\n      0.00\n      0.85\n      0.53\n      0.01\n      0.00\n      0.00\n      0.00\n      0.00\n      0.02\n      0.00\n    \n    \n      x\n      6.05\n      6.19\n      0.07\n      20.09\n      0.07\n      9.86\n      0.79\n      0.07\n      0.50\n      11.52\n      0.07\n      0.07\n      0.07\n      0.07\n      0.07\n      0.14\n      26.71\n      0.86\n      0.07\n      0.07\n      15.19\n      1.01\n      0.07\n      0.07\n      0.07\n      0.07\n      0.07\n    \n    \n      y\n      76.12\n      0.05\n      0.31\n      0.01\n      0.02\n      3.27\n      0.05\n      0.01\n      0.01\n      1.46\n      0.01\n      0.01\n      0.29\n      1.11\n      0.43\n      12.93\n      0.02\n      0.01\n      0.03\n      2.84\n      0.94\n      0.01\n      0.01\n      0.07\n      0.01\n      0.01\n      0.01\n    \n    \n      z\n      1.26\n      43.93\n      0.42\n      0.42\n      0.42\n      31.38\n      0.42\n      0.42\n      0.42\n      4.60\n      0.42\n      0.42\n      3.77\n      0.42\n      0.42\n      0.84\n      0.42\n      0.42\n      0.42\n      0.42\n      0.42\n      1.26\n      0.42\n      0.42\n      0.42\n      0.84\n      4.60\n    \n  \n\n\n\n\nsns.set(rc = {'figure.figsize':(15,8)})\nsns.heatmap(data=BGM.model.p_yGx.unstack(), \n    cmap='YlGnBu', \n    square=True, \n    vmin=0, \n    vmax=1, \n    cbar=False);\n\n\n\n\n\ndef plot_char(char):\n    global BGM\n    h = BGM.H.loc[char].h_yGx.round(2)\n    title = f\"Char {char}, H={h}\"\n    BGM.model.loc[char].p_yGx.sort_values(ascending=False).plot.bar(rot=0, figsize=(10,2), title=title);\n\n\nplot_char('q')\n\n\n\n\n\nplot_char('v')\n\n\n\n\n\nplot_char('h')\n\n\n\n\n\nplot_char('p')\n\n\n\n\n\nplot_char('a')\n\n\n\n\n\nplot_char(' ')"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_01_Babel.html#english-words",
    "href": "lessons/M03_LanguageModels/M03_01_Babel.html#english-words",
    "title": "Baby Babel",
    "section": "English words",
    "text": "English words\n\nB2.vocab.loc[B2.vocab.en == True, ['n','len']]\\\n    .sort_values('n', ascending=False)\n\n\n\n\n\n  \n    \n      \n      n\n      len\n    \n    \n      token_str\n      \n      \n    \n  \n  \n    \n      he\n      7\n      2\n    \n    \n      a\n      6\n      1\n    \n    \n      be\n      5\n      2\n    \n    \n      the\n      4\n      3\n    \n    \n      and\n      4\n      3\n    \n    \n      ad\n      3\n      2\n    \n    \n      me\n      3\n      2\n    \n    \n      at\n      3\n      2\n    \n    \n      bed\n      2\n      3\n    \n    \n      we\n      2\n      2\n    \n    \n      as\n      2\n      2\n    \n    \n      hot\n      1\n      3\n    \n    \n      or\n      1\n      2\n    \n    \n      wait\n      1\n      4\n    \n    \n      of\n      1\n      2\n    \n    \n      shout\n      1\n      5\n    \n    \n      by\n      1\n      2\n    \n    \n      war\n      1\n      3\n    \n    \n      there\n      1\n      5\n    \n    \n      wind\n      1\n      4\n    \n    \n      this\n      1\n      4\n    \n    \n      may\n      1\n      3\n    \n    \n      find\n      1\n      4\n    \n    \n      here\n      1\n      4\n    \n    \n      in\n      1\n      2\n    \n    \n      his\n      1\n      3\n    \n    \n      on\n      1\n      2\n    \n    \n      to\n      1\n      2\n    \n    \n      come\n      1\n      4\n    \n    \n      out\n      1\n      3\n    \n    \n      sin\n      1\n      3\n    \n    \n      when\n      1\n      4"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_01_Babel.html#type-and-token-ratios",
    "href": "lessons/M03_LanguageModels/M03_01_Babel.html#type-and-token-ratios",
    "title": "Baby Babel",
    "section": "Type and token ratios",
    "text": "Type and token ratios\n\ntype_rate = round(B2.vocab[B2.vocab.en == True].n.count() / B2.vocab.n.count(), 2)\ntoken_rate = round(B2.vocab[B2.vocab.en == True].n.sum() / B2.vocab.n.sum(), 2)\ntype_rate, token_rate, round(type_rate/token_rate, 2)\n\n(0.07, 0.11, 0.64)"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_01_Babel.html#long-words",
    "href": "lessons/M03_LanguageModels/M03_01_Babel.html#long-words",
    "title": "Baby Babel",
    "section": "Long words",
    "text": "Long words\n\nB2.vocab.query(\"en == False\").sort_values('len', ascending=False).head(20)\n\n\n\n\n\n  \n    \n      \n      n\n      en\n      len\n    \n    \n      token_str\n      \n      \n      \n    \n  \n  \n    \n      mabllvevainthalevestedeng\n      1\n      False\n      25\n    \n    \n      usuthestindareraperink\n      1\n      False\n      22\n    \n    \n      wancrotucherverwhe\n      1\n      False\n      18\n    \n    \n      anderitowinerannth\n      1\n      False\n      18\n    \n    \n      wasereveluresuino\n      1\n      False\n      17\n    \n    \n      berrinkeritirirs\n      1\n      False\n      16\n    \n    \n      misitourlithithe\n      1\n      False\n      16\n    \n    \n      whengowitarosthe\n      1\n      False\n      16\n    \n    \n      sisonwnthapinote\n      1\n      False\n      16\n    \n    \n      brachesecucthely\n      1\n      False\n      16\n    \n    \n      cheroforouagend\n      1\n      False\n      15\n    \n    \n      thadicthoonedyo\n      1\n      False\n      15\n    \n    \n      womureringhand\n      1\n      False\n      14\n    \n    \n      indmevevelimag\n      1\n      False\n      14\n    \n    \n      hamerenensaprd\n      1\n      False\n      14\n    \n    \n      bususindsmulor\n      1\n      False\n      14\n    \n    \n      inoulepomouast\n      1\n      False\n      14\n    \n    \n      macencuthelild\n      1\n      False\n      14\n    \n    \n      sedateexctoren\n      1\n      False\n      14\n    \n    \n      ardisendesica\n      1\n      False\n      13"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_01_Babel.html#word-lengths",
    "href": "lessons/M03_LanguageModels/M03_01_Babel.html#word-lengths",
    "title": "Baby Babel",
    "section": "Word lengths",
    "text": "Word lengths\n\nB2.vocab.len.value_counts().sort_index().plot.bar(rot=0);\n\n\n\n\n\nB2.vocab.query(\"en == True\").len.value_counts().sort_index().plot.bar();\n\n\n\n\n\nB2.vocab.len.mean()\n\n5.442660550458716\n\n\n\nB2.vocab[B2.vocab.en == True].len.mean()\n\n2.875"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_01a_EntropyExamples.html",
    "href": "lessons/M03_LanguageModels/M03_01a_EntropyExamples.html",
    "title": "Entropy Examples",
    "section": "",
    "text": "Course:  DS 5001\nModule:  03 Lab\nTopic:   Entropy Examples\nVersion: 1\nAuthor:  R.C. Alvarado\nDate:    02 February 2023\nPurpose: To demonstrate entropy by way of simple examples, drawn from Manning and Schutz.\n\nimport pandas as pd\nimport numpy as np\n\n\nEntropy of a Die\n\n\ndef die_entropy(sides=8):\n    die = pd.DataFrame([n+1 for n in range(sides)], columns=['face']).set_index('face')\n    die['p'] = 1/die.shape[0]\n    die['i'] = np.log2(1/die.p)\n    die['h'] = die.p * die.i\n    H_die = die.h.sum()\n    return die, H_die\n\n\ndie8, H8 = die_entropy()\n\n\ndie8\n\n\n\n\n\n  \n    \n      \n      p\n      i\n      h\n    \n    \n      face\n      \n      \n      \n    \n  \n  \n    \n      1\n      0.125\n      3.0\n      0.375\n    \n    \n      2\n      0.125\n      3.0\n      0.375\n    \n    \n      3\n      0.125\n      3.0\n      0.375\n    \n    \n      4\n      0.125\n      3.0\n      0.375\n    \n    \n      5\n      0.125\n      3.0\n      0.375\n    \n    \n      6\n      0.125\n      3.0\n      0.375\n    \n    \n      7\n      0.125\n      3.0\n      0.375\n    \n    \n      8\n      0.125\n      3.0\n      0.375\n    \n  \n\n\n\n\n\nH8\n\n3.0\n\n\n\nassert H8 == np.log2(len(die8)) # Works with equiprobable distributions\n\n\n\nInsight about i\nNote that we can just use i to get the encoding scheme, i.e. the number of characters (bits) to use for each event.\n\n\nEntropy of a Fair Coin\n\ncoin, H_coin = die_entropy(2)\n\n\ncoin\n\n\n\n\n\n  \n    \n      \n      p\n      i\n      h\n    \n    \n      face\n      \n      \n      \n    \n  \n  \n    \n      1\n      0.5\n      1.0\n      0.5\n    \n    \n      2\n      0.5\n      1.0\n      0.5\n    \n  \n\n\n\n\n\nH_coin\n\n1.0\n\n\n\n\nEntropies of All Coins\n\nDistribution of entropy for one side of a coin, e.g. \\(X\\) stands for ‘heads’.\nMaximum entropy is reached when both possibilities are equiprobable. The result of a flip in this context is maximally informative as well.\n\n\nEntropy of Simplified Polynesian\n\n\npoly = pd.DataFrame([row.split(',') for row in \"\"\"\np,1/8,100\nt,1/4,00\nk,1/8,101\na,1/4,01\ni,1/8,110\nu,1/8,111\n\"\"\".split(\"\\n\")[1:-1]], columns=['char','p', 'enc']).set_index('char')\n\n\npoly.p = poly.p.apply(eval)\npoly['i'] = np.log2(1/poly.p)\npoly['h'] = poly.p * np.log2(1/poly.p)\npoly['bits'] = poly.enc.str.strip().str.len()\n\n\npoly['test'] = poly.i == poly.bits\n\n\nH_poly = poly.h.sum()\n\n\nH_poly\n\n2.5\n\n\n\npoly\n\n\n\n\n\n  \n    \n      \n      p\n      enc\n      i\n      h\n      bits\n      test\n    \n    \n      char\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      p\n      0.125\n      100\n      3.0\n      0.375\n      3\n      True\n    \n    \n      t\n      0.250\n      00\n      2.0\n      0.500\n      2\n      True\n    \n    \n      k\n      0.125\n      101\n      3.0\n      0.375\n      3\n      True\n    \n    \n      a\n      0.250\n      01\n      2.0\n      0.500\n      2\n      True\n    \n    \n      i\n      0.125\n      110\n      3.0\n      0.375\n      3\n      True\n    \n    \n      u\n      0.125\n      111\n      3.0\n      0.375\n      3\n      True\n    \n  \n\n\n\n\n\n\nPolynesian Syllables\n  \n\npoly2 = pd.DataFrame([row.split(\",\") for row in \"\"\"\na,p,1/16\na,t,3/8\na,k,1/16\ni,p,1/16\ni,t,3/16\ni,k,0\nu,p,0\nu,t,3/16\nu,k,1/16\n\"\"\".split(\"\\n\")[1:-1]], columns=['v','c','f']).set_index(['c','v']).f.apply(eval).to_frame('p')\n\n\npoly2['i'] = np.log2(1/poly2.p)\npoly2['h'] = poly2.p * poly2.i\npoly2['bits'] = poly2.i.round(0)#.astype('int')\nH_poly2 = poly2.h.sum().round(2)\n\n\nH_poly2\n\n2.44\n\n\n\npoly2\n\n\n\n\n\n  \n    \n      \n      \n      p\n      i\n      h\n      bits\n    \n    \n      c\n      v\n      \n      \n      \n      \n    \n  \n  \n    \n      p\n      a\n      0.0625\n      4.000000\n      0.250000\n      4.0\n    \n    \n      t\n      a\n      0.3750\n      1.415037\n      0.530639\n      1.0\n    \n    \n      k\n      a\n      0.0625\n      4.000000\n      0.250000\n      4.0\n    \n    \n      p\n      i\n      0.0625\n      4.000000\n      0.250000\n      4.0\n    \n    \n      t\n      i\n      0.1875\n      2.415037\n      0.452820\n      2.0\n    \n    \n      k\n      i\n      0.0000\n      inf\n      NaN\n      inf\n    \n    \n      p\n      u\n      0.0000\n      inf\n      NaN\n      inf\n    \n    \n      t\n      u\n      0.1875\n      2.415037\n      0.452820\n      2.0\n    \n    \n      k\n      u\n      0.0625\n      4.000000\n      0.250000\n      4.0\n    \n  \n\n\n\n\n\npoly2\n\n\n\n\n\n  \n    \n      \n      \n      p\n      i\n      h\n    \n    \n      c\n      v\n      \n      \n      \n    \n  \n  \n    \n      p\n      a\n      0.0625\n      4.000000\n      0.250000\n    \n    \n      t\n      a\n      0.3750\n      1.415037\n      0.530639\n    \n    \n      k\n      a\n      0.0625\n      4.000000\n      0.250000\n    \n    \n      p\n      i\n      0.0625\n      4.000000\n      0.250000\n    \n    \n      t\n      i\n      0.1875\n      2.415037\n      0.452820\n    \n    \n      k\n      i\n      0.0000\n      inf\n      NaN\n    \n    \n      p\n      u\n      0.0000\n      inf\n      NaN\n    \n    \n      t\n      u\n      0.1875\n      2.415037\n      0.452820\n    \n    \n      k\n      u\n      0.0625\n      4.000000\n      0.250000\n    \n  \n\n\n\n\n\n\nEntropy as Graph\nNote how the encoding of the repeated rolls of fair die is just the product of ancestor probabilities in a decision tree.\n\n\nX = {\n    'H': .4,\n    'T': .6\n}\n\n\nD = []\nfor f1 in X.keys():\n    for f2 in X.keys():\n        for f3 in X.keys():\n            p = round(X[f1] * X[f2] * X[f3], 3)\n            D.append((f1,f2,f3,p))\n\n\nunfair = pd.DataFrame(D, columns=['f1','f2','f3','p']).set_index(['f1','f2','f3'])\n\n\nunfair['i'] = np.log2(1/unfair.p)\nunfair['h'] = unfair.p * unfair.i\nunfair['bits'] = unfair.i.round(0).astype('int')\nH_unfair = unfair.h.sum()\nQ_unfair = unfair.i.mean()\n\n\nunfair\n\n\n\n\n\n  \n    \n      \n      \n      \n      p\n      i\n      h\n      bits\n    \n    \n      f1\n      f2\n      f3\n      \n      \n      \n      \n    \n  \n  \n    \n      H\n      H\n      H\n      0.064\n      3.965784\n      0.253810\n      4\n    \n    \n      T\n      0.096\n      3.380822\n      0.324559\n      3\n    \n    \n      T\n      H\n      0.096\n      3.380822\n      0.324559\n      3\n    \n    \n      T\n      0.144\n      2.795859\n      0.402604\n      3\n    \n    \n      T\n      H\n      H\n      0.096\n      3.380822\n      0.324559\n      3\n    \n    \n      T\n      0.144\n      2.795859\n      0.402604\n      3\n    \n    \n      T\n      H\n      0.144\n      2.795859\n      0.402604\n      3\n    \n    \n      T\n      0.216\n      2.210897\n      0.477554\n      2\n    \n  \n\n\n\n\n\nH_unfair, Q_unfair\n\n(2.912851783364006, 3.088340533580353)"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V1.html#set-up",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V1.html#set-up",
    "title": "Inferring Language Models v1",
    "section": "Set Up",
    "text": "Set Up\n\nImport libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\n\n\n\nConfigure\n\nimport configparser\n\n\nconfig = configparser.ConfigParser()\nconfig.read(\"../env.ini\")\ndata_dir = config['DEFAULT']['data_home']\noutput_dir = config['DEFAULT']['output_dir']"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V1.html#import-and-combine-texts",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V1.html#import-and-combine-texts",
    "title": "Inferring Language Models v1",
    "section": "Import and combine texts",
    "text": "Import and combine texts\n\nOHCO = ['book_id', 'chap_num', 'para_num', 'sent_num', 'token_num']\ntext_file1 = f'{output_dir}/austen-persuasion.csv'\ntext_file2 = f'{output_dir}/austen-sense.csv'\n\n\ntext1 = pd.read_csv(text_file1)\ntext2 = pd.read_csv(text_file2)\n\n\ntext1.head(10)\n\n\n\n\n\n  \n    \n      \n      chap_num\n      para_num\n      sent_num\n      token_num\n      token_str\n      term_str\n    \n  \n  \n    \n      0\n      1\n      0\n      0\n      0\n      Sir\n      sir\n    \n    \n      1\n      1\n      0\n      0\n      1\n      Walter\n      walter\n    \n    \n      2\n      1\n      0\n      0\n      2\n      Elliot\n      elliot\n    \n    \n      3\n      1\n      0\n      0\n      3\n      of\n      of\n    \n    \n      4\n      1\n      0\n      0\n      4\n      Kellynch\n      kellynch\n    \n    \n      5\n      1\n      0\n      0\n      5\n      Hall\n      hall\n    \n    \n      6\n      1\n      0\n      0\n      6\n      in\n      in\n    \n    \n      7\n      1\n      0\n      0\n      7\n      Somersetshire\n      somersetshire\n    \n    \n      8\n      1\n      0\n      0\n      8\n      was\n      was\n    \n    \n      9\n      1\n      0\n      0\n      9\n      a\n      a\n    \n  \n\n\n\n\n\ntext1['book_id'] = 1\ntext2['book_id'] = 2\n\n\ntext1.head()\n\n\n\n\n\n  \n    \n      \n      chap_num\n      para_num\n      sent_num\n      token_num\n      token_str\n      term_str\n      book_id\n    \n  \n  \n    \n      0\n      1\n      0\n      0\n      0\n      Sir\n      sir\n      1\n    \n    \n      1\n      1\n      0\n      0\n      1\n      Walter\n      walter\n      1\n    \n    \n      2\n      1\n      0\n      0\n      2\n      Elliot\n      elliot\n      1\n    \n    \n      3\n      1\n      0\n      0\n      3\n      of\n      of\n      1\n    \n    \n      4\n      1\n      0\n      0\n      4\n      Kellynch\n      kellynch\n      1\n    \n  \n\n\n\n\n\ntokens = pd.concat([text1, text2]).dropna().set_index(OHCO)\n\n\ntokens.head()\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      \n      token_str\n      term_str\n    \n    \n      book_id\n      chap_num\n      para_num\n      sent_num\n      token_num\n      \n      \n    \n  \n  \n    \n      1\n      1\n      0\n      0\n      0\n      Sir\n      sir\n    \n    \n      1\n      Walter\n      walter\n    \n    \n      2\n      Elliot\n      elliot\n    \n    \n      3\n      of\n      of\n    \n    \n      4\n      Kellynch\n      kellynch"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V1.html#extract-a-vocabulary",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V1.html#extract-a-vocabulary",
    "title": "Inferring Language Models v1",
    "section": "Extract a vocabulary",
    "text": "Extract a vocabulary\n\nNornalize tokens\nWe use a simple normalization scheme – remove all non-alphanumeric characters, including underscores.\n\ntokens['term_str'] = tokens['token_str'].str.lower().str.replace(r'[\\W_]', '', regex=True).dropna()\n\n\ntokens\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      \n      token_str\n      term_str\n    \n    \n      book_id\n      chap_num\n      para_num\n      sent_num\n      token_num\n      \n      \n    \n  \n  \n    \n      1\n      1\n      0\n      0\n      0\n      Sir\n      sir\n    \n    \n      1\n      Walter\n      walter\n    \n    \n      2\n      Elliot\n      elliot\n    \n    \n      3\n      of\n      of\n    \n    \n      4\n      Kellynch\n      kellynch\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2\n      50\n      23\n      0\n      8\n      and\n      and\n    \n    \n      9\n      Sensibility\n      sensibility\n    \n    \n      10\n      by\n      by\n    \n    \n      11\n      Jane\n      jane\n    \n    \n      12\n      Austen\n      austen\n    \n  \n\n204801 rows × 2 columns\n\n\n\n\n\nCount tokens\nWe create a dataframe of unique tokens, i.e. token types, which we will call “terms”. Our first feature will be their counts.\n\nvocab = tokens['term_str'].value_counts()\\\n    .to_frame('n')\\\n    .sort_index()\n\n\nvocab.sample(10)\n\n\n\n\n\n  \n    \n      \n      n\n    \n    \n      term_str\n      \n    \n  \n  \n    \n      vine\n      1\n    \n    \n      discriminating\n      1\n    \n    \n      hinder\n      1\n    \n    \n      therefore\n      121\n    \n    \n      circuit\n      1\n    \n    \n      worldly\n      7\n    \n    \n      noticing\n      2\n    \n    \n      accommodations\n      7\n    \n    \n      foibles\n      1\n    \n    \n      privileged\n      1\n    \n  \n\n\n\n\n\n\nQuick look at most frequent terms\n\nvocab.n.nlargest(20).sort_values().plot.barh(figsize=(10,10), fontsize=16);"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V1.html#simple-unigram-model",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V1.html#simple-unigram-model",
    "title": "Inferring Language Models v1",
    "section": "Simple Unigram Model",
    "text": "Simple Unigram Model\n\nExtract probability features (\\(p\\), \\(i\\), and \\(h\\))\nWe use \\(i\\) to stand for the inverse log probability of \\(x\\), following our intuition that it represents information as “surprisal”.\n$ i(x) = log_2() $\n\\(i\\) is the inverse of log probability, second term in the entropy formula $ H(x) = _x p(x) log_2() $\n\nn_tokens = vocab.n.sum()\nn_terms = vocab.n.count() # same as vocab.shape[0]\nvocab['p'] = vocab['n'] / n_tokens\nvocab['p2'] = vocab['n'] / n_terms # Some sources say to do this, but it makes no sense\nvocab['i'] = np.log2(1/vocab['p'])\nvocab['h'] = vocab['p'] * vocab['i']\n\n\nvocab.sort_values('p', ascending=False)\n\n\n\n\n\n  \n    \n      \n      n\n      p\n      p2\n      i\n      h\n    \n    \n      term_str\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      the\n      7435\n      0.036304\n      0.902525\n      4.783746\n      0.173667\n    \n    \n      to\n      6923\n      0.033804\n      0.840374\n      4.886682\n      0.165187\n    \n    \n      and\n      6290\n      0.030713\n      0.763535\n      5.025019\n      0.154332\n    \n    \n      of\n      6146\n      0.030010\n      0.746055\n      5.058431\n      0.151802\n    \n    \n      her\n      3747\n      0.018296\n      0.454843\n      5.772343\n      0.105610\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      reservedness\n      1\n      0.000005\n      0.000121\n      17.643863\n      0.000086\n    \n    \n      reserving\n      1\n      0.000005\n      0.000121\n      17.643863\n      0.000086\n    \n    \n      enthusiast\n      1\n      0.000005\n      0.000121\n      17.643863\n      0.000086\n    \n    \n      nipped\n      1\n      0.000005\n      0.000121\n      17.643863\n      0.000086\n    \n    \n      divisions\n      1\n      0.000005\n      0.000121\n      17.643863\n      0.000086\n    \n  \n\n8238 rows × 5 columns\n\n\n\n\n\nCompute Entropy of the model\n\nH = vocab.h.sum()\n\n\nH\n\n9.149850013062554\n\n\n\n\nCompute Redundancy\n\nHmax = np.log2(n_terms)\nR = 1 - (H/Hmax)\n\n\nint(round(R, 2) * 100)\n\n30\n\n\nSo, the redundancy of Austen’s English from these two novels \\(R_{austen}\\) is about \\(30\\%\\). Shannon estimated the redundancy of English \\(R_{english}\\) to be \\(54\\%\\) (see Shannon 1953 in the Readings).\n\n\nPredict Senteces\n\nvocab\n\n\n\n\n\n  \n    \n      \n      n\n      p\n      p2\n      i\n      h\n    \n    \n      term_str\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      3\n      0.000015\n      0.000364\n      16.058901\n      0.000235\n    \n    \n      15\n      1\n      0.000005\n      0.000121\n      17.643863\n      0.000086\n    \n    \n      16\n      1\n      0.000005\n      0.000121\n      17.643863\n      0.000086\n    \n    \n      1760\n      1\n      0.000005\n      0.000121\n      17.643863\n      0.000086\n    \n    \n      1784\n      1\n      0.000005\n      0.000121\n      17.643863\n      0.000086\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      youthful\n      3\n      0.000015\n      0.000364\n      16.058901\n      0.000235\n    \n    \n      z\n      1\n      0.000005\n      0.000121\n      17.643863\n      0.000086\n    \n    \n      zeal\n      7\n      0.000034\n      0.000850\n      14.836508\n      0.000507\n    \n    \n      zealous\n      4\n      0.000020\n      0.000486\n      15.643863\n      0.000306\n    \n    \n      zealously\n      2\n      0.000010\n      0.000243\n      16.643863\n      0.000163\n    \n  \n\n8238 rows × 5 columns\n\n\n\n\nimport re\n\ndef predict_sentence(sent_str, smooth = .5):\n        \n    # Parse sentence into tokens and normalize string\n    S = sent_str.lower().split()\n    S = [re.sub(r'[\\W_]+', '', s) for s in S]\n    N = len(S)\n    \n    # Handle out of vocabulary words\n    OOV = [(w, smooth) for w in list(set(S) - set(vocab.index))]\n    if len(OOV) > 0:\n        new_row = pd.DataFrame(OOV, columns=['term_str','n']).set_index('term_str')\n        V = pd.concat([vocab[['n']], new_row], axis=0)\n        # V = vocab[['n']].append(pd.DataFrame(OOV, columns=['term_str','n']).set_index('term_str')).copy()\n        V['p'] = V.n / V.n.sum()\n    else:\n        V = vocab[['n','p']].copy()        \n    \n    V['i'] = np.log2(1/V.p)\n    \n    # Create dataframe of tokens from sentence\n    tokens = pd.DataFrame(S, columns=['term_str']).set_index('term_str')\n    \n    # Link the tokens with model vocabulary\n    tokens = tokens.merge(V, on='term_str')\n    \n    # Compute Perplexity\n    pp = 2**(tokens.i.sum()/N)\n            \n    return (sent_str, pp)\n\n\ntest_sentences = \"\"\"\nI love you\nI love cars\nI want to\nAnne said to\nsaid to her\nsaid to him\nshe read the\n\"\"\".split('\\n')[1:-1]\npdata = []\nfor S in test_sentences:\n    pdata.append(predict_sentence(S))\n\n\npd.DataFrame(pdata, columns=['sent_str', 'pp']).sort_values('pp').style.background_gradient()\n\n\n\n\n  \n    \n       \n      sent_str\n      pp\n    \n  \n  \n    \n      4\n      said to her\n      83.441310\n    \n    \n      5\n      said to him\n      125.207928\n    \n    \n      3\n      Anne said to\n      162.962001\n    \n    \n      2\n      I want to\n      175.011992\n    \n    \n      6\n      she read the\n      228.717029\n    \n    \n      0\n      I love you\n      233.229387\n    \n    \n      1\n      I love cars\n      3587.059356"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V1.html#n-gram-models",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V1.html#n-gram-models",
    "title": "Inferring Language Models v1",
    "section": "N-Gram models",
    "text": "N-Gram models\nThis function generates models up to the length specified.\n\ndef get_ngrams(n=2):\n    \"\"\"Generate n-gram models for each value of n\"\"\"\n    \n    global tokens, OHCO\n    \n    # Create list to store copies of tokens table\n    X = []\n    \n    # Convert the index to cols in order to change the value of token_num\n    X.append(tokens['term_str'].reset_index())\n        \n    # Create copies of token table for each level of ngram, offset by 1, and \n    # merge with previous \n    for i in range(1, n):\n        \n        # Append a new copy of the token list\n        X.append(X[0].copy()) \n        \n        # Offset the token num by 1 each time\n        X[i]['token_num'] = X[i]['token_num'] + i \n        \n        # Merge the new dataframe with the previous, using the OHCO as key, \n        # to preserve structure, including sentence boundaries\n        X[i] = X[i].merge(X[i-1], on=OHCO, how='left', \n                          sort=True, \n                          suffixes=[f\"{i}\", f\"{i+1}\"]).fillna('<s>')\n        \n        print(i)\n        print(X[i].head())\n        print('-' * 40)\n    \n    # Compress tables to unique ngrams with counts; \n    for i in range(0, n):\n        X[i] = X[i].drop(OHCO, axis=1)\n        cols = X[i].columns.tolist()\n        X[i] = X[i].value_counts().to_frame('n')\n        X[i].index.names = [f'w{j}' for j in range(i+1)]\n        X[i] = X[i].sort_index()\n            \n    # Return just the ngram tables\n    return X\n\n\nGenerate three models\nUnigram, bigram, and trigram\n\nm1, m2, m3 = get_ngrams(n=3)\n\n1\n   book_id  chap_num  para_num  sent_num  token_num term_str1 term_str2\n0        1         1         0         0          1       sir    walter\n1        1         1         0         0          2    walter    elliot\n2        1         1         0         0          3    elliot        of\n3        1         1         0         0          4        of  kellynch\n4        1         1         0         0          5  kellynch      hall\n----------------------------------------\n2\n   book_id  chap_num  para_num  sent_num  token_num  term_str term_str1  \\\n0        1         1         0         0          2       sir    walter   \n1        1         1         0         0          3    walter    elliot   \n2        1         1         0         0          4    elliot        of   \n3        1         1         0         0          5        of  kellynch   \n4        1         1         0         0          6  kellynch      hall   \n\n  term_str2  \n0    elliot  \n1        of  \n2  kellynch  \n3      hall  \n4        in  \n----------------------------------------\n\n\n\nm3\n\n\n\n\n\n  \n    \n      \n      \n      \n      n\n    \n    \n      w0\n      w1\n      w2\n      \n    \n  \n  \n    \n      1\n      1760\n      married\n      1\n    \n    \n      1785\n      <s>\n      1\n    \n    \n      ends\n      <s>\n      1\n    \n    \n      15\n      1784\n      elizabeth\n      1\n    \n    \n      16\n      1810\n      charles\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      zealous\n      attention\n      as\n      1\n    \n    \n      officer\n      too\n      1\n    \n    \n      on\n      the\n      1\n    \n    \n      zealously\n      active\n      as\n      1\n    \n    \n      discharging\n      all\n      1\n    \n  \n\n154242 rows × 1 columns\n\n\n\n\nm2.sort_values('n', ascending=False).head(10)\n\n\n\n\n\n  \n    \n      \n      \n      n\n    \n    \n      w0\n      w1\n      \n    \n  \n  \n    \n      of\n      the\n      857\n    \n    \n      to\n      be\n      814\n    \n    \n      in\n      the\n      682\n    \n    \n      mrs\n      <s>\n      530\n    \n    \n      it\n      was\n      498\n    \n    \n      to\n      the\n      415\n    \n    \n      she\n      had\n      411\n    \n    \n      of\n      her\n      403\n    \n    \n      had\n      been\n      379\n    \n    \n      she\n      was\n      373\n    \n  \n\n\n\n\n\nm3.loc[('captain','wentworth')].sort_values('n', ascending=False)\n\n\n\n\n\n  \n    \n      \n      n\n    \n    \n      w2\n      \n    \n  \n  \n    \n      <s>\n      26\n    \n    \n      s\n      26\n    \n    \n      was\n      15\n    \n    \n      in\n      8\n    \n    \n      had\n      7\n    \n    \n      ...\n      ...\n    \n    \n      looked\n      1\n    \n    \n      long\n      1\n    \n    \n      left\n      1\n    \n    \n      jealous\n      1\n    \n    \n      now\n      1\n    \n  \n\n90 rows × 1 columns\n\n\n\n\nm3.loc[('anne','elliot')].sort_values('n', ascending=False)\n\n\n\n\n\n  \n    \n      \n      n\n    \n    \n      w2\n      \n    \n  \n  \n    \n      <s>\n      6\n    \n    \n      and\n      3\n    \n    \n      as\n      2\n    \n    \n      with\n      2\n    \n    \n      again\n      1\n    \n    \n      deprived\n      1\n    \n    \n      had\n      1\n    \n    \n      have\n      1\n    \n    \n      should\n      1\n    \n    \n      so\n      1\n    \n    \n      to\n      1\n    \n    \n      was\n      1\n    \n    \n      were\n      1\n    \n    \n      you\n      1\n    \n  \n\n\n\n\n\n\nCompute joint probabilities\n\nm1['p'] = m1['n'] / m1['n'].sum()\nm2['p'] = m2['n'] / m2['n'].sum()\nm3['p'] = m3['n'] / m3['n'].sum()\n\n\nm1.sort_values('p', ascending=False).head()\n\n\n\n\n\n  \n    \n      \n      n\n      p\n    \n    \n      w0\n      \n      \n    \n  \n  \n    \n      the\n      7435\n      0.036304\n    \n    \n      to\n      6923\n      0.033804\n    \n    \n      and\n      6290\n      0.030713\n    \n    \n      of\n      6146\n      0.030010\n    \n    \n      her\n      3747\n      0.018296\n    \n  \n\n\n\n\n\nm2.sort_values('p', ascending=False).head()\n\n\n\n\n\n  \n    \n      \n      \n      n\n      p\n    \n    \n      w0\n      w1\n      \n      \n    \n  \n  \n    \n      of\n      the\n      857\n      0.004185\n    \n    \n      to\n      be\n      814\n      0.003975\n    \n    \n      in\n      the\n      682\n      0.003330\n    \n    \n      mrs\n      <s>\n      530\n      0.002588\n    \n    \n      it\n      was\n      498\n      0.002432\n    \n  \n\n\n\n\n\nm3.sort_values('p', ascending=False).head(15)\n\n\n\n\n\n  \n    \n      \n      \n      \n      n\n      p\n    \n    \n      w0\n      w1\n      w2\n      \n      \n    \n  \n  \n    \n      mrs\n      <s>\n      <s>\n      530\n      0.002588\n    \n    \n      it\n      <s>\n      <s>\n      369\n      0.001802\n    \n    \n      her\n      <s>\n      <s>\n      241\n      0.001177\n    \n    \n      him\n      <s>\n      <s>\n      229\n      0.001118\n    \n    \n      mr\n      <s>\n      <s>\n      179\n      0.000874\n    \n    \n      you\n      <s>\n      <s>\n      172\n      0.000840\n    \n    \n      them\n      <s>\n      <s>\n      163\n      0.000796\n    \n    \n      me\n      <s>\n      <s>\n      162\n      0.000791\n    \n    \n      elinor\n      <s>\n      <s>\n      119\n      0.000581\n    \n    \n      i\n      am\n      sure\n      107\n      0.000522\n    \n    \n      she\n      could\n      not\n      93\n      0.000454\n    \n    \n      marianne\n      <s>\n      <s>\n      90\n      0.000439\n    \n    \n      again\n      <s>\n      <s>\n      85\n      0.000415\n    \n    \n      as\n      soon\n      as\n      82\n      0.000400\n    \n    \n      all\n      <s>\n      <s>\n      82\n      0.000400\n    \n  \n\n\n\n\n\n\nCompute conditional probabilities\n\\(p(w_1|w_0) = p(w_0, w_1) / p(w_0)\\)\n\\(p(w_2|w_0,w_1) = p(w_0, w_1, w_2) / p(w_0, w_1)\\)\n\n# m2['cp'] = (m2.n / m1.n).sort_values().to_frame('p').sort_index()\n\n\nm2['m1n'] = m1.n\nm2['cp'] = m2.n / m2.m1n\n\n\nm2\n\n\n\n\n\n  \n    \n      \n      \n      n\n      p\n      m1n\n      cp\n    \n    \n      w0\n      w1\n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      1760\n      1\n      0.000005\n      3\n      0.333333\n    \n    \n      1785\n      1\n      0.000005\n      3\n      0.333333\n    \n    \n      ends\n      1\n      0.000005\n      3\n      0.333333\n    \n    \n      15\n      1784\n      1\n      0.000005\n      1\n      1.000000\n    \n    \n      16\n      1810\n      1\n      0.000005\n      1\n      1.000000\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      zealous\n      attention\n      1\n      0.000005\n      4\n      0.250000\n    \n    \n      officer\n      1\n      0.000005\n      4\n      0.250000\n    \n    \n      on\n      1\n      0.000005\n      4\n      0.250000\n    \n    \n      zealously\n      active\n      1\n      0.000005\n      2\n      0.500000\n    \n    \n      discharging\n      1\n      0.000005\n      2\n      0.500000\n    \n  \n\n77458 rows × 4 columns\n\n\n\n\n# m3['cp'] = (m3.n / m2.n).sort_values().to_frame('p').sort_index()\n\n\nm3['m2n'] = m2.n\nm3['cp'] = m3.n / m3.m2n\n\n\nm3.sort_values('cp', ascending=False)\n\n\n\n\n\n  \n    \n      \n      \n      \n      n\n      p\n      m2n\n      cp\n    \n    \n      w0\n      w1\n      w2\n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      1760\n      married\n      1\n      0.000005\n      1\n      1.000000\n    \n    \n      met\n      mrs\n      <s>\n      2\n      0.000010\n      2\n      1.000000\n    \n    \n      middletons\n      dining\n      at\n      1\n      0.000005\n      1\n      1.000000\n    \n    \n      as\n      much\n      1\n      0.000005\n      1\n      1.000000\n    \n    \n      arrived\n      so\n      1\n      0.000005\n      1\n      1.000000\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      of\n      the\n      household\n      1\n      0.000005\n      857\n      0.001167\n    \n    \n      hours\n      1\n      0.000005\n      857\n      0.001167\n    \n    \n      higher\n      1\n      0.000005\n      857\n      0.001167\n    \n    \n      heavy\n      1\n      0.000005\n      857\n      0.001167\n    \n    \n      agitation\n      1\n      0.000005\n      857\n      0.001167\n    \n  \n\n154242 rows × 4 columns\n\n\n\n\nm2.cp.unstack(fill_value=0)\n\n\n\n\n\n  \n    \n      w1\n      1\n      15\n      16\n      1760\n      1784\n      1785\n      1787\n      1789\n      1791\n      1800\n      ...\n      your\n      yours\n      yourself\n      yourselves\n      youth\n      youthful\n      z\n      zeal\n      zealous\n      zealously\n    \n    \n      w0\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      0.0\n      0.0\n      0.0\n      0.333333\n      0.0\n      0.333333\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      15\n      0.0\n      0.0\n      0.0\n      0.000000\n      1.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      16\n      0.0\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      1760\n      0.0\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      1784\n      0.0\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      youthful\n      0.0\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      z\n      0.0\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      zeal\n      0.0\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      zealous\n      0.0\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      zealously\n      0.0\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n8238 rows × 8194 columns\n\n\n\n\nm3.cp.unstack(fill_value=0)\n\n\n\n\n\n  \n    \n      \n      w2\n      1\n      15\n      16\n      1760\n      1784\n      1785\n      1787\n      1789\n      1791\n      1800\n      ...\n      your\n      yours\n      yourself\n      yourselves\n      youth\n      youthful\n      z\n      zeal\n      zealous\n      zealously\n    \n    \n      w0\n      w1\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      1760\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      1785\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      ends\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      15\n      1784\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      16\n      1810\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      zealous\n      attention\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      officer\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      on\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      zealously\n      active\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      discharging\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n77458 rows × 8079 columns"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V1.html#predict-sentences",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V1.html#predict-sentences",
    "title": "Inferring Language Models v1",
    "section": "Predict Sentences",
    "text": "Predict Sentences\n\ndef predict_sentence2(sent_str, M):\n    \n    # Model order\n    n = len(M.index.names)\n    \n    # Parse sentence into tokens and normalize string\n    S = sent_str.lower().split()\n    S = [re.sub(r'[\\W_]+', '', s) for s in S]\n    S = S + ['<s>' for i in range(n-1)]\n    N = len(S)\n    \n    # Generate ngram keys \n    ngrams = []\n    offset = n - 1\n    for i in range(offset, len(S)):\n        ngram = []\n        w = S[i]\n        for j in range(n):\n            ngram.append(S[i-j])\n        ngram.reverse()\n        ngrams.append(ngram)\n        \n    # Get smoothing -- HACK\n    smooth = M.p.min()\n            \n    # Compute the probability of the sentence\n    log_prob = 0\n    for ngram in ngrams:\n        try:\n            p_ngram = M.loc[tuple(ngram)].p\n        except KeyError:\n            p_ngram = smooth\n        log_prob += np.log(p_ngram)\n    p = np.exp(log_prob)\n    pp = round(p**(-1/N)) \n    \n    return([n, sent_str, p, pp])\n\n\ntest_sentences = \"\"\"\nI love you\nI love cars\nI want to\nAnne said to\nsaid to her\nsaid to him\nshe read the\n\"\"\".split('\\n')[1:-1]\npdata = []\nfor S in test_sentences:\n    pdata.append(predict_sentence2(S, m1))\n    pdata.append(predict_sentence2(S, m2))\n    pdata.append(predict_sentence2(S, m3))\nR1 = pd.DataFrame(pdata, columns=['model','sent_str', 'p', 'pp']).set_index(['sent_str','model'])\n\n\ncols = [1,2,3]\npd.concat([R1.p.unstack().rename(columns={col:f\"p{col}\" for col in cols}), \n           R1.pp.unstack().rename(columns={col:f\"pp{col}\" for col in cols} )], axis=1).style.background_gradient()\n\n\n\n\n  \n    \n      model\n      p1\n      p2\n      p3\n      pp1\n      pp2\n      pp3\n    \n    \n      sent_str\n       \n       \n       \n       \n       \n       \n    \n  \n  \n    \n      Anne said to\n      0.000000\n      0.000000\n      0.000000\n      163\n      2142\n      814\n    \n    \n      I love cars\n      0.000000\n      0.000000\n      0.000000\n      2847\n      5724\n      1537\n    \n    \n      I love you\n      0.000000\n      0.000000\n      0.000000\n      233\n      1201\n      549\n    \n    \n      I want to\n      0.000000\n      0.000000\n      0.000000\n      175\n      1776\n      814\n    \n    \n      said to her\n      0.000002\n      0.000000\n      0.000000\n      83\n      284\n      245\n    \n    \n      said to him\n      0.000001\n      0.000000\n      0.000000\n      125\n      381\n      229\n    \n    \n      she read the\n      0.000000\n      0.000000\n      0.000000\n      229\n      6438\n      1537"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V1.html#explore-pairs",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V1.html#explore-pairs",
    "title": "Inferring Language Models v1",
    "section": "Explore Pairs",
    "text": "Explore Pairs\n\ndef explore_pairs(list1, list2):\n    global m2\n    test_pairs = []\n    for pair in zip(list1, list2):\n        try:\n            m2.loc[pair]\n            test_pairs.append(pair)\n        except:\n            pass\n    return m2.loc[test_pairs].cp.unstack(fill_value=0).style.background_gradient()\n\n\nexplore_pairs(['he','she','it','anne','wentworth'], ['is','had','was','did','felt','thought','looked','said','saw'])\n\n\n\n\n  \n    \n      w1\n      is\n      had\n      was\n      did\n    \n    \n      w0\n       \n       \n       \n       \n    \n  \n  \n    \n      he\n      0.057803\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      she\n      0.000000\n      0.148967\n      0.000000\n      0.000000\n    \n    \n      it\n      0.000000\n      0.000000\n      0.178175\n      0.000000\n    \n    \n      anne\n      0.000000\n      0.000000\n      0.000000\n      0.015905\n    \n  \n\n\n\n\nexplore_pairs(['he', 'she', 'it'], ['said','felt'])\n\n\n\n\n  \n    \n      w1\n      said\n      felt\n    \n    \n      w0\n       \n       \n    \n  \n  \n    \n      he\n      0.016378\n      0.000000\n    \n    \n      she\n      0.000000\n      0.018123"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V1.html#generate-text",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V1.html#generate-text",
    "title": "Inferring Language Models v1",
    "section": "Generate Text",
    "text": "Generate Text\nWe use stupid back-off to account for missing ngrams.\n\ndef generate_text(start_word='she', n=250):\n    words = [start_word]\n    for i in range(n):\n        if len(words) == 1:\n            w = m2.loc[start_word].p\n            next_word = m2.loc[start_word].sample(weights=w).index.values[0]\n        elif len(words) > 1:\n            bg = tuple(words[-2:])\n            try:\n                w = m3.loc[bg].p\n                next_word = m3.loc[bg].sample(weights=w).index.values[0]\n            except KeyError:\n                ug = bg[1]\n                if ug == '<s>':\n                    next_word = m1.sample(weights=m1.p).index[0]\n                else:\n                    w = m2.loc[ug].p\n                    next_word = m2.loc[ug].sample(weights=w).index.values[0]\n\n        # Some words are returned as single item tuples\n        if isinstance(next_word, tuple):\n            next_word = next_word[0]\n        \n        words.append(next_word)\n        \n    text = ' '.join(words)\n    text = text.replace(' <s> <s>', '.') + '.'\n    text = text.upper() # To give that telegraph message look :-)\n    print(text)\n\n\ngenerate_text('the')\n\nTHE MORE I SAW YOU WEAR A RING BEFORE EDWARD. APPARENTLY VERY LITTLE BESIDES SENDING AWAY SOME OF THESE SUPERIOR BLESSINGS COULD BE MORE LOST TO ME AND IN ENDEAVOURING TO APPEAR IN THEIR PARSONAGE BY MICHAELMAS AND SHE HAS MADE EVERYTHING BELONGING TO A BELIEF OF BEING SEEN WITH HIM. HER NERVES SUFFICIENTLY TO FEEL IT A MORE WILLING MIND IN HER NATURAL TONE OF CORDIALITY I BEG YOUR PARDON MR. TWO THOUSAND POUNDS WHICH HAD DESERTED AND MELANCHOLY LOOKING ROOMS AND A HALF IS A GOOD WALKER OH YES. A MIXTURE OF THOSE MOST CONCERNED. ELINOR HAD MET. SEEMED NO OTHERWISE INTELLIGIBLE. SISTER OR WHAT DIABOLICAL MOTIVE YOU MAY AS WELL SATISFIED WITH HER LAUGHED WITH HER STATE OF SPIRITS. BE DONE IN RAPID MOMENTS CAPTAIN BENWICK WHICH HE COMMUNICATED TO HIS PRESENT CURACY AND OBTAINING THAT OF MY FAMILY. KNOW HOW TO HOLD THE BLESSINGS OF HEALTH AND LOOKS OR LADY MIDDLETON WAS NOT LIKELY TO BE DONE. AN ACCOUNT OF HER HEART PROPHESIED SOME MISCHANCE TO DAMP THE PERFECTION OF HER MOTHER. LIBERTY WITH YOU. AND THOSE WHO HAVE NOT BEEN VERY AGREEABLE TO THEIR FORMER GOOD UNDERSTANDING WAS COMPLETELY A GENTLEMAN OF CONSEQUENCE. BY. PRODUCED A VERY PLEASANT ADDITION TO THESE. OF THE MIDDLETONS AND PALMERS HOW AM I BE OF THE HOUSE FOR THEIR MEETING.\n\n\n\ngenerate_text('she')\n\nSHE I MIGHT SEE IT TO EDWARD SHE BELIEVED HER CAPABLE OF THE GENTLEMAN. THE NUMBER OF HER SICK CHILD IS ALWAYS SUPPOSING THAT PERHAPS. SHE ADDED IN A MANNER FORCED ON ME BY THE BYE HAD A VERY DIFFERENT EFFECT SAW NOTHING TO BE HOWEVER DISAGREEABLE YET TOO RAPID. HILL WHICH PARTED UPPERCROSS AND IN THE FAMILY AT THE FORM OF IT FOR IT HAS CARRIED AWAY FROM HIS DAUGHTER S WISHES COULD BE CANVASSED ONLY IN GENERAL SO EARNESTLY GRATEFUL SO FULL OF AFFECTION AND INFLUENCE. DEAR COUSIN YOU ARE CERTAIN OF MEETING THEM AND NOBODY BUT LUCY S BEAU IS QUITE UNDERSTOOD I GIVE THEM ALL WITH THE SIZE AND MENTAL ALACRITY DID NOT MAKE MORE THAN A THICK SCREEN OF THEM ALL INTO THE SICK ROOM THAN MARIANNE RINGING THE BELL. FULL OF MEANING TO BE CONTRADICTED. DESIRE YOU WILL SOON BE BACK AGAIN DIRECTLY. WILL BE THE MEANING OF IT. AS THIS CANNOT BE FORCED INTO PRUDENCE IN HER NOTIONS OF A DIFFERENT CAUSE NO LESS CONTENTED WITH THE HARVILLES WHILE THE EYES OF SENSIBLE PEOPLE BY ACTING LIKE A MAN WHO FOR MORE THAN EQUAL ANY OF THEM ALL THAT REMAINS. TELL ME OF IT AT ALL WONDER AT HERSELF FOR BEING SO GENUINELY AND UNAFFECTEDLY ILL NATURED OR ILL BRED AS HE WAS SILENT. SWEET THINGS THAT THEY ONLY HAVE A LITTLE COLD ALREADY."
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V1.html#save",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V1.html#save",
    "title": "Inferring Language Models v1",
    "section": "Save",
    "text": "Save\n\npath_prefix = f\"{output_dir}/austen-combo\"\nvocab.to_csv(f\"{path_prefix}-VOCAB-v1.csv\", index=True)\ntokens.to_csv(f\"{path_prefix}-TOKENS-v1.csv\", index=True)\nm1.to_csv(f\"{path_prefix}-M1-v1.csv\", index=True)\nm2.to_csv(f\"{path_prefix}-M2-v1.csv\", index=True)\nm3.to_csv(f\"{path_prefix}-M3-v1.csv\", index=True)"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V2.html",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V2.html",
    "title": "Inferring Language Models (v2)",
    "section": "",
    "text": "Set Up\nProblem: These texts treat Mr. and Mrs. as sentences. Since sentence breaks are represented in the OHCO, to fix this requires reparsing the source text into the TOKEN table with the abbreviation periods removed.\nWe use stupid back-off to account for missing ngrams.\n“What do we do with words that are in our vocabulary (they are not unknown words) but appear in a test set in an unseen context (for example they appear after a word they never appeared after in training)?” (Jurafsky and Martin)\nHow to generalize ngram models when training data lacks certain instances of ngrams.\nNote that smoothing is expensive and not very effective."
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V2.html#import-libraries",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V2.html#import-libraries",
    "title": "Inferring Language Models (v2)",
    "section": "Import libraries",
    "text": "Import libraries\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport seaborn as sns\nfrom IPython.core.display import HTML\n\n\nsns.set()"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V2.html#configure",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V2.html#configure",
    "title": "Inferring Language Models (v2)",
    "section": "Configure",
    "text": "Configure\n\nimport configparser\n\n\nconfig = configparser.ConfigParser()\nconfig.read(\"../env.ini\")\ndata_dir = config['DEFAULT']['data_home']\noutput_dir = config['DEFAULT']['output_dir']\n\n\nOHCO = ['book_id', 'chap_num', 'para_num', 'sent_num', 'token_num']\ntext_file = f'{output_dir}/austen-combo.csv'"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V2.html#prepare-test-data",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V2.html#prepare-test-data",
    "title": "Inferring Language Models (v2)",
    "section": "Prepare test data",
    "text": "Prepare test data\n\nDefine list of test sentences\n\n# Some paragraphs from Austen's _Emma_\ntest_sentences = \"\"\"\nThe event had every promise of happiness for her friend \nMr Weston was a man of unexceptionable character easy fortune suitable age and pleasant manners\nand there was some satisfaction in considering with what self-denying generous friendship she had always wished and promoted the match\nbut it was a black morning's work for her \nThe want of Miss Taylor would be felt every hour of every day \nShe recalled her past kindness the kindness the affection of sixteen years \nhow she had taught and how she had played with her from five years old \nhow she had devoted all her powers to attach and amuse her in health \nand how nursed her through the various illnesses of childhood \nA large debt of gratitude was owing here \nbut the intercourse of the last seven years \nthe equal footing and perfect unreserve which had soon followed Isabella's marriage \non their being left to each other was yet a dearer tenderer recollection \nShe had been a friend and companion such as few possessed intelligent well-informed useful gentle \nknowing all the ways of the family \ninterested in all its concerns \nand peculiarly interested in herself in every pleasure every scheme of hers \none to whom she could speak every thought as it arose \nand who had such an affection for her as could never find fault \nHow was she to bear the change \nIt was true that her friend was going only half a mile from them \nbut Emma was aware that great must be the difference between a Mrs Weston \nonly half a mile from them \nand a Miss Taylor in the house \nand with all her advantages natural and domestic \nshe was now in great danger of suffering from intellectual solitude \nShe dearly loved her father \nbut he was no companion for her \nHe could not meet her in conversation rational or playful \nThe evil of the actual disparity in their ages\nand Mr Woodhouse had not married early\nwas much increased by his constitution and habits \nfor having been a valetudinarian all his life \nwithout activity of mind or body \nhe was a much older man in ways than in years \nand though everywhere beloved for the friendliness of his heart and his amiable temper \nhis talents could not have recommended him at any time \nHer sister though comparatively but little removed by matrimony \nbeing settled in London only sixteen miles off was much beyond her daily reach \nand many a long October and November evening must be struggled through at Hartfield \nbefore Christmas brought the next visit from Isabella and her husband \nand their little children to fill the house and give her pleasant society again \n\"\"\".split('\\n')[1:-1]\n\n\n\nConvert list to data frame\n\nTS = pd.DataFrame(dict(sent_str=test_sentences))\nTS.index.name = 'sent_num'\n\n\n\nTokenize\n\nTEST_TOKENS = TS.sent_str.apply(lambda x: pd.Series(x.split())).stack().to_frame('token_str')\nTEST_TOKENS.index.names = ['sent_num', 'token_num']\nTEST_TOKENS['term_str'] = TEST_TOKENS.token_str.str.lower().str.replace(r'\\W+', '', regex=True)\n\n\nTEST_TOKENS.head()\n\n\n\n\n\n  \n    \n      \n      \n      token_str\n      term_str\n    \n    \n      sent_num\n      token_num\n      \n      \n    \n  \n  \n    \n      0\n      0\n      The\n      the\n    \n    \n      1\n      event\n      event\n    \n    \n      2\n      had\n      had\n    \n    \n      3\n      every\n      every\n    \n    \n      4\n      promise\n      promise\n    \n  \n\n\n\n\n\n\nConvert to Corpus Object\n\ntest_corpus = Corpus(TEST_TOKENS)\ntest_corpus.extract_vocab()\ntest_corpus.extract_hapax_data() # No really used ...\n\n\n# test_corpus.TOKENS.loc[3]\n\n\n# test_corpus.UNK"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V2.html#generate-test-data",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V2.html#generate-test-data",
    "title": "Inferring Language Models (v2)",
    "section": "Generate test data",
    "text": "Generate test data\n\nLMT = NgramModel(test_corpus, unk_list=training_corpus.UNK)\nLMT.make_index()\nLMT.make_model()\n\nConverting tokens to list of sentences ...\nConverting sentences to tokens with special tokens ...\nReplacing rare words with UNK ...\nCreating ngrams from offset lists ...\nCreating 3-gram model ...\nCreating 2-gram model ...\nCreating 1-gram model ...\nGetting lo freqs for level 2\nGetting lo freqs for level 3"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V2.html#compare-models",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V2.html#compare-models",
    "title": "Inferring Language Models (v2)",
    "section": "Compare Models",
    "text": "Compare Models\n\nclass ModelComparator():\n    \n    def __init__(self, MODEL:NgramModel, TEST:NgramModel, w=3):\n        \"\"\"Compare two models, such as training and test models.\"\"\"\n        \n        self.MODEL = MODEL\n        self.TEST = TEST\n        self.w = w\n                    \n    def compare(self):\n        \"\"\"Join model to data.\"\"\" \n\n        M = self.MODEL.M[self.w]\n        T = self.TEST.M[self.w]\n\n        # We use a right join to keep test data that was not found in training\n        self.comp = M.join(T, how='right', rsuffix='_test')\n\n        # Compute differences\n        self.ng_intersection = self.comp[~self.comp.n.isna()].index.to_list()\n        self.ng_difference = self.comp[self.comp.n.isna()].index.to_list()\n        self.hits = round(len(self.ng_intersection) / len(self.comp), 2)\n\n        # Handle OOV words\n        unk_key = tuple('<UNK>' for _ in range(self.w))\n        unk_add_n = self.comp[self.comp.n.isna()].n_test.sum()\n        self.comp.at[unk_key, 'n_test'] += unk_add_n # Update number of UNK items\n        self.comp = self.comp[~self.comp.n.isna()] # Remove misses\n                        \n        # Compute cross-entropy and perplexity of model\n        N = self.comp.n_test.sum()\n        if self.w > 1: \n            i_col = 'ci'\n        else:\n            i_col = 'i'\n        self.LL = (self.comp[i_col] * self.comp.n_test).sum()\n        self.CH = self.LL / N\n        self.PP = np.power(2, self.CH)\n\n\nC = {}\nfor i in range(1, 4):\n    C[i] = ModelComparator(LM, LMT, i)\n    C[i].compare()\n\n\nC[3].comp.style.background_gradient()\n\n\n\n\n  \n    \n       \n       \n       \n      n\n      p\n      i\n      h\n      cp\n      ci\n      ch\n      n_test\n    \n    \n      w0\n      w1\n      w2\n       \n       \n       \n       \n       \n       \n       \n       \n    \n  \n  \n    \n      \n      \n      a\n      153.000000\n      0.000411\n      11.249385\n      0.004621\n      0.011230\n      6.476475\n      0.072732\n      1.000000\n    \n    \n      and\n      1440.000000\n      0.003866\n      8.014919\n      0.030986\n      0.105696\n      3.242010\n      0.342667\n      10.000000\n    \n    \n      before\n      8.000000\n      0.000021\n      15.506772\n      0.000333\n      0.000587\n      10.733863\n      0.006303\n      1.000000\n    \n    \n      being\n      2.000000\n      0.000005\n      17.506772\n      0.000094\n      0.000147\n      12.733863\n      0.001869\n      1.000000\n    \n    \n      but\n      941.000000\n      0.002526\n      8.628722\n      0.021799\n      0.069069\n      3.855812\n      0.266318\n      4.000000\n    \n    \n      for\n      226.000000\n      0.000607\n      10.686593\n      0.006484\n      0.016588\n      5.913684\n      0.098098\n      1.000000\n    \n    \n      he\n      577.000000\n      0.001549\n      9.334345\n      0.014460\n      0.042352\n      4.561435\n      0.193185\n      2.000000\n    \n    \n      her\n      167.000000\n      0.000448\n      11.123068\n      0.004987\n      0.012258\n      6.350158\n      0.077839\n      1.000000\n    \n    \n      his\n      120.000000\n      0.000322\n      11.599882\n      0.003737\n      0.008808\n      6.826972\n      0.060132\n      1.000000\n    \n    \n      how\n      124.000000\n      0.000333\n      11.552576\n      0.003846\n      0.009102\n      6.779666\n      0.061706\n      3.000000\n    \n    \n      it\n      444.000000\n      0.001192\n      9.712357\n      0.011577\n      0.032590\n      4.939447\n      0.160974\n      1.000000\n    \n    \n      knowing\n      3.000000\n      0.000008\n      16.921810\n      0.000136\n      0.000220\n      12.148900\n      0.002675\n      1.000000\n    \n    \n      mr\n      75.000000\n      0.000201\n      12.277954\n      0.002472\n      0.005505\n      7.505044\n      0.041315\n      1.000000\n    \n    \n      on\n      37.000000\n      0.000099\n      13.297319\n      0.001321\n      0.002716\n      8.524409\n      0.023151\n      1.000000\n    \n    \n      one\n      38.000000\n      0.000102\n      13.258845\n      0.001353\n      0.002789\n      8.485935\n      0.023669\n      1.000000\n    \n    \n      only\n      13.000000\n      0.000035\n      14.806333\n      0.000517\n      0.000954\n      10.033423\n      0.009574\n      1.000000\n    \n    \n      she\n      706.000000\n      0.001895\n      9.043248\n      0.017141\n      0.051820\n      4.270338\n      0.221290\n      4.000000\n    \n    \n      the\n      514.000000\n      0.001380\n      9.501148\n      0.013111\n      0.037728\n      4.728238\n      0.178385\n      3.000000\n    \n    \n      was\n      31.000000\n      0.000083\n      13.552576\n      0.001128\n      0.002275\n      8.779666\n      0.019977\n      1.000000\n    \n    \n      without\n      8.000000\n      0.000021\n      15.506772\n      0.000333\n      0.000587\n      10.733863\n      0.006303\n      1.000000\n    \n    \n      \n      a\n      large\n      2.000000\n      0.000005\n      17.506772\n      0.000094\n      0.013072\n      6.257388\n      0.081796\n      1.000000\n    \n    \n      and\n      a\n      13.000000\n      0.000035\n      14.806333\n      0.000517\n      0.009028\n      6.791413\n      0.061311\n      1.000000\n    \n    \n      how\n      11.000000\n      0.000030\n      15.047341\n      0.000444\n      0.007639\n      7.032421\n      0.053720\n      1.000000\n    \n    \n      many\n      2.000000\n      0.000005\n      17.506772\n      0.000094\n      0.001389\n      9.491853\n      0.013183\n      1.000000\n    \n    \n      mr\n      12.000000\n      0.000032\n      14.921810\n      0.000481\n      0.008333\n      6.906891\n      0.057557\n      1.000000\n    \n    \n      their\n      5.000000\n      0.000013\n      16.184844\n      0.000217\n      0.003472\n      8.169925\n      0.028368\n      1.000000\n    \n    \n      there\n      17.000000\n      0.000046\n      14.419310\n      0.000658\n      0.011806\n      6.404390\n      0.075607\n      1.000000\n    \n    \n      though\n      33.000000\n      0.000089\n      13.462378\n      0.001193\n      0.022917\n      5.447459\n      0.124838\n      1.000000\n    \n    \n      who\n      12.000000\n      0.000032\n      14.921810\n      0.000481\n      0.008333\n      6.906891\n      0.057557\n      1.000000\n    \n    \n      with\n      26.000000\n      0.000070\n      13.806333\n      0.000964\n      0.018056\n      5.791413\n      0.104567\n      1.000000\n    \n    \n      but\n      he\n      34.000000\n      0.000091\n      13.419310\n      0.001225\n      0.036132\n      4.790588\n      0.173092\n      1.000000\n    \n    \n      it\n      56.000000\n      0.000150\n      12.699418\n      0.001909\n      0.059511\n      4.070696\n      0.242252\n      1.000000\n    \n    \n      the\n      51.000000\n      0.000137\n      12.834347\n      0.001757\n      0.054198\n      4.205626\n      0.227935\n      1.000000\n    \n    \n      he\n      could\n      14.000000\n      0.000038\n      14.699418\n      0.000553\n      0.024263\n      5.365073\n      0.130175\n      1.000000\n    \n    \n      was\n      82.000000\n      0.000220\n      12.149220\n      0.002675\n      0.142114\n      2.814876\n      0.400034\n      1.000000\n    \n    \n      her\n      sister\n      3.000000\n      0.000008\n      16.921810\n      0.000136\n      0.017964\n      5.798742\n      0.104169\n      1.000000\n    \n    \n      how\n      she\n      2.000000\n      0.000005\n      17.506772\n      0.000094\n      0.016129\n      5.954196\n      0.096035\n      2.000000\n    \n    \n      was\n      3.000000\n      0.000008\n      16.921810\n      0.000136\n      0.024194\n      5.369234\n      0.129901\n      1.000000\n    \n    \n      it\n      was\n      180.000000\n      0.000483\n      11.014919\n      0.005323\n      0.405405\n      1.302563\n      0.528066\n      1.000000\n    \n    \n      knowing\n      all\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.333333\n      1.584963\n      0.528321\n      1.000000\n    \n    \n      on\n      their\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.027027\n      5.209453\n      0.140796\n      1.000000\n    \n    \n      she\n      had\n      105.000000\n      0.000282\n      11.792527\n      0.003324\n      0.148725\n      2.749279\n      0.408887\n      1.000000\n    \n    \n      was\n      115.000000\n      0.000309\n      11.661282\n      0.003600\n      0.162890\n      2.618034\n      0.426450\n      1.000000\n    \n    \n      the\n      event\n      2.000000\n      0.000005\n      17.506772\n      0.000094\n      0.003883\n      8.008429\n      0.031101\n      1.000000\n    \n    \n      evil\n      2.000000\n      0.000005\n      17.506772\n      0.000094\n      0.003883\n      8.008429\n      0.031101\n      1.000000\n    \n    \n      a\n      friend\n      and\n      2.000000\n      0.000005\n      17.506772\n      0.000094\n      0.105263\n      3.247928\n      0.341887\n      1.000000\n    \n    \n      man\n      of\n      23.000000\n      0.000062\n      13.983210\n      0.000863\n      0.273810\n      1.868755\n      0.511683\n      1.000000\n    \n    \n      mile\n      from\n      2.000000\n      0.000005\n      17.506772\n      0.000094\n      0.285714\n      1.807355\n      0.516387\n      2.000000\n    \n    \n      affection\n      for\n      her\n      4.000000\n      0.000011\n      16.506772\n      0.000177\n      0.173913\n      2.523562\n      0.438880\n      1.000000\n    \n    \n      an\n      affection\n      for\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.200000\n      2.321928\n      0.464386\n      1.000000\n    \n    \n      and\n      her\n      husband\n      3.000000\n      0.000008\n      16.921810\n      0.000136\n      0.023438\n      5.415037\n      0.126915\n      1.000000\n    \n    \n      many\n      a\n      3.000000\n      0.000008\n      16.921810\n      0.000136\n      0.333333\n      1.584963\n      0.528321\n      1.000000\n    \n    \n      there\n      was\n      4.000000\n      0.000011\n      16.506772\n      0.000177\n      0.111111\n      3.169925\n      0.352214\n      1.000000\n    \n    \n      who\n      had\n      4.000000\n      0.000011\n      16.506772\n      0.000177\n      0.129032\n      2.954196\n      0.381187\n      1.000000\n    \n    \n      with\n      all\n      2.000000\n      0.000005\n      17.506772\n      0.000094\n      0.035088\n      4.832890\n      0.169575\n      1.000000\n    \n    \n      any\n      time\n      \n      2.000000\n      0.000005\n      17.506772\n      0.000094\n      0.250000\n      2.000000\n      0.500000\n      1.000000\n    \n    \n      arose\n      \n      \n      3.000000\n      0.000008\n      16.921810\n      0.000136\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      at\n      any\n      time\n      7.000000\n      0.000019\n      15.699418\n      0.000295\n      0.437500\n      1.192645\n      0.521782\n      1.000000\n    \n    \n      being\n      left\n      to\n      2.000000\n      0.000005\n      17.506772\n      0.000094\n      0.400000\n      1.321928\n      0.528771\n      1.000000\n    \n    \n      body\n      \n      \n      10.000000\n      0.000027\n      15.184844\n      0.000408\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      but\n      he\n      was\n      14.000000\n      0.000038\n      14.699418\n      0.000553\n      0.233333\n      2.099536\n      0.489892\n      1.000000\n    \n    \n      it\n      was\n      32.000000\n      0.000086\n      13.506772\n      0.001160\n      0.376471\n      1.409391\n      0.530594\n      1.000000\n    \n    \n      change\n      \n      \n      8.000000\n      0.000021\n      15.506772\n      0.000333\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      companion\n      for\n      her\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.500000\n      1.000000\n      0.500000\n      1.000000\n    \n    \n      concerns\n      \n      \n      2.000000\n      0.000005\n      17.506772\n      0.000094\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      could\n      not\n      have\n      10.000000\n      0.000027\n      15.184844\n      0.000408\n      0.032154\n      4.958843\n      0.159448\n      1.000000\n    \n    \n      danger\n      of\n      suffering\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.111111\n      3.169925\n      0.352214\n      1.000000\n    \n    \n      day\n      \n      \n      45.000000\n      0.000121\n      13.014919\n      0.001572\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      early\n      \n      \n      3.000000\n      0.000008\n      16.921810\n      0.000136\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      every\n      day\n      \n      11.000000\n      0.000030\n      15.047341\n      0.000444\n      0.314286\n      1.669851\n      0.524810\n      1.000000\n    \n    \n      hour\n      of\n      3.000000\n      0.000008\n      16.921810\n      0.000136\n      0.750000\n      0.415037\n      0.311278\n      1.000000\n    \n    \n      evil\n      of\n      the\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.166667\n      2.584963\n      0.430827\n      1.000000\n    \n    \n      family\n      \n      \n      30.000000\n      0.000081\n      13.599882\n      0.001095\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      father\n      \n      \n      10.000000\n      0.000027\n      15.184844\n      0.000408\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      fault\n      \n      \n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      for\n      having\n      been\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.071429\n      3.807355\n      0.271954\n      1.000000\n    \n    \n      her\n      \n      17.000000\n      0.000046\n      14.419310\n      0.000658\n      0.141667\n      2.819428\n      0.399419\n      2.000000\n    \n    \n      as\n      3.000000\n      0.000008\n      16.921810\n      0.000136\n      0.025000\n      5.321928\n      0.133048\n      1.000000\n    \n    \n      friend\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.008333\n      6.906891\n      0.057557\n      1.000000\n    \n    \n      friend\n      \n      \n      24.000000\n      0.000064\n      13.921810\n      0.000897\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      friendliness\n      of\n      his\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      from\n      them\n      \n      2.000000\n      0.000005\n      17.506772\n      0.000094\n      0.250000\n      2.000000\n      0.500000\n      2.000000\n    \n    \n      gentle\n      \n      \n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      habits\n      \n      \n      2.000000\n      0.000005\n      17.506772\n      0.000094\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      had\n      been\n      a\n      19.000000\n      0.000051\n      14.258845\n      0.000727\n      0.050132\n      4.318127\n      0.216476\n      1.000000\n    \n    \n      such\n      an\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.090909\n      3.459432\n      0.314494\n      1.000000\n    \n    \n      half\n      a\n      mile\n      3.000000\n      0.000008\n      16.921810\n      0.000136\n      0.150000\n      2.736966\n      0.410545\n      2.000000\n    \n    \n      having\n      been\n      a\n      4.000000\n      0.000011\n      16.506772\n      0.000177\n      0.190476\n      2.392317\n      0.455680\n      1.000000\n    \n    \n      he\n      could\n      not\n      22.000000\n      0.000059\n      14.047341\n      0.000830\n      0.305556\n      1.710493\n      0.522651\n      1.000000\n    \n    \n      was\n      a\n      7.000000\n      0.000019\n      15.699418\n      0.000295\n      0.027888\n      5.164189\n      0.144021\n      1.000000\n    \n    \n      health\n      \n      \n      6.000000\n      0.000016\n      15.921810\n      0.000256\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      her\n      \n      \n      241.000000\n      0.000647\n      10.593883\n      0.006855\n      1.000000\n      0.000000\n      0.000000\n      2.000000\n    \n    \n      father\n      \n      2.000000\n      0.000005\n      17.506772\n      0.000094\n      0.035088\n      4.832890\n      0.169575\n      1.000000\n    \n    \n      friend\n      \n      3.000000\n      0.000008\n      16.921810\n      0.000136\n      0.100000\n      3.321928\n      0.332193\n      1.000000\n    \n    \n      was\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.033333\n      4.906891\n      0.163563\n      1.000000\n    \n    \n      husband\n      \n      11.000000\n      0.000030\n      15.047341\n      0.000444\n      0.189655\n      2.398549\n      0.454897\n      1.000000\n    \n    \n      in\n      conversation\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.016949\n      5.882643\n      0.099706\n      1.000000\n    \n    \n      here\n      \n      \n      24.000000\n      0.000064\n      13.921810\n      0.000897\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      hers\n      \n      \n      10.000000\n      0.000027\n      15.184844\n      0.000408\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      his\n      constitution\n      and\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      heart\n      and\n      3.000000\n      0.000008\n      16.921810\n      0.000136\n      0.187500\n      2.415037\n      0.452820\n      1.000000\n    \n    \n      life\n      \n      2.000000\n      0.000005\n      17.506772\n      0.000094\n      0.333333\n      1.584963\n      0.528321\n      1.000000\n    \n    \n      hour\n      of\n      every\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.100000\n      3.321928\n      0.332193\n      1.000000\n    \n    \n      house\n      \n      \n      49.000000\n      0.000132\n      12.892063\n      0.001696\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      how\n      she\n      had\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.090909\n      3.459432\n      0.314494\n      3.000000\n    \n    \n      husband\n      \n      \n      18.000000\n      0.000048\n      14.336847\n      0.000693\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      in\n      the\n      house\n      14.000000\n      0.000038\n      14.699418\n      0.000553\n      0.020528\n      5.606273\n      0.115085\n      1.000000\n    \n    \n      intercourse\n      of\n      the\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.500000\n      1.000000\n      0.500000\n      1.000000\n    \n    \n      it\n      was\n      a\n      54.000000\n      0.000145\n      12.751885\n      0.001849\n      0.108434\n      3.205114\n      0.347543\n      1.000000\n    \n    \n      true\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.002008\n      8.960002\n      0.017992\n      1.000000\n    \n    \n      life\n      \n      \n      30.000000\n      0.000081\n      13.599882\n      0.001095\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      little\n      children\n      to\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      manners\n      \n      \n      6.000000\n      0.000016\n      15.921810\n      0.000256\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      marriage\n      \n      \n      10.000000\n      0.000027\n      15.184844\n      0.000408\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      match\n      \n      \n      10.000000\n      0.000027\n      15.184844\n      0.000408\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      matrimony\n      \n      \n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      much\n      increased\n      by\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      must\n      be\n      the\n      8.000000\n      0.000021\n      15.506772\n      0.000333\n      0.055556\n      4.169925\n      0.231663\n      1.000000\n    \n    \n      of\n      every\n      day\n      2.000000\n      0.000005\n      17.506772\n      0.000094\n      0.046512\n      4.426265\n      0.205873\n      1.000000\n    \n    \n      hers\n      \n      2.000000\n      0.000005\n      17.506772\n      0.000094\n      0.285714\n      1.807355\n      0.516387\n      1.000000\n    \n    \n      his\n      heart\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.003165\n      8.303781\n      0.026278\n      1.000000\n    \n    \n      suffering\n      from\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.333333\n      1.584963\n      0.528321\n      1.000000\n    \n    \n      the\n      family\n      18.000000\n      0.000048\n      14.336847\n      0.000693\n      0.021004\n      5.573226\n      0.117057\n      1.000000\n    \n    \n      last\n      7.000000\n      0.000019\n      15.699418\n      0.000295\n      0.008168\n      6.935796\n      0.056652\n      1.000000\n    \n    \n      old\n      \n      \n      4.000000\n      0.000011\n      16.506772\n      0.000177\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      one\n      to\n      whom\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.083333\n      3.584963\n      0.298747\n      1.000000\n    \n    \n      only\n      half\n      a\n      2.000000\n      0.000005\n      17.506772\n      0.000094\n      0.285714\n      1.807355\n      0.516387\n      2.000000\n    \n    \n      reach\n      \n      \n      2.000000\n      0.000005\n      17.506772\n      0.000094\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      recollection\n      \n      \n      3.000000\n      0.000008\n      16.921810\n      0.000136\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      she\n      could\n      speak\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.004545\n      7.781360\n      0.035370\n      1.000000\n    \n    \n      had\n      always\n      3.000000\n      0.000008\n      16.921810\n      0.000136\n      0.007299\n      7.098032\n      0.051810\n      1.000000\n    \n    \n      been\n      52.000000\n      0.000140\n      12.806333\n      0.001788\n      0.126521\n      2.982555\n      0.377355\n      1.000000\n    \n    \n      played\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.002433\n      8.682995\n      0.021127\n      1.000000\n    \n    \n      was\n      now\n      4.000000\n      0.000011\n      16.506772\n      0.000177\n      0.010724\n      6.543032\n      0.070167\n      1.000000\n    \n    \n      solitude\n      \n      \n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      some\n      satisfaction\n      in\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      temper\n      \n      \n      7.000000\n      0.000019\n      15.699418\n      0.000295\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      that\n      her\n      friend\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.025641\n      5.285402\n      0.135523\n      1.000000\n    \n    \n      the\n      affection\n      of\n      3.000000\n      0.000008\n      16.921810\n      0.000136\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      difference\n      between\n      3.000000\n      0.000008\n      16.921810\n      0.000136\n      0.272727\n      1.874469\n      0.511219\n      1.000000\n    \n    \n      evil\n      of\n      2.000000\n      0.000005\n      17.506772\n      0.000094\n      0.285714\n      1.807355\n      0.516387\n      1.000000\n    \n    \n      family\n      \n      7.000000\n      0.000019\n      15.699418\n      0.000295\n      0.189189\n      2.402098\n      0.454451\n      1.000000\n    \n    \n      friendliness\n      of\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      house\n      \n      17.000000\n      0.000046\n      14.419310\n      0.000658\n      0.149123\n      2.745427\n      0.409406\n      1.000000\n    \n    \n      and\n      14.000000\n      0.000038\n      14.699418\n      0.000553\n      0.122807\n      3.025535\n      0.371557\n      1.000000\n    \n    \n      intercourse\n      of\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      kindness\n      the\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.142857\n      2.807355\n      0.401051\n      1.000000\n    \n    \n      match\n      \n      4.000000\n      0.000011\n      16.506772\n      0.000177\n      0.666667\n      0.584963\n      0.389975\n      1.000000\n    \n    \n      want\n      of\n      4.000000\n      0.000011\n      16.506772\n      0.000177\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      ways\n      of\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.500000\n      1.000000\n      0.500000\n      1.000000\n    \n    \n      them\n      \n      \n      163.000000\n      0.000438\n      11.158044\n      0.004883\n      1.000000\n      0.000000\n      0.000000\n      2.000000\n    \n    \n      there\n      was\n      some\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.008475\n      6.882643\n      0.058327\n      1.000000\n    \n    \n      time\n      \n      \n      53.000000\n      0.000142\n      12.778852\n      0.001818\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      to\n      bear\n      the\n      3.000000\n      0.000008\n      16.921810\n      0.000136\n      0.428571\n      1.222392\n      0.523882\n      1.000000\n    \n    \n      each\n      other\n      15.000000\n      0.000040\n      14.599882\n      0.000588\n      0.937500\n      0.093109\n      0.087290\n      1.000000\n    \n    \n      fill\n      the\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.500000\n      1.000000\n      0.500000\n      1.000000\n    \n    \n      whom\n      she\n      2.000000\n      0.000005\n      17.506772\n      0.000094\n      0.166667\n      2.584963\n      0.430827\n      1.000000\n    \n    \n      was\n      a\n      man\n      3.000000\n      0.000008\n      16.921810\n      0.000136\n      0.018405\n      5.763766\n      0.106082\n      1.000000\n    \n    \n      much\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.006135\n      7.348728\n      0.045084\n      1.000000\n    \n    \n      now\n      in\n      2.000000\n      0.000005\n      17.506772\n      0.000094\n      0.047619\n      4.392317\n      0.209158\n      1.000000\n    \n    \n      she\n      to\n      2.000000\n      0.000005\n      17.506772\n      0.000094\n      0.181818\n      2.459432\n      0.447169\n      1.000000\n    \n    \n      whom\n      she\n      could\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.040000\n      4.643856\n      0.185754\n      1.000000\n    \n    \n      with\n      all\n      her\n      9.000000\n      0.000024\n      15.336847\n      0.000371\n      0.145161\n      2.784271\n      0.404168\n      1.000000\n    \n    \n      her\n      from\n      1.000000\n      0.000003\n      18.506772\n      0.000050\n      0.008772\n      6.832890\n      0.059938\n      1.000000\n    \n    \n      years\n      \n      \n      15.000000\n      0.000040\n      14.599882\n      0.000588\n      1.000000\n      0.000000\n      0.000000\n      3.000000\n    \n    \n      old\n      \n      2.000000\n      0.000005\n      17.506772\n      0.000094\n      0.333333\n      1.584963\n      0.528321\n      1.000000\n    \n    \n      \n      \n      \n      140423.000000\n      0.377003\n      1.407353\n      0.530576\n      1.000000\n      0.000000\n      0.000000\n      808.000000\n    \n  \n\n\n\n\nfor n in range(1, 4): print(n, C[n].LL, C[n].CH, C[n].PP)\n\n1 4587.973858436094 8.656554449879422 403.5362450418705\n2 1969.5968357164093 2.124699930654163 4.361123756505375\n3 673.340338258245 0.6693243919068042 1.590328049084836"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V2.html#unigram-case",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V2.html#unigram-case",
    "title": "Inferring Language Models (v2)",
    "section": "Unigram Case",
    "text": "Unigram Case\n\nLaPlace Smoothing\n\nUG['laplace_n'] = UG.n + 1\n\n\n# UG['laplace_p'] = UG.laplace_n / (N + M)\nUG['laplace_p'] = UG.laplace_n / UG.laplace_n.sum()\n\n\nUG\n\n\n\n\n\n  \n    \n      \n      n\n      p\n      i\n      h\n      laplace_n\n      laplace_p\n    \n    \n      w0\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      3\n      0.000013\n      16.239120\n      0.000210\n      4\n      0.000017\n    \n    \n      1760\n      1\n      0.000004\n      17.824082\n      0.000077\n      2\n      0.000008\n    \n    \n      1784\n      1\n      0.000004\n      17.824082\n      0.000077\n      2\n      0.000008\n    \n    \n      1785\n      1\n      0.000004\n      17.824082\n      0.000077\n      2\n      0.000008\n    \n    \n      1787\n      1\n      0.000004\n      17.824082\n      0.000077\n      2\n      0.000008\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      youth\n      22\n      0.000095\n      13.364651\n      0.001267\n      23\n      0.000096\n    \n    \n      youthful\n      3\n      0.000013\n      16.239120\n      0.000210\n      4\n      0.000017\n    \n    \n      zeal\n      7\n      0.000030\n      15.016727\n      0.000453\n      8\n      0.000033\n    \n    \n      zealous\n      4\n      0.000017\n      15.824082\n      0.000273\n      5\n      0.000021\n    \n    \n      zealously\n      2\n      0.000009\n      16.824082\n      0.000145\n      3\n      0.000012\n    \n  \n\n8206 rows × 6 columns\n\n\n\n\n\nAdjusted Count \\(c^{*}\\) and \\(p^{*}\\)\n\nUG['n_adj'] = (UG.laplace_n) * (N / (N + M))\n\n\nUG['p_adj'] = UG.n_adj / N\n\n\nUG\n\n\n\n\n\n  \n    \n      \n      n\n      p\n      i\n      h\n      laplace_n\n      laplace_p\n      n_adj\n      p_adj\n    \n    \n      w0\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      3\n      0.000013\n      16.239120\n      0.000210\n      4\n      0.000017\n      3.863380\n      0.000017\n    \n    \n      1760\n      1\n      0.000004\n      17.824082\n      0.000077\n      2\n      0.000008\n      1.931690\n      0.000008\n    \n    \n      1784\n      1\n      0.000004\n      17.824082\n      0.000077\n      2\n      0.000008\n      1.931690\n      0.000008\n    \n    \n      1785\n      1\n      0.000004\n      17.824082\n      0.000077\n      2\n      0.000008\n      1.931690\n      0.000008\n    \n    \n      1787\n      1\n      0.000004\n      17.824082\n      0.000077\n      2\n      0.000008\n      1.931690\n      0.000008\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      youth\n      22\n      0.000095\n      13.364651\n      0.001267\n      23\n      0.000096\n      22.214433\n      0.000096\n    \n    \n      youthful\n      3\n      0.000013\n      16.239120\n      0.000210\n      4\n      0.000017\n      3.863380\n      0.000017\n    \n    \n      zeal\n      7\n      0.000030\n      15.016727\n      0.000453\n      8\n      0.000033\n      7.726759\n      0.000033\n    \n    \n      zealous\n      4\n      0.000017\n      15.824082\n      0.000273\n      5\n      0.000021\n      4.829225\n      0.000021\n    \n    \n      zealously\n      2\n      0.000009\n      16.824082\n      0.000145\n      3\n      0.000012\n      2.897535\n      0.000012\n    \n  \n\n8206 rows × 8 columns\n\n\n\n\n\nDiscounting\n\nUG['disc'] = UG.n_adj / UG.n\n\n\nUG.disc.sort_values().plot(style='.')\n\n<Axes: xlabel='w0'>"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V2.html#bigram-case",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V2.html#bigram-case",
    "title": "Inferring Language Models (v2)",
    "section": "Bigram Case",
    "text": "Bigram Case\n\nBG = LM.M[2].n.unstack(fill_value=0).stack().to_frame('n')\n\n\nlen(BG)\n\n67338436\n\n\n\nBG\n\n\n\n\n\n  \n    \n      \n      \n      n\n    \n    \n      w0\n      w1\n      \n    \n  \n  \n    \n      1\n      1\n      0.0\n    \n    \n      1760\n      1.0\n    \n    \n      1784\n      0.0\n    \n    \n      1785\n      1.0\n    \n    \n      1787\n      0.0\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      zealously\n      youth\n      0.0\n    \n    \n      youthful\n      0.0\n    \n    \n      zeal\n      0.0\n    \n    \n      zealous\n      0.0\n    \n    \n      zealously\n      0.0\n    \n  \n\n67338436 rows × 1 columns\n\n\n\n\nBG['p'] = BG.n / BG.n.sum()\n\n\nLaPlace\n\nBG['laplace_n'] = BG.n + 1\n\n\n# BG\n\n\nBG['laplace_p'] = BG.laplace_n / (BG.groupby('w0').laplace_n.sum())\n\n\n# BG\n\n\n\nAdjusted Count\n\nBG['n_adj'] = (BG.laplace_n * BG.groupby('w0').n.sum()) / (BG.groupby('w0').laplace_n.sum())\n\n\n# BG\n\n\n\nDiscounting\n\nBG['disc'] = BG.n_adj / BG.laplace_n\n\n\n\nAdd K\n\nk = .5\nBG['addk_n'] = BG.n + k\nBG['addk_p'] = BG.addk_n / (BG.groupby('w0').addk_n.sum())\n\n\nBG\n\n\n\n\n\n  \n    \n      \n      \n      n\n      p\n      laplace_n\n      laplace_p\n      n_adj\n      disc\n      addk_n\n      addk_p\n    \n    \n      w0\n      w1\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      1\n      0.0\n      0.000000\n      1.0\n      0.000122\n      0.000365\n      0.000365\n      0.5\n      0.000122\n    \n    \n      1760\n      1.0\n      0.000003\n      2.0\n      0.000244\n      0.000731\n      0.000365\n      1.5\n      0.000365\n    \n    \n      1784\n      0.0\n      0.000000\n      1.0\n      0.000122\n      0.000365\n      0.000365\n      0.5\n      0.000122\n    \n    \n      1785\n      1.0\n      0.000003\n      2.0\n      0.000244\n      0.000731\n      0.000365\n      1.5\n      0.000365\n    \n    \n      1787\n      0.0\n      0.000000\n      1.0\n      0.000122\n      0.000365\n      0.000365\n      0.5\n      0.000122\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      zealously\n      youth\n      0.0\n      0.000000\n      1.0\n      0.000122\n      0.000244\n      0.000244\n      0.5\n      0.000122\n    \n    \n      youthful\n      0.0\n      0.000000\n      1.0\n      0.000122\n      0.000244\n      0.000244\n      0.5\n      0.000122\n    \n    \n      zeal\n      0.0\n      0.000000\n      1.0\n      0.000122\n      0.000244\n      0.000244\n      0.5\n      0.000122\n    \n    \n      zealous\n      0.0\n      0.000000\n      1.0\n      0.000122\n      0.000244\n      0.000244\n      0.5\n      0.000122\n    \n    \n      zealously\n      0.0\n      0.000000\n      1.0\n      0.000122\n      0.000244\n      0.000244\n      0.5\n      0.000122\n    \n  \n\n67338436 rows × 8 columns\n\n\n\n\n\nBackoff and Interpolation\n\n# BG.plot.scatter('laplace_p','addk_p')"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html",
    "title": "Inferring Language Models (v3)",
    "section": "",
    "text": "Set Up\nProblem: These texts treat Mr. and Mrs. as sentences. Since sentence breaks are represented in the OHCO, to fix this requires reparsing the source text into the TOKEN table with the abbreviation periods removed.\nWe handle out-of-vocabulary (OOV) terms by replacing some of our single-instance (i.e. hapax legomena) words with the <UNK> sign.\nWe now create a dataframe of ngrams, calculating MLE and with smoothing."
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#import-libraries",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#import-libraries",
    "title": "Inferring Language Models (v3)",
    "section": "Import libraries",
    "text": "Import libraries\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport re\nimport seaborn as sns\nfrom IPython.core.display import HTML\n\n\nsns.set()"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#configure",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#configure",
    "title": "Inferring Language Models (v3)",
    "section": "Configure",
    "text": "Configure\n\nimport configparser\nconfig = configparser.ConfigParser()\nconfig.read(\"../env.ini\")\ndata_dir = config['DEFAULT']['data_home']\noutput_dir = config['DEFAULT']['output_dir']\n\n\nOHCO = ['book_id', 'chap_num', 'para_num', 'sent_num', 'token_num']\ntext_file = f'{output_dir}/austen-combo.csv' # Generated in HW 02\n\n\nngram_size = 3\n\n# Add-k Smoothing parameter\nk = .1"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#import-tokens",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#import-tokens",
    "title": "Inferring Language Models (v3)",
    "section": "Import TOKENS",
    "text": "Import TOKENS\nWe use our way of representing a text as input. Normally, term_str would already be included in the dataframe.\n\nTOKENS = pd.read_csv(text_file).set_index(OHCO).dropna()\nTOKENS['term_str'] = TOKENS.token_str.str.lower().str.replace(r'[\\W_]+', '', regex=True)\nTOKENS = TOKENS[TOKENS.term_str != '']\n\n\nTOKENS.head()\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      \n      token_str\n      term_str\n    \n    \n      book_id\n      chap_num\n      para_num\n      sent_num\n      token_num\n      \n      \n    \n  \n  \n    \n      1\n      1\n      1\n      0\n      0\n      The\n      the\n    \n    \n      1\n      family\n      family\n    \n    \n      2\n      of\n      of\n    \n    \n      3\n      Dashwood\n      dashwood\n    \n    \n      4\n      had\n      had\n    \n  \n\n\n\n\nLook at book 2 (Persuasion) only:\n\nTOKENS.loc[2].head()\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      token_str\n      term_str\n    \n    \n      chap_num\n      para_num\n      sent_num\n      token_num\n      \n      \n    \n  \n  \n    \n      1\n      0\n      0\n      0\n      Sir\n      sir\n    \n    \n      1\n      Walter\n      walter\n    \n    \n      2\n      Elliot\n      elliot\n    \n    \n      3\n      of\n      of\n    \n    \n      4\n      Kellynch\n      kellynch"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#extract-vocab-from-tokens",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#extract-vocab-from-tokens",
    "title": "Inferring Language Models (v3)",
    "section": "Extract VOCAB from TOKENS",
    "text": "Extract VOCAB from TOKENS\n\nVOCAB = TOKENS.term_str.value_counts().to_frame('n').sort_index()\nVOCAB.index.name = 'term_str'\nVOCAB['n_chars'] = VOCAB.index.str.len()\n\n\nVOCAB.head()\n\n\n\n\n\n  \n    \n      \n      n\n      n_chars\n    \n    \n      term_str\n      \n      \n    \n  \n  \n    \n      1\n      3\n      1\n    \n    \n      15\n      1\n      2\n    \n    \n      16\n      1\n      2\n    \n    \n      1760\n      1\n      4\n    \n    \n      1784\n      1\n      4"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#compute-hapax-info",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#compute-hapax-info",
    "title": "Inferring Language Models (v3)",
    "section": "Compute HAPAX info",
    "text": "Compute HAPAX info\nTo select a subset from the set of hapax terms, we use the probability features of terms based on length.\nIntuition: This prevents us from getting rid of “good” words that appear in low frequency.\n\nHAPAX = VOCAB[VOCAB.n == 1].n_chars.value_counts().to_frame('n').sort_index() \nHAPAX.index.name = 'n_chars'\n\n\n# HAPAX\n\n\nHAPAX.plot();\n\n\n\n\n\nHAPAX['p'] = HAPAX.n / HAPAX.n.sum()\nHAPAX['s'] = 1/HAPAX.p\nHAPAX['i'] = np.log2(HAPAX.s)\nHAPAX['h'] = HAPAX.p * HAPAX.i\nH = HAPAX.h.sum().round(2)\nH_max = np.log2(HAPAX.shape[0]).round(2)\nR = (1 - H/H_max).round(2) * 100\n\n\nH, H_max, R\n\n(3.32, 4.09, 19.0)\n\n\n\nHAPAX.style.background_gradient()\n\n\n\n\n  \n    \n       \n      n\n      p\n      s\n      i\n      h\n    \n    \n      n_chars\n       \n       \n       \n       \n       \n    \n  \n  \n    \n      1\n      6\n      0.001933\n      517.333333\n      9.014950\n      0.017426\n    \n    \n      2\n      8\n      0.002577\n      388.000000\n      8.599913\n      0.022165\n    \n    \n      3\n      35\n      0.011276\n      88.685714\n      6.470630\n      0.072961\n    \n    \n      4\n      152\n      0.048969\n      20.421053\n      4.351985\n      0.213113\n    \n    \n      5\n      294\n      0.094716\n      10.557823\n      3.400240\n      0.322059\n    \n    \n      6\n      397\n      0.127899\n      7.818640\n      2.966918\n      0.379467\n    \n    \n      7\n      477\n      0.153673\n      6.507338\n      2.702067\n      0.415234\n    \n    \n      8\n      478\n      0.153995\n      6.493724\n      2.699046\n      0.415639\n    \n    \n      9\n      452\n      0.145619\n      6.867257\n      2.779734\n      0.404781\n    \n    \n      10\n      305\n      0.098260\n      10.177049\n      3.347247\n      0.328902\n    \n    \n      11\n      235\n      0.075709\n      13.208511\n      3.723396\n      0.281894\n    \n    \n      12\n      147\n      0.047358\n      21.115646\n      4.400240\n      0.208388\n    \n    \n      13\n      64\n      0.020619\n      48.500000\n      5.599913\n      0.115462\n    \n    \n      14\n      33\n      0.010631\n      94.060606\n      6.555519\n      0.069695\n    \n    \n      15\n      15\n      0.004832\n      206.933333\n      7.693022\n      0.037176\n    \n    \n      16\n      3\n      0.000966\n      1034.666667\n      10.014950\n      0.009679\n    \n    \n      17\n      3\n      0.000966\n      1034.666667\n      10.014950\n      0.009679\n    \n  \n\n\n\n\n(HAPAX / HAPAX.mean())[['p','h']].plot();\n\n\n\n\nAside: Note how entropy “regularizes” probability, lowering the higher probabilities and raising the lower ones."
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#method-1-mean-self-entropy",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#method-1-mean-self-entropy",
    "title": "Inferring Language Models (v3)",
    "section": "Method 1: Mean Self-Entropy",
    "text": "Method 1: Mean Self-Entropy\n\nHAPAX.h.plot(title=\"Entropy by Word Length\")\nHM = HAPAX.h.mean() # Use mean self-entropy as threshhold\nplt.axhline(y=HM, color='orange');\n\n\n\n\n\nunk_list_h = HAPAX[HAPAX.h < HM].index.to_list()\n\n\nunk_list_h\n\n[1, 2, 3, 13, 14, 15, 16, 17]"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#method-2-mean-information",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#method-2-mean-information",
    "title": "Inferring Language Models (v3)",
    "section": "Method 2: Mean Information",
    "text": "Method 2: Mean Information\n\nhapax_mean = HAPAX.i.mean() # Use mean as threshhold\n\n\nHAPAX.i.plot(title=\"Information by Word Length\")\nplt.axhline(y=hapax_mean, color='orange');\n\n\n\n\nGet lengths that are below the average information (or self-entropy).\n\nunk_list_i = HAPAX[HAPAX.i > hapax_mean].index.to_list()\n\n\nunk_list_i\n\n[1, 2, 3, 13, 14, 15, 16, 17]\n\n\nKeep the words with lengths with i or h below the mean.\n\nUNK = VOCAB[VOCAB.n_chars.isin(unk_list_i) & (VOCAB.n == 1)].index.to_list()        \n\n\nHTML(' '.join(UNK))\n\n\n15 16 1st 20 200 9 acknowledgements acquaintances administering aggrandizement aim anticipations ash au bee brotherliness characteristic circumspection circumstanced circumventing cod commiseration communications companionableness comprehensible condescending congratulating connoisseurship conscientiously consternation constitutional consultations contemplating corresponding cot counteracting counterbalance demonstrations desirableness disadvantages disagreements discriminating discrimination disengagement disinterestedness disqualifications disrespectful dissatisfaction e eligibilities em encouragements encroachments enfranchisement err expensiveness extraordinarily fastidiousness fed felicitations fir forgetfulness fox gad grandchildren ham hey hid hit ho hon hop hue hum ice imperfections impoverishing impracticable inadvertencies incautiousness inconsiderately indefatigable independently indistinctness inexhaustible inexpressibly inquisitiveness insignificance instantaneous insufficiency insurmountable intrinsically introductions irreconcilable jar l leg mab mad mar mediterranean mid misconstructions n net non obtrusiveness ostentatiously overspreading p particularize personableness philanthropic possibilities prepossessing prescriptions professionally proportionately pry qualification qualifications rat reasonableness recapitulation relinquishment reprehensible representative representatives reproachfully resuscitation retrenchments retrospections rug rut sarcastically sequestration sin solicitations spontaneously strengthening superannuated superiorities thoughtlessness tranquillized transgressions unaccountably unconquerable unconsciousness undesirableness unexceptionable unexhilarating unfashionable unintelligible unintentional uninteresting unnecessarily unobjectionable unobtrusiveness unpersuadable unpleasantest unproductively unreasonableness unsuitableness ve wm woe z"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#create-training-sentences-and-vocab",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#create-training-sentences-and-vocab",
    "title": "Inferring Language Models (v3)",
    "section": "Create Training sentences and vocab",
    "text": "Create Training sentences and vocab\nGrab terms (normalized tokens) from TOKENS.\n\nTOKENS['term_str_train'] = TOKENS.term_str\n\nReplace unknowns.\n\nTOKENS.loc[TOKENS.term_str.isin(UNK), 'term_str_train'] = '<UNK>'\n\nConvert tokens to sentences.\n\nS_TRAIN = list(TOKENS.groupby(OHCO[:-1]).term_str_train.apply(lambda x: ' '.join(x)).values)\n\nInspect results:\n\nS_TRAIN[:5]\n\n['the family of dashwood had long been settled in sussex',\n 'their estate was large and their residence was at norland park in the centre of their property where for many generations they had lived in so respectable a manner as to engage the general good opinion of their surrounding acquaintance',\n 'the late owner of this estate was a single man who lived to a very advanced age and who for many years of his life had a constant companion and housekeeper in his sister',\n 'but her death which happened ten years before his own produced a great alteration in his home',\n 'for to supply her loss he invited and received into his house the family of his nephew mr']\n\n\nExtract the training VOCAB as a list.\n\n# V_TRAIN = TOKENS.term_str_train.drop_duplicates().sort_values().to_list()\nV_TRAIN = sorted(list(set(VOCAB.index) - set(UNK))  + ['<UNK>'])\n\nInspect a slice.\n\nV_TRAIN[100:105]\n\n['accustomary', 'aches', 'aching', 'acknowledge', 'acknowledged']"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#parameters",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#parameters",
    "title": "Inferring Language Models (v3)",
    "section": "Parameters",
    "text": "Parameters\nCreate names used for ngram element columns (w0, w1, etc.)\n\nwidx = [f'w{i}' for i in range(ngram_size)]"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#model",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#model",
    "title": "Inferring Language Models (v3)",
    "section": "Model",
    "text": "Model\nWe define a class to compute n-gram counts. These counts will be used in our models below.\n\nclass NgramCounter():\n    \n    def __init__(self, sents:[], vocab:[], n:int=3):\n        self.sents = sents\n        self.vocab = vocab\n        self.n = n\n        self.widx = [f'w{i}' for i in range(self.n)]\n        \n    def generate(self):\n        \n        self.S = pd.DataFrame(dict(sent_str=self.sents))\n    \n        # Pad sentences \n        pad = '<s> ' *  (self.n - 1)\n        \n        self.I = (pad + self.S.sent_str + ' </s>')\\\n            .str.split(expand=True).stack().to_frame('w0')\n        self.I.index.names = ['sent_num', 'token_num']\n        self.I.loc[~self.I.w0.isin(self.vocab + ['<s>','</s>']), 'w0'] = '<UNK>'\n\n        # Get sentence lengths (these include pads)\n        self.S['len'] = self.I.groupby('sent_num').w0.count()\n                \n        # Add w columns\n        for i in range(1, self.n): \n            self.I[f'w{i}'] = self.I[f\"w{i-1}\"].shift(-1)         \n        \n        # Generate ngrams\n        self.NG = []\n        for i in range(self.n):\n            self.NG.append(self.I.iloc[:, :i+1].copy())\n           \n            # Remove spurious rows\n#             self.NG[i] = self.NG[i].query(f\"w{i} != '<s>'\")\n                    \n        # Generate counts\n        self.LM = []\n        for i in range(self.n):\n            self.LM.append(self.NG[i].value_counts().to_frame('n')) # 2 offset is to remove blank lines\n\n        # Hack to remove single value tuple ...\n        self.LM[0].index = [i[0] for i in self.LM[0].index]\n        self.LM[0].index.name = 'w0'\n\n\ntrain = NgramCounter(S_TRAIN, V_TRAIN)\ntrain.generate()\n\n\ntrain.NG[2].loc[0]\n\n\n\n\n\n  \n    \n      \n      w0\n      w1\n      w2\n    \n    \n      token_num\n      \n      \n      \n    \n  \n  \n    \n      0\n      <s>\n      <s>\n      the\n    \n    \n      1\n      <s>\n      the\n      family\n    \n    \n      2\n      the\n      family\n      of\n    \n    \n      3\n      family\n      of\n      dashwood\n    \n    \n      4\n      of\n      dashwood\n      had\n    \n    \n      5\n      dashwood\n      had\n      long\n    \n    \n      6\n      had\n      long\n      been\n    \n    \n      7\n      long\n      been\n      settled\n    \n    \n      8\n      been\n      settled\n      in\n    \n    \n      9\n      settled\n      in\n      sussex\n    \n    \n      10\n      in\n      sussex\n      </s>\n    \n    \n      11\n      sussex\n      </s>\n      <s>\n    \n    \n      12\n      </s>\n      <s>\n      <s>\n    \n  \n\n\n\n\n\nclass NgramLanguageModel():\n    \n    k:float = .5\n    \n    def __init__(self, ngc:NgramCounter):\n        self.S = ngc.S\n        self.LM = ngc.LM\n        self.NG = ngc.NG\n        self.n = ngc.n\n        \n    def apply_smoothing(self):\n        \n        self.Z1 = [None for i in range(self.n)] # Unseen N grams, but seen N-1 grams\n        self.Z2 = [None for i in range(self.n)] # Unsess N-1 grams too\n        V = len(self.LM[0])\n\n        for i in range(self.n):      \n            \n            # Joint probabilities\n            B = V**(i+1)\n            self.LM[i]['p'] = (self.LM[i].n + self.k) / (self.LM[i].n.sum() + B * self.k)\n            self.LM[i]['log_p'] = np.log2(self.LM[i].p)\n\n            if i > 0:\n            \n                # Conditional probabilities\n                self.LM[i]['cp'] = self.LM[i].p / self.LM[i-1].p\n                self.LM[i]['log_cp'] = np.log2(self.LM[i].cp)\n\n                # Handle unseen data\n                self.LM[i].sort_index(inplace=True)     \n                self.Z1[i] = np.log2(self.k / (self.LM[i-1].n + B*self.k))\n                self.Z2[i] = np.log2(self.k / B*self.k)\n\n    def predict(self, test):\n        self.T = test\n        self.PP = []\n        p_key = 'log_cp'\n        for i in range(self.n):\n            ng = i + 1\n            if i == 0:\n                self.T.S[f'ng_{ng}_ll'] = self.T.NG[0].join(self.LM[0].log_p, on=widx[:ng])\\\n                    .groupby('sent_num').log_p.sum()\n            else:\n                self.T.S[f'ng_{ng}_ll'] = self.T.NG[i].join(self.LM[i][p_key], on=widx[:ng])\\\n                    .fillna(Z1[i]).fillna(Z2[i])\\\n                    .groupby('sent_num')[p_key].sum()\n                \n            self.T.S[f'pp{ng}'] = 2**( -self.T.S[f'ng_{ng}_ll'] / self.T.S['len'])\n\n                \n\n\n# def get_pp(self, sent_str):\n        # pp = 2 ** -(\n        #   sum(mle.logprob(word) for word in sample.split())\n        #   /\n        #   len(sample.split())\n        # )\n\n\n# model = NgramLanguageModel(S_TRAIN, V_TRAIN)\nmodel = NgramLanguageModel(train)\nmodel.k = 1\nmodel.apply_smoothing()\n\n\nNG = model.NG\nLM = model.LM\nZ1 = model.Z1\nZ2 = model.Z2\n\n\nLM[2].sort_index()\n\n\n\n\n\n  \n    \n      \n      \n      \n      n\n      p\n      log_p\n      cp\n      log_cp\n    \n    \n      w0\n      w1\n      w2\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      1760\n      married\n      1\n      3.799825e-12\n      -37.937204\n      0.000124\n      -12.973642\n    \n    \n      1785\n      </s>\n      1\n      3.799825e-12\n      -37.937204\n      0.000124\n      -12.973642\n    \n    \n      ends\n      </s>\n      1\n      3.799825e-12\n      -37.937204\n      0.000124\n      -12.973642\n    \n    \n      1760\n      married\n      july\n      1\n      3.799825e-12\n      -37.937204\n      0.000124\n      -12.973642\n    \n    \n      1784\n      elizabeth\n      daughter\n      1\n      3.799825e-12\n      -37.937204\n      0.000124\n      -12.973642\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      zealous\n      attention\n      as\n      1\n      3.799825e-12\n      -37.937204\n      0.000124\n      -12.973642\n    \n    \n      officer\n      too\n      1\n      3.799825e-12\n      -37.937204\n      0.000124\n      -12.973642\n    \n    \n      on\n      the\n      1\n      3.799825e-12\n      -37.937204\n      0.000124\n      -12.973642\n    \n    \n      zealously\n      active\n      as\n      1\n      3.799825e-12\n      -37.937204\n      0.000124\n      -12.973642\n    \n    \n      discharging\n      all\n      1\n      3.799825e-12\n      -37.937204\n      0.000124\n      -12.973642\n    \n  \n\n160317 rows × 5 columns\n\n\n\n\nLM[2].loc[('anne', 'was')].sort_values('n', ascending=False)\n\n\n\n\n\n  \n    \n      \n      n\n      p\n      log_p\n      cp\n      log_cp\n    \n    \n      w2\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      not\n      2\n      5.699737e-12\n      -37.352242\n      0.000008\n      -16.973642\n    \n    \n      at\n      2\n      5.699737e-12\n      -37.352242\n      0.000008\n      -16.973642\n    \n    \n      startled\n      2\n      5.699737e-12\n      -37.352242\n      0.000008\n      -16.973642\n    \n    \n      so\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      now\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      obliged\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      one\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      out\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      really\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      renewing\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      set\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      shewn\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      a\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      nineteen\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      still\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      sure\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      tenderness\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      the\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      to\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      too\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      walking\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      sporting\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      never\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      all\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      conscious\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      almost\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      already\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      among\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      amused\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      ashamed\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      astonished\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      caught\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      come\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      considering\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      much\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      delighted\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      enabled\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      her\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      improved\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      inevitable\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      left\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      mary\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      most\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n    \n      well\n      1\n      3.799825e-12\n      -37.937204\n      0.000005\n      -17.558604\n    \n  \n\n\n\n\n\nPP1 = round(2**(-LM[0].log_p.sum() / len(LM[0])))\nPP2 = round(2**(-LM[1].log_cp.sum() / len(LM[1])))\nPP3 = round(2**(-LM[2].log_cp.sum() / len(LM[2])))\n\n\nPP1, PP2, PP3\n\n(47891, 12663, 27600)"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#choose-test-sentences",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#choose-test-sentences",
    "title": "Inferring Language Models (v3)",
    "section": "Choose Test Sentences",
    "text": "Choose Test Sentences\n\n# Some paragraphs from Austen's _Emma_ and other stuff (first two)\nS_TEST = \"\"\"\nThe car was brand new\nComputer programs are full of bugs\nThe event had every promise of happiness for her friend \nMr Weston was a man of unexceptionable character easy fortune suitable age and pleasant manners\nand there was some satisfaction in considering with what self-denying generous friendship she had always wished and promoted the match\nbut it was a black morning's work for her \nThe want of Miss Taylor would be felt every hour of every day \nShe recalled her past kindness the kindness the affection of sixteen years \nhow she had taught and how she had played with her from five years old \nhow she had devoted all her powers to attach and amuse her in health \nand how nursed her through the various illnesses of childhood \nA large debt of gratitude was owing here \nbut the intercourse of the last seven years \nthe equal footing and perfect unreserve which had soon followed Isabella's marriage \non their being left to each other was yet a dearer tenderer recollection \nShe had been a friend and companion such as few possessed intelligent well-informed useful gentle \nknowing all the ways of the family \ninterested in all its concerns \nand peculiarly interested in herself in every pleasure every scheme of hers \none to whom she could speak every thought as it arose \nand who had such an affection for her as could never find fault \nHow was she to bear the change \nIt was true that her friend was going only half a mile from them \nbut Emma was aware that great must be the difference between a Mrs Weston \nonly half a mile from them \nand a Miss Taylor in the house \nand with all her advantages natural and domestic \nshe was now in great danger of suffering from intellectual solitude \nShe dearly loved her father \nbut he was no companion for her \nHe could not meet her in conversation rational or playful \nThe evil of the actual disparity in their ages\nand Mr Woodhouse had not married early\nwas much increased by his constitution and habits \nfor having been a valetudinarian all his life \nwithout activity of mind or body \nhe was a much older man in ways than in years \nand though everywhere beloved for the friendliness of his heart and his amiable temper \nhis talents could not have recommended him at any time \nHer sister though comparatively but little removed by matrimony \nbeing settled in London only sixteen miles off was much beyond her daily reach \nand many a long October and November evening must be struggled through at Hartfield \nbefore Christmas brought the next visit from Isabella and her husband \nand their little children to fill the house and give her pleasant society again \n\"\"\".split('\\n')[1:-1]\n\n\ntest = NgramCounter(S_TEST, V_TRAIN)\ntest.generate()\nmodel.predict(test)\n\n\nround(test.I.w0.value_counts().loc['<UNK>'] / test.I.w0.value_counts().sum(), 2)\n\n0.07\n\n\n\nUnigrams\n\ntest.S.sort_values('pp1')[['sent_str', 'pp1', 'pp2', 'pp3']].style.background_gradient()\n\n\n\n\n  \n    \n       \n      sent_str\n      pp1\n      pp2\n      pp3\n    \n  \n  \n    \n      25\n      and a Miss Taylor in the house \n      87.412144\n      4751.850773\n      37755411.179800\n    \n    \n      29\n      but he was no companion for her \n      93.951976\n      3802.394084\n      4682878.661787\n    \n    \n      21\n      How was she to bear the change \n      117.397965\n      17240.626738\n      173496810.074686\n    \n    \n      16\n      knowing all the ways of the family \n      146.993577\n      6312.879539\n      7660979.413575\n    \n    \n      34\n      for having been a valetudinarian all his life \n      147.853328\n      14121.858948\n      61839121.223824\n    \n    \n      5\n      but it was a black morning's work for her \n      171.204171\n      68217.025314\n      34969326.666392\n    \n    \n      32\n      and Mr Woodhouse had not married early\n      179.521312\n      43021.404002\n      951208476.216935\n    \n    \n      12\n      but the intercourse of the last seven years \n      183.222666\n      9222.324715\n      14723451.234162\n    \n    \n      26\n      and with all her advantages natural and domestic \n      187.761585\n      174505.987741\n      401750944.008146\n    \n    \n      24\n      only half a mile from them \n      189.727209\n      5480.937450\n      1592281.296982\n    \n    \n      1\n      Computer programs are full of bugs\n      204.889395\n      50178.870196\n      471518771.271039\n    \n    \n      19\n      one to whom she could speak every thought as it arose \n      218.053753\n      19415.245795\n      210911228.252883\n    \n    \n      20\n      and who had such an affection for her as could never find fault \n      222.492168\n      17123.218058\n      59925664.492147\n    \n    \n      17\n      interested in all its concerns \n      234.696180\n      25795.316432\n      5852123850.396474\n    \n    \n      38\n      his talents could not have recommended him at any time \n      240.594727\n      24129.068881\n      278879749.357072\n    \n    \n      2\n      The event had every promise of happiness for her friend \n      249.471781\n      63963.144014\n      1509459952.503219\n    \n    \n      36\n      he was a much older man in ways than in years \n      249.583133\n      81468.524122\n      593732992.509362\n    \n    \n      42\n      before Christmas brought the next visit from Isabella and her husband \n      255.493401\n      49017.923353\n      2061646603.988296\n    \n    \n      23\n      but Emma was aware that great must be the difference between a Mrs Weston \n      260.416254\n      35771.688938\n      3888889733.499942\n    \n    \n      27\n      she was now in great danger of suffering from intellectual solitude \n      268.214690\n      45454.750220\n      49069793.601134\n    \n    \n      22\n      It was true that her friend was going only half a mile from them \n      281.947173\n      20556.547893\n      111686698.328527\n    \n    \n      6\n      The want of Miss Taylor would be felt every hour of every day \n      282.301211\n      42755.167142\n      464149482.763934\n    \n    \n      35\n      without activity of mind or body \n      286.437147\n      150041.420201\n      2946749731.346935\n    \n    \n      43\n      and their little children to fill the house and give her pleasant society again \n      288.060115\n      119697.102316\n      4811902838.722778\n    \n    \n      33\n      was much increased by his constitution and habits \n      292.503590\n      10012.099238\n      255649395.017749\n    \n    \n      10\n      and how nursed her through the various illnesses of childhood \n      294.478223\n      25668.077037\n      970289088.937635\n    \n    \n      8\n      how she had taught and how she had played with her from five years old \n      305.980639\n      37017.697340\n      223744869.982958\n    \n    \n      41\n      and many a long October and November evening must be struggled through at Hartfield \n      333.665901\n      175312.019713\n      1715843192.065487\n    \n    \n      9\n      how she had devoted all her powers to attach and amuse her in health \n      334.747696\n      62744.745350\n      4823964561.868869\n    \n    \n      0\n      The car was brand new\n      335.216232\n      79372.301417\n      1621303376.527397\n    \n    \n      28\n      She dearly loved her father \n      341.301872\n      11353.498974\n      247557601.459594\n    \n    \n      14\n      on their being left to each other was yet a dearer tenderer recollection \n      348.634570\n      66879.796782\n      1123002512.972523\n    \n    \n      37\n      and though everywhere beloved for the friendliness of his heart and his amiable temper \n      381.328198\n      45607.494510\n      230529367.970228\n    \n    \n      30\n      He could not meet her in conversation rational or playful \n      388.348636\n      183901.969908\n      19472101524.800404\n    \n    \n      18\n      and peculiarly interested in herself in every pleasure every scheme of hers \n      389.294091\n      77921.389727\n      5376440814.247145\n    \n    \n      4\n      and there was some satisfaction in considering with what self-denying generous friendship she had always wished and promoted the match\n      432.129618\n      81604.351914\n      992880948.217300\n    \n    \n      7\n      She recalled her past kindness the kindness the affection of sixteen years \n      478.301859\n      206675.290141\n      2398176123.870548\n    \n    \n      31\n      The evil of the actual disparity in their ages\n      514.116720\n      335722.021357\n      12369076443.092093\n    \n    \n      39\n      Her sister though comparatively but little removed by matrimony \n      556.387269\n      578784.387813\n      11142900944.559072\n    \n    \n      3\n      Mr Weston was a man of unexceptionable character easy fortune suitable age and pleasant manners\n      567.723466\n      200370.394496\n      7190760080.413895\n    \n    \n      13\n      the equal footing and perfect unreserve which had soon followed Isabella's marriage \n      578.221956\n      578536.742798\n      17901330037.913425\n    \n    \n      11\n      A large debt of gratitude was owing here \n      618.182104\n      1178824.474314\n      7848598104.883666\n    \n    \n      15\n      She had been a friend and companion such as few possessed intelligent well-informed useful gentle \n      626.969481\n      413751.430339\n      7136392566.400505\n    \n    \n      40\n      being settled in London only sixteen miles off was much beyond her daily reach \n      665.316057\n      227269.194290\n      36069296442.502388\n    \n  \n\n\n\n\ntest.S.loc[1].sent_str\n\n'Computer programs are full of bugs'\n\n\n\ntest.S.loc[0].sent_str\n\n'The car was brand new'\n\n\n\n\nBigrams\n\ntest.S.sort_values('pp2')[['sent_str', 'pp1', 'pp2', 'pp3']].style.background_gradient()\n\n\n\n\n  \n    \n       \n      sent_str\n      pp1\n      pp2\n      pp3\n    \n  \n  \n    \n      29\n      but he was no companion for her \n      93.951976\n      3802.394084\n      4682878.661787\n    \n    \n      25\n      and a Miss Taylor in the house \n      87.412144\n      4751.850773\n      37755411.179800\n    \n    \n      24\n      only half a mile from them \n      189.727209\n      5480.937450\n      1592281.296982\n    \n    \n      16\n      knowing all the ways of the family \n      146.993577\n      6312.879539\n      7660979.413575\n    \n    \n      12\n      but the intercourse of the last seven years \n      183.222666\n      9222.324715\n      14723451.234162\n    \n    \n      33\n      was much increased by his constitution and habits \n      292.503590\n      10012.099238\n      255649395.017749\n    \n    \n      28\n      She dearly loved her father \n      341.301872\n      11353.498974\n      247557601.459594\n    \n    \n      34\n      for having been a valetudinarian all his life \n      147.853328\n      14121.858948\n      61839121.223824\n    \n    \n      20\n      and who had such an affection for her as could never find fault \n      222.492168\n      17123.218058\n      59925664.492147\n    \n    \n      21\n      How was she to bear the change \n      117.397965\n      17240.626738\n      173496810.074686\n    \n    \n      19\n      one to whom she could speak every thought as it arose \n      218.053753\n      19415.245795\n      210911228.252883\n    \n    \n      22\n      It was true that her friend was going only half a mile from them \n      281.947173\n      20556.547893\n      111686698.328527\n    \n    \n      38\n      his talents could not have recommended him at any time \n      240.594727\n      24129.068881\n      278879749.357072\n    \n    \n      10\n      and how nursed her through the various illnesses of childhood \n      294.478223\n      25668.077037\n      970289088.937635\n    \n    \n      17\n      interested in all its concerns \n      234.696180\n      25795.316432\n      5852123850.396474\n    \n    \n      23\n      but Emma was aware that great must be the difference between a Mrs Weston \n      260.416254\n      35771.688938\n      3888889733.499942\n    \n    \n      8\n      how she had taught and how she had played with her from five years old \n      305.980639\n      37017.697340\n      223744869.982958\n    \n    \n      6\n      The want of Miss Taylor would be felt every hour of every day \n      282.301211\n      42755.167142\n      464149482.763934\n    \n    \n      32\n      and Mr Woodhouse had not married early\n      179.521312\n      43021.404002\n      951208476.216935\n    \n    \n      27\n      she was now in great danger of suffering from intellectual solitude \n      268.214690\n      45454.750220\n      49069793.601134\n    \n    \n      37\n      and though everywhere beloved for the friendliness of his heart and his amiable temper \n      381.328198\n      45607.494510\n      230529367.970228\n    \n    \n      42\n      before Christmas brought the next visit from Isabella and her husband \n      255.493401\n      49017.923353\n      2061646603.988296\n    \n    \n      1\n      Computer programs are full of bugs\n      204.889395\n      50178.870196\n      471518771.271039\n    \n    \n      9\n      how she had devoted all her powers to attach and amuse her in health \n      334.747696\n      62744.745350\n      4823964561.868869\n    \n    \n      2\n      The event had every promise of happiness for her friend \n      249.471781\n      63963.144014\n      1509459952.503219\n    \n    \n      14\n      on their being left to each other was yet a dearer tenderer recollection \n      348.634570\n      66879.796782\n      1123002512.972523\n    \n    \n      5\n      but it was a black morning's work for her \n      171.204171\n      68217.025314\n      34969326.666392\n    \n    \n      18\n      and peculiarly interested in herself in every pleasure every scheme of hers \n      389.294091\n      77921.389727\n      5376440814.247145\n    \n    \n      0\n      The car was brand new\n      335.216232\n      79372.301417\n      1621303376.527397\n    \n    \n      36\n      he was a much older man in ways than in years \n      249.583133\n      81468.524122\n      593732992.509362\n    \n    \n      4\n      and there was some satisfaction in considering with what self-denying generous friendship she had always wished and promoted the match\n      432.129618\n      81604.351914\n      992880948.217300\n    \n    \n      43\n      and their little children to fill the house and give her pleasant society again \n      288.060115\n      119697.102316\n      4811902838.722778\n    \n    \n      35\n      without activity of mind or body \n      286.437147\n      150041.420201\n      2946749731.346935\n    \n    \n      26\n      and with all her advantages natural and domestic \n      187.761585\n      174505.987741\n      401750944.008146\n    \n    \n      41\n      and many a long October and November evening must be struggled through at Hartfield \n      333.665901\n      175312.019713\n      1715843192.065487\n    \n    \n      30\n      He could not meet her in conversation rational or playful \n      388.348636\n      183901.969908\n      19472101524.800404\n    \n    \n      3\n      Mr Weston was a man of unexceptionable character easy fortune suitable age and pleasant manners\n      567.723466\n      200370.394496\n      7190760080.413895\n    \n    \n      7\n      She recalled her past kindness the kindness the affection of sixteen years \n      478.301859\n      206675.290141\n      2398176123.870548\n    \n    \n      40\n      being settled in London only sixteen miles off was much beyond her daily reach \n      665.316057\n      227269.194290\n      36069296442.502388\n    \n    \n      31\n      The evil of the actual disparity in their ages\n      514.116720\n      335722.021357\n      12369076443.092093\n    \n    \n      15\n      She had been a friend and companion such as few possessed intelligent well-informed useful gentle \n      626.969481\n      413751.430339\n      7136392566.400505\n    \n    \n      13\n      the equal footing and perfect unreserve which had soon followed Isabella's marriage \n      578.221956\n      578536.742798\n      17901330037.913425\n    \n    \n      39\n      Her sister though comparatively but little removed by matrimony \n      556.387269\n      578784.387813\n      11142900944.559072\n    \n    \n      11\n      A large debt of gratitude was owing here \n      618.182104\n      1178824.474314\n      7848598104.883666\n    \n  \n\n\n\n\n\nTrigrams\n\ntest.S.sort_values('pp3')[['sent_str', 'pp1', 'pp2', 'pp3']].style.background_gradient()\n\n\n\n\n  \n    \n       \n      sent_str\n      pp1\n      pp2\n      pp3\n    \n  \n  \n    \n      24\n      only half a mile from them \n      189.727209\n      5480.937450\n      1592281.296982\n    \n    \n      29\n      but he was no companion for her \n      93.951976\n      3802.394084\n      4682878.661787\n    \n    \n      16\n      knowing all the ways of the family \n      146.993577\n      6312.879539\n      7660979.413575\n    \n    \n      12\n      but the intercourse of the last seven years \n      183.222666\n      9222.324715\n      14723451.234162\n    \n    \n      5\n      but it was a black morning's work for her \n      171.204171\n      68217.025314\n      34969326.666392\n    \n    \n      25\n      and a Miss Taylor in the house \n      87.412144\n      4751.850773\n      37755411.179800\n    \n    \n      27\n      she was now in great danger of suffering from intellectual solitude \n      268.214690\n      45454.750220\n      49069793.601134\n    \n    \n      20\n      and who had such an affection for her as could never find fault \n      222.492168\n      17123.218058\n      59925664.492147\n    \n    \n      34\n      for having been a valetudinarian all his life \n      147.853328\n      14121.858948\n      61839121.223824\n    \n    \n      22\n      It was true that her friend was going only half a mile from them \n      281.947173\n      20556.547893\n      111686698.328527\n    \n    \n      21\n      How was she to bear the change \n      117.397965\n      17240.626738\n      173496810.074686\n    \n    \n      19\n      one to whom she could speak every thought as it arose \n      218.053753\n      19415.245795\n      210911228.252883\n    \n    \n      8\n      how she had taught and how she had played with her from five years old \n      305.980639\n      37017.697340\n      223744869.982958\n    \n    \n      37\n      and though everywhere beloved for the friendliness of his heart and his amiable temper \n      381.328198\n      45607.494510\n      230529367.970228\n    \n    \n      28\n      She dearly loved her father \n      341.301872\n      11353.498974\n      247557601.459594\n    \n    \n      33\n      was much increased by his constitution and habits \n      292.503590\n      10012.099238\n      255649395.017749\n    \n    \n      38\n      his talents could not have recommended him at any time \n      240.594727\n      24129.068881\n      278879749.357072\n    \n    \n      26\n      and with all her advantages natural and domestic \n      187.761585\n      174505.987741\n      401750944.008146\n    \n    \n      6\n      The want of Miss Taylor would be felt every hour of every day \n      282.301211\n      42755.167142\n      464149482.763934\n    \n    \n      1\n      Computer programs are full of bugs\n      204.889395\n      50178.870196\n      471518771.271039\n    \n    \n      36\n      he was a much older man in ways than in years \n      249.583133\n      81468.524122\n      593732992.509362\n    \n    \n      32\n      and Mr Woodhouse had not married early\n      179.521312\n      43021.404002\n      951208476.216935\n    \n    \n      10\n      and how nursed her through the various illnesses of childhood \n      294.478223\n      25668.077037\n      970289088.937635\n    \n    \n      4\n      and there was some satisfaction in considering with what self-denying generous friendship she had always wished and promoted the match\n      432.129618\n      81604.351914\n      992880948.217300\n    \n    \n      14\n      on their being left to each other was yet a dearer tenderer recollection \n      348.634570\n      66879.796782\n      1123002512.972523\n    \n    \n      2\n      The event had every promise of happiness for her friend \n      249.471781\n      63963.144014\n      1509459952.503219\n    \n    \n      0\n      The car was brand new\n      335.216232\n      79372.301417\n      1621303376.527397\n    \n    \n      41\n      and many a long October and November evening must be struggled through at Hartfield \n      333.665901\n      175312.019713\n      1715843192.065487\n    \n    \n      42\n      before Christmas brought the next visit from Isabella and her husband \n      255.493401\n      49017.923353\n      2061646603.988296\n    \n    \n      7\n      She recalled her past kindness the kindness the affection of sixteen years \n      478.301859\n      206675.290141\n      2398176123.870548\n    \n    \n      35\n      without activity of mind or body \n      286.437147\n      150041.420201\n      2946749731.346935\n    \n    \n      23\n      but Emma was aware that great must be the difference between a Mrs Weston \n      260.416254\n      35771.688938\n      3888889733.499942\n    \n    \n      43\n      and their little children to fill the house and give her pleasant society again \n      288.060115\n      119697.102316\n      4811902838.722778\n    \n    \n      9\n      how she had devoted all her powers to attach and amuse her in health \n      334.747696\n      62744.745350\n      4823964561.868869\n    \n    \n      18\n      and peculiarly interested in herself in every pleasure every scheme of hers \n      389.294091\n      77921.389727\n      5376440814.247145\n    \n    \n      17\n      interested in all its concerns \n      234.696180\n      25795.316432\n      5852123850.396474\n    \n    \n      15\n      She had been a friend and companion such as few possessed intelligent well-informed useful gentle \n      626.969481\n      413751.430339\n      7136392566.400505\n    \n    \n      3\n      Mr Weston was a man of unexceptionable character easy fortune suitable age and pleasant manners\n      567.723466\n      200370.394496\n      7190760080.413895\n    \n    \n      11\n      A large debt of gratitude was owing here \n      618.182104\n      1178824.474314\n      7848598104.883666\n    \n    \n      39\n      Her sister though comparatively but little removed by matrimony \n      556.387269\n      578784.387813\n      11142900944.559072\n    \n    \n      31\n      The evil of the actual disparity in their ages\n      514.116720\n      335722.021357\n      12369076443.092093\n    \n    \n      13\n      the equal footing and perfect unreserve which had soon followed Isabella's marriage \n      578.221956\n      578536.742798\n      17901330037.913425\n    \n    \n      30\n      He could not meet her in conversation rational or playful \n      388.348636\n      183901.969908\n      19472101524.800404\n    \n    \n      40\n      being settled in London only sixteen miles off was much beyond her daily reach \n      665.316057\n      227269.194290\n      36069296442.502388\n    \n  \n\n\n\n\ntest.S.reset_index().set_index('sent_str').pp3.sort_values(ascending=False).plot.barh(figsize=(5,15));"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#handling-missing-ngrams",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#handling-missing-ngrams",
    "title": "Inferring Language Models (v3)",
    "section": "Handling Missing Ngrams",
    "text": "Handling Missing Ngrams\n\nSeen ngram: $ $\nUnseen ngram with seen context: $ $\nUnseen ngram with unseen context: $ $\nUnknown Unigrams: <UNK>"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#effective-counts",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#effective-counts",
    "title": "Inferring Language Models (v3)",
    "section": "Effective Counts",
    "text": "Effective Counts\n$ (c_i + ) $"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#perplexity",
    "href": "lessons/M03_LanguageModels/M03_02_LanguageModels_V3.html#perplexity",
    "title": "Inferring Language Models (v3)",
    "section": "Perplexity",
    "text": "Perplexity\n$ PP(w) = 2^{-} $\n$ (w) = i(w) $\n$ PP(w) = 2^{} $"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_02_SimpleLangModel.html",
    "href": "lessons/M03_LanguageModels/M03_02_SimpleLangModel.html",
    "title": "Demonstration of Simple Language Model",
    "section": "",
    "text": "Course:  DS 5001\nModule:  03 Lab\nTopic:   Demonstration of Simple Language Model\nAuthor:  R.C. Alvarado\nDate:    31 January 2023\nPurpose: Demonstrates use of simple langauge model based on Berkeley restaurant data (from Jurafsky and Martin).\n\nSet Up\n\nimport pandas as pd\nimport numpy as np\n\n\n\nMake the Data\nWe convert these three tables into Pandas dataframes and use them to predict and generate sentences.\n\nFirst, we create the table of unigrams and their frrequencies, i.e. the priors or marginals of a bigram model.\nNote that this counts are relative to the entire corpus, not just the subset represented in the bigram table. So, we can’t compute the probabilities of the unigrams.\n\ndata1 = dict(\n    w0 = \"i want to eat chinese food lunch spend\".split(),\n    n = [2533, 927, 2417, 746, 158, 1093, 341, 278]\n)\n\n\ndf1 = pd.DataFrame(data1).set_index(['w0'])\ndf1.n = df1.n.astype('int')\n\nSince we will want to use unigram probabilities for our model, we will estimate the number of tokens in the corpus. We will chose \\(50000\\), based on the data found in The Berkeley Restaurant Project (BeRP) Transcripts. (We sum the counts in the wordhist.txt file and round down, since the data in the repo contain higher word counts for our words.)\n\nC = 50000\n\n\ndf1['p'] = df1.n / C\ndf1['i'] = np.log2(1/df1.p)\n\n\ndf1\n\n\n\n\n\n  \n    \n      \n      n\n      p\n      i\n    \n    \n      w0\n      \n      \n      \n    \n  \n  \n    \n      i\n      2533\n      0.05066\n      4.303009\n    \n    \n      want\n      927\n      0.01854\n      5.753215\n    \n    \n      to\n      2417\n      0.04834\n      4.370639\n    \n    \n      eat\n      746\n      0.01492\n      6.066609\n    \n    \n      chinese\n      158\n      0.00316\n      8.305860\n    \n    \n      food\n      1093\n      0.02186\n      5.515563\n    \n    \n      lunch\n      341\n      0.00682\n      7.196013\n    \n    \n      spend\n      278\n      0.00556\n      7.490699\n    \n  \n\n\n\n\nNext, we create a table joint probabilities that will be the basis for our bigram model.\n\ndata2 = [row.split(\", \") for row in \"\"\"\ni, i, 5\ni, want, 837\ni, eat, 9\ni, spend, 2\nwant, i, 2\nwant, to, 608\nwant, eat, 1\nwant, chinese, 6\nwant, food, 6\nwant, lunch, 5\nwant, spend, 1\nto, i, 2\nto, to, 4\nto, eat, 686\nto, chinese, 2\nto, lunch, 6\nto, spend, 211\neat, to, 2\neat, chinese, 16\neat, food, 2\neat, lunch, 42\nchinese, i, 1\nchinese, food, 82\nchinese, lunch, 1\nfood, i, 15\nfood, to, 15\nfood, chinese, 1\nfood, food, 4\nlunch, i, 2\nlunch, food, 1\nspend, i, 1\nspend, to, 1\n\"\"\".split(\"\\n\")[1:-1]]\n\n\ndf2 = pd.DataFrame(data2, columns = ['w0', 'w1', 'n']).set_index(['w0','w1'])\ndf2.n = df2.n.astype('int')\n\n\ndf2.sort_values('n', ascending=False)\n\n\n\n\n\n  \n    \n      \n      \n      n\n    \n    \n      w0\n      w1\n      \n    \n  \n  \n    \n      i\n      want\n      837\n    \n    \n      to\n      eat\n      686\n    \n    \n      want\n      to\n      608\n    \n    \n      to\n      spend\n      211\n    \n    \n      chinese\n      food\n      82\n    \n    \n      eat\n      lunch\n      42\n    \n    \n      chinese\n      16\n    \n    \n      food\n      to\n      15\n    \n    \n      i\n      15\n    \n    \n      i\n      eat\n      9\n    \n    \n      want\n      chinese\n      6\n    \n    \n      food\n      6\n    \n    \n      to\n      lunch\n      6\n    \n    \n      i\n      i\n      5\n    \n    \n      want\n      lunch\n      5\n    \n    \n      food\n      food\n      4\n    \n    \n      to\n      to\n      4\n    \n    \n      chinese\n      2\n    \n    \n      i\n      2\n    \n    \n      eat\n      to\n      2\n    \n    \n      food\n      2\n    \n    \n      lunch\n      i\n      2\n    \n    \n      want\n      i\n      2\n    \n    \n      i\n      spend\n      2\n    \n    \n      spend\n      i\n      1\n    \n    \n      lunch\n      food\n      1\n    \n    \n      chinese\n      lunch\n      1\n    \n    \n      food\n      chinese\n      1\n    \n    \n      want\n      eat\n      1\n    \n    \n      chinese\n      i\n      1\n    \n    \n      want\n      spend\n      1\n    \n    \n      spend\n      to\n      1\n    \n  \n\n\n\n\nNext, we compute conditional probabilities \\(p\\) with the formula \\(p(B|A) = \\large \\frac{P(A,B)}{P(A)}\\) converted to \\(\\large \\frac{C(A,B)}{C(A)}\\).\nWe also compute the information \\(i\\) for each \\(p\\). This will allow use to compute perplexity \\(pp\\) easily.\nNote that information \\(i\\) is just the negative of the log probability of the event, so we can use this to compute the log probability of sentences later.\n\ndf2['p'] = df2.n / df1.n\ndf2['i'] = np.log2(1/df2.p)\n\n\ndf2.sort_values('p', ascending=False).style.background_gradient(cmap='YlGnBu')\n\n\n\n\n  \n    \n       \n       \n      n\n      p\n      i\n    \n    \n      w0\n      w1\n       \n       \n       \n    \n  \n  \n    \n      want\n      to\n      608\n      0.655879\n      0.608498\n    \n    \n      chinese\n      food\n      82\n      0.518987\n      0.946229\n    \n    \n      i\n      want\n      837\n      0.330438\n      1.597548\n    \n    \n      to\n      eat\n      686\n      0.283823\n      1.816937\n    \n    \n      spend\n      211\n      0.087298\n      3.517903\n    \n    \n      eat\n      lunch\n      42\n      0.056300\n      4.150714\n    \n    \n      chinese\n      16\n      0.021448\n      5.543032\n    \n    \n      food\n      to\n      15\n      0.013724\n      6.187187\n    \n    \n      i\n      15\n      0.013724\n      6.187187\n    \n    \n      want\n      chinese\n      6\n      0.006472\n      7.271463\n    \n    \n      food\n      6\n      0.006472\n      7.271463\n    \n    \n      chinese\n      lunch\n      1\n      0.006329\n      7.303781\n    \n    \n      i\n      1\n      0.006329\n      7.303781\n    \n    \n      lunch\n      i\n      2\n      0.005865\n      7.413628\n    \n    \n      want\n      lunch\n      5\n      0.005394\n      7.534497\n    \n    \n      food\n      food\n      4\n      0.003660\n      8.094078\n    \n    \n      spend\n      i\n      1\n      0.003597\n      8.118941\n    \n    \n      to\n      1\n      0.003597\n      8.118941\n    \n    \n      i\n      eat\n      9\n      0.003553\n      8.136706\n    \n    \n      lunch\n      food\n      1\n      0.002933\n      8.413628\n    \n    \n      eat\n      to\n      2\n      0.002681\n      8.543032\n    \n    \n      food\n      2\n      0.002681\n      8.543032\n    \n    \n      to\n      lunch\n      6\n      0.002482\n      8.654039\n    \n    \n      want\n      i\n      2\n      0.002157\n      8.856426\n    \n    \n      i\n      i\n      5\n      0.001974\n      8.984703\n    \n    \n      to\n      to\n      4\n      0.001655\n      9.239002\n    \n    \n      want\n      spend\n      1\n      0.001079\n      9.856426\n    \n    \n      eat\n      1\n      0.001079\n      9.856426\n    \n    \n      food\n      chinese\n      1\n      0.000915\n      10.094078\n    \n    \n      to\n      chinese\n      2\n      0.000827\n      10.239002\n    \n    \n      i\n      2\n      0.000827\n      10.239002\n    \n    \n      i\n      spend\n      2\n      0.000790\n      10.306631\n    \n  \n\n\n\nWe convert a selection from the narrow table into a wide one, projecting the second index feature w1 onto the second axis.\nThis is strictly not necessary, but it is convenient for certain computations and it is visually appealing.\n\ndf3 = df2.p.unstack(fill_value=0)\n\n\ndf3.style.background_gradient(cmap='YlGnBu', axis=None)\n\n\n\n\n  \n    \n      w1\n      chinese\n      eat\n      food\n      i\n      lunch\n      spend\n      to\n      want\n    \n    \n      w0\n       \n       \n       \n       \n       \n       \n       \n       \n    \n  \n  \n    \n      chinese\n      0.000000\n      0.000000\n      0.518987\n      0.006329\n      0.006329\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      eat\n      0.021448\n      0.000000\n      0.002681\n      0.000000\n      0.056300\n      0.000000\n      0.002681\n      0.000000\n    \n    \n      food\n      0.000915\n      0.000000\n      0.003660\n      0.013724\n      0.000000\n      0.000000\n      0.013724\n      0.000000\n    \n    \n      i\n      0.000000\n      0.003553\n      0.000000\n      0.001974\n      0.000000\n      0.000790\n      0.000000\n      0.330438\n    \n    \n      lunch\n      0.000000\n      0.000000\n      0.002933\n      0.005865\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      spend\n      0.000000\n      0.000000\n      0.000000\n      0.003597\n      0.000000\n      0.000000\n      0.003597\n      0.000000\n    \n    \n      to\n      0.000827\n      0.283823\n      0.000000\n      0.000827\n      0.002482\n      0.087298\n      0.001655\n      0.000000\n    \n    \n      want\n      0.006472\n      0.001079\n      0.006472\n      0.002157\n      0.005394\n      0.001079\n      0.655879\n      0.000000\n    \n  \n\n\n\n\n\nApply the Model\nNow we generate sentences from the model and also compute their probabilities and perplexities.\nWe also demonstrate the relationship between sum of log probability, mean information, and perplexity.\n\nN = 20\nM = 10\nresults = []\n\n# Generate N sentences\nfor i in range(N):\n    \n    # Get a start word from the unigram list (since we don't have sentence boundaries in our bigram model\n    start = df1.sample(weights='n')\n    w0 = start.index[0]\n    i = start.i.values[0] # This is why we needed to estimate the number of tokens in the corpus C\n    \n    # Initialize sentence feature lists\n    W = [w0]\n    I = [i]\n    LP = [-i]\n    \n    # Select M words for each sentence\n    for j in range(M - 1):\n        w1 = df3.loc[w0].sample(weights=df3.loc[w0].values).index[0]\n        i = df2.loc[(w0, w1), 'i']\n        W.append(w1)\n        I.append(i)\n        LP.append(-i)\n        w0 = w1\n\n    # Compute sentence stats\n    S = pd.DataFrame(dict(w=W, i=I, lp=LP)) # This is expensive; we'd do it differently at scale\n    i_mean = S.i.mean()\n    lp_sum = S.lp.sum()\n    pp = 2**(i_mean)\n    sent_str = ' '.join(W)\n    \n    # Append to results \n    results.append((sent_str, lp_sum, i_mean, pp))\n    \n# Put results in a dataframe\nR = pd.DataFrame(results, columns=['sentence', 'logprob', 'i_mean', 'pp'])\n\n# Keep only unique sentences, i.e. treat sentences as index values\nsentence_counts = R.sentence.value_counts().to_frame('n')\nR = R.drop_duplicates()\nR = R.reset_index(drop=True).set_index('sentence')\nR['n'] = sentence_counts\n\n# Display results\nR.sort_values('i_mean', ascending=True).style.background_gradient(cmap=\"YlGnBu\")\n\n\n\n\n  \n    \n       \n      logprob\n      i_mean\n      pp\n      n\n    \n    \n      sentence\n       \n       \n       \n       \n    \n  \n  \n    \n      i want to eat lunch i want to eat lunch\n      -28.064031\n      2.806403\n      6.995383\n      3\n    \n    \n      i want to spend i want to spend i want\n      -33.586335\n      3.358634\n      10.257687\n      1\n    \n    \n      i want to spend to eat lunch i want to\n      -33.733223\n      3.373322\n      10.362659\n      1\n    \n    \n      to spend i want to eat lunch i want to\n      -33.800853\n      3.380085\n      10.411350\n      2\n    \n    \n      i want to eat chinese food to spend i want\n      -34.236831\n      3.423683\n      10.730780\n      2\n    \n    \n      to eat chinese food i want to spend i want\n      -34.304460\n      3.430446\n      10.781201\n      1\n    \n    \n      to spend to eat chinese food i want to eat\n      -34.523850\n      3.452385\n      10.946403\n      1\n    \n    \n      want to spend to eat lunch i want to eat\n      -35.402818\n      3.540282\n      11.634053\n      1\n    \n    \n      to spend to spend i want to eat chinese food\n      -38.156569\n      3.815657\n      14.080795\n      1\n    \n    \n      food i want to eat chinese food to spend to\n      -40.039024\n      4.003902\n      16.043337\n      1\n    \n    \n      i want to eat lunch food to eat lunch i\n      -40.458800\n      4.045880\n      16.517003\n      1\n    \n    \n      lunch i want to spend i want to eat food\n      -41.018544\n      4.101854\n      17.170432\n      1\n    \n    \n      i want to eat lunch food to eat lunch food\n      -41.458800\n      4.145880\n      17.702486\n      1\n    \n    \n      eat lunch food to eat lunch i want to eat\n      -42.222400\n      4.222240\n      18.664695\n      1\n    \n    \n      i want to eat lunch food to spend i eat\n      -46.851071\n      4.685107\n      25.725142\n      1\n    \n    \n      food chinese food to spend to spend to spend to\n      -57.653587\n      5.765359\n      54.393363\n      1\n    \n  \n\n\n\n\n\nInterpet Results\nCompare mean i to the sum of log p. Notice they are identical when normalized by mean.\n\n(R / R.mean()).sort_values('i_mean')[['i_mean','logprob']].reset_index().plot();\n\n\n\n\nCompre perplexity pp to mean information i. Notice how perplexity amplifies the difference.\n\n(R / R.mean()).sort_values('i_mean')[['i_mean','pp']].reset_index().plot();\n\n\n\n\nSo, we will use perplexity to measure of how well a model performs.\n\nR.reset_index().set_index('sentence').sort_values('pp').i_mean.sort_values(ascending=False)\\\n    .plot.barh(figsize=(7,7), title=\"Generated Sentences Sorted by Perplexity\");\n\n\n\n\nSince perplexity looks something like squaring the error of our model (where mean information is interpreted as error), let’s create a test feature that just squares mean information and compare to the other features.\n\nR['test'] = R.i_mean**2\n\n\n(R / R.mean()).sort_values('i_mean')[['i_mean', 'test','pp']].reset_index().plot();"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_03_Entropy-and-Perplexity.html",
    "href": "lessons/M03_LanguageModels/M03_03_Entropy-and-Perplexity.html",
    "title": "Entropy and Peplexity",
    "section": "",
    "text": "Entropy\nEntropy \\(H\\) is the expectation of information in a distribution.\nSelf-entropy \\(h\\) is the information of an event.\nInformation \\(i\\) is log normalized surprise of an event.\nSurprise \\(s\\) is just the inverse probability on an event.\nREDO: See https://stackoverflow.com/questions/54941966/how-can-i-calculate-perplexity-using-nltk\nPerplexity is a measure of how well a probabilistic model is able to predict a sample. It is calculated as 2 to the power of the cross-entropy of the model and the sample. The lower the perplexity, the better the model is at predicting the sample.\nHere is an example of how to calculate perplexity in Python using the Natural Language Toolkit (NLTK):\nIn this example, the sample text is passed to the FreqDist() function to create a frequency distribution of the words in the sample. This frequency distribution is then passed to the MLEProbDist() function to create a maximum likelihood estimate probability distribution. Finally, the logprob() function is used to calculate the log probability of each word in the sample, and these probabilities are summed and divided by the number of words in the sample to calculate the cross-entropy. The perplexity is then calculated by raising 2 to the power of the negative of the cross-entropy.\nChatGPT Jan 9 Version. Free Research Preview. Our goal"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_03_Entropy-and-Perplexity.html#probability-p",
    "href": "lessons/M03_LanguageModels/M03_03_Entropy-and-Perplexity.html#probability-p",
    "title": "Entropy and Peplexity",
    "section": "Probability \\(p\\)",
    "text": "Probability \\(p\\)\n\\(\\Large p = \\frac{n}{N}\\)\n\\(p(w) = \\Large\\frac{n_w}{N_{corpus}}\\)\np = n / n.sum()\nMost terms have low probability."
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_03_Entropy-and-Perplexity.html#surprise-s",
    "href": "lessons/M03_LanguageModels/M03_03_Entropy-and-Perplexity.html#surprise-s",
    "title": "Entropy and Peplexity",
    "section": "Surprise \\(s\\)",
    "text": "Surprise \\(s\\)\n\\(\\Large s = \\Large\\frac{1}{p}\\)\n\\(s(w) = p(w)^{-1}\\)\nSurrprise \\(s\\) increases as the inverse of \\(p\\). Note how inverting \\(p\\) adds variance to the long tail; the curve now looks like a simple quadratic. We can see a more gradual increase in surprise as terms become more rare."
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_03_Entropy-and-Perplexity.html#information-i",
    "href": "lessons/M03_LanguageModels/M03_03_Entropy-and-Perplexity.html#information-i",
    "title": "Entropy and Peplexity",
    "section": "Information \\(i\\)",
    "text": "Information \\(i\\)\n\\(\\Large i= log_2(s)\\)\n\\(i(w) = log_2(s(w))\\)\nAs normalized suprise, information now has a long tail structure. But notice also the range of information – it is between 1 and 18. What does this correspond to?"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_03_Entropy-and-Perplexity.html#entropy-h",
    "href": "lessons/M03_LanguageModels/M03_03_Entropy-and-Perplexity.html#entropy-h",
    "title": "Entropy and Peplexity",
    "section": "Entropy \\(h\\)",
    "text": "Entropy \\(h\\)\n\\(\\Large h = p i\\)\n\\(h(w) = p(w)i(w)\\)\nFor the self-entropy of each term, we multiply \\(p\\) and \\(i\\). When summed, this will give us the expectation of the information in the distribution, i.e. it’s entropy."
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_03_Entropy-and-Perplexity.html#perplexity-pp",
    "href": "lessons/M03_LanguageModels/M03_03_Entropy-and-Perplexity.html#perplexity-pp",
    "title": "Entropy and Peplexity",
    "section": "Perplexity \\(PP\\)",
    "text": "Perplexity \\(PP\\)\n\\(\\Large PP = \\Large 2^{i}\\)"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_03_Entropy-and-Perplexity.html#chiasmus",
    "href": "lessons/M03_LanguageModels/M03_03_Entropy-and-Perplexity.html#chiasmus",
    "title": "Entropy and Peplexity",
    "section": "Chiasmus",
    "text": "Chiasmus\nThe process of computing entropy follows a chiasmus pattern.\n\\(A_1 \\rightarrow B_1 \\rightarrow B_2 \\rightarrow A_2\\)\n\n\\(p \\rightarrow s \\rightarrow i \\rightarrow h\\)\n\\(A: \\{p,h\\}\\)\n\\(B: \\{s,i\\}\\)"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_03_Entropy-and-Perplexity.html#set-up",
    "href": "lessons/M03_LanguageModels/M03_03_Entropy-and-Perplexity.html#set-up",
    "title": "Entropy and Peplexity",
    "section": "Set up",
    "text": "Set up\n\nimport pandas as pd\n\n\nimport configparser\nconfig = configparser.ConfigParser()\nconfig.read(\"../env.ini\")\n# data_dir = config['DEFAULT']['data_home']\noutput_dir = config['DEFAULT']['output_dir']\n\n\nohco = ['book_id','chap_num','para_num','sent_num','token_num']"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_03_Entropy-and-Perplexity.html#import-data",
    "href": "lessons/M03_LanguageModels/M03_03_Entropy-and-Perplexity.html#import-data",
    "title": "Entropy and Peplexity",
    "section": "Import data",
    "text": "Import data\n\nK = pd.read_csv(f\"{output_dir}/austen-combo-TOKENS-v2.csv\").set_index(ohco)\nV = pd.read_csv(f\"{output_dir}/austen-combo-VOCAB-v2.csv\").set_index('term_str')\n\n\nK.head()\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      \n      token_str\n      term_str\n    \n    \n      book_id\n      chap_num\n      para_num\n      sent_num\n      token_num\n      \n      \n    \n  \n  \n    \n      1\n      1\n      1\n      0\n      0\n      The\n      the\n    \n    \n      1\n      family\n      family\n    \n    \n      2\n      of\n      of\n    \n    \n      3\n      Dashwood\n      dashwood\n    \n    \n      4\n      had\n      had\n    \n  \n\n\n\n\n\nV.head()\n\n\n\n\n\n  \n    \n      \n      n\n      n_chars\n      p\n      i\n      h\n    \n    \n      term_str\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      3\n      1\n      0.000015\n      16.058901\n      0.000235\n    \n    \n      15\n      1\n      2\n      0.000005\n      17.643863\n      0.000086\n    \n    \n      16\n      1\n      2\n      0.000005\n      17.643863\n      0.000086\n    \n    \n      1760\n      1\n      4\n      0.000005\n      17.643863\n      0.000086\n    \n    \n      1784\n      1\n      4\n      0.000005\n      17.643863\n      0.000086\n    \n  \n\n\n\n\nAssumes language models have been created.\n\nLM = {}\nfor n in range(1, 4):\n    widx = [f\"w{i}\" for i in range(n)]\n    LM[n] = pd.read_csv(f\"{output_dir}/austen-combo-LM{n}-v2.csv\").set_index(widx)\n\n\nLM[1]\n\n\n\n\n\n  \n    \n      \n      n\n      p\n      i\n      h\n    \n    \n      w0\n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      3\n      0.000013\n      16.239120\n      0.000210\n    \n    \n      1760\n      1\n      0.000004\n      17.824082\n      0.000077\n    \n    \n      1784\n      1\n      0.000004\n      17.824082\n      0.000077\n    \n    \n      1785\n      1\n      0.000004\n      17.824082\n      0.000077\n    \n    \n      1787\n      1\n      0.000004\n      17.824082\n      0.000077\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      youth\n      22\n      0.000095\n      13.364651\n      0.001267\n    \n    \n      youthful\n      3\n      0.000013\n      16.239120\n      0.000210\n    \n    \n      zeal\n      7\n      0.000030\n      15.016727\n      0.000453\n    \n    \n      zealous\n      4\n      0.000017\n      15.824082\n      0.000273\n    \n    \n      zealously\n      2\n      0.000009\n      16.824082\n      0.000145\n    \n  \n\n8206 rows × 4 columns"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_03_Entropy-and-Perplexity.html#cross-entropy-and-perplexity",
    "href": "lessons/M03_LanguageModels/M03_03_Entropy-and-Perplexity.html#cross-entropy-and-perplexity",
    "title": "Entropy and Peplexity",
    "section": "Cross Entropy and Perplexity",
    "text": "Cross Entropy and Perplexity\n\nProbabilities of Sequences\n$ W = W_1^N = (w_1, w_2 … w_N)$\nTrue distribution: $ p = p(W) $\nModel distribution: $ q = q(W) $\n\n\nCross Entropy\n$ H(p, q) = - _{x}^{} p(x) log_2(q(x)) $\n$ H(p, q) = _{x}^{} p(x) log_2() $\n$ i_q(x) = log_2() $\n$ H(p, q) = _{x} p(x) i_q(x) $\n$ H(p, q) = $\n\n\nCross Entropy relative to MaxEnt\n$ N = C(x) = _x c(x) $\n$ p_{u} = $\n$ H_{cross} = H(p_u, q) $\n$ H_{cross} = _{x} i(x) $\n$ H_{cross} = _{x} i(x) $\n$ H_{cross} = $\n$ H_{cross} = $\n\nPerplexity\n$ PP(W) = P(w_1, w_2 … w_N)^{-1/N} $\n$ PP(p) = 2^{H(p)}$\n$ PP(p_u, q) = 2^{H_{cross}}$\n\n\nRedundancy\n$ H_{max} = log_2(N) $\n$ H_{max} = i(p_u) $\n$ R = 1 - $"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_03_Entropy-and-Perplexity.html#from-j-m",
    "href": "lessons/M03_LanguageModels/M03_03_Entropy-and-Perplexity.html#from-j-m",
    "title": "Entropy and Peplexity",
    "section": "From J & M",
    "text": "From J & M"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_03_Entropy-and-Perplexity.html#from-stack-overflow",
    "href": "lessons/M03_LanguageModels/M03_03_Entropy-and-Perplexity.html#from-stack-overflow",
    "title": "Entropy and Peplexity",
    "section": "From Stack Overflow",
    "text": "From Stack Overflow\nhttps://stats.stackexchange.com/questions/129352/how-to-find-the-perplexity-of-a-corpus"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_04-Entropy-and-Term-Length.html",
    "href": "lessons/M03_LanguageModels/M03_04-Entropy-and-Term-Length.html",
    "title": "Entropy and Term Length",
    "section": "",
    "text": "Set Up\nNote the range of \\(i\\): it is one the same order as the range of n_chars.\nNow let’s look at how these four features relate to term length \\(L\\), the number of characters in a term (n_chars)\nCan we learn anything about the structure of our corpus from distribution of term lengths? If information is related to length, there must be something to observe."
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_04-Entropy-and-Term-Length.html#libraries",
    "href": "lessons/M03_LanguageModels/M03_04-Entropy-and-Term-Length.html#libraries",
    "title": "Entropy and Term Length",
    "section": "Libraries",
    "text": "Libraries\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly_express as px\n\n\nsns.set()"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_04-Entropy-and-Term-Length.html#config",
    "href": "lessons/M03_LanguageModels/M03_04-Entropy-and-Term-Length.html#config",
    "title": "Entropy and Term Length",
    "section": "Config",
    "text": "Config\n\nimport configparser\nconfig = configparser.ConfigParser()\nconfig.read(\"../env.ini\")\ndata_dir = config['DEFAULT']['data_home']\noutput_dir = config['DEFAULT']['output_dir']\n\n\nOHCO = ['book_id', 'chap_num', 'para_num', 'sent_num', 'token_num']"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_04-Entropy-and-Term-Length.html#import-data",
    "href": "lessons/M03_LanguageModels/M03_04-Entropy-and-Term-Length.html#import-data",
    "title": "Entropy and Term Length",
    "section": "Import Data",
    "text": "Import Data\n\nK = pd.read_csv(f\"{output_dir}/austen-combo-TOKENS.csv\").set_index(OHCO)\nV = pd.read_csv(f\"{output_dir}/austen-combo-VOCAB.csv\").set_index('term_str')\n\n# Add some features for this analysis\nV['s'] = 1 / V.p\n\n\nK.head()\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      \n      token_str\n      term_str\n    \n    \n      book_id\n      chap_num\n      para_num\n      sent_num\n      token_num\n      \n      \n    \n  \n  \n    \n      1\n      1\n      1\n      0\n      0\n      The\n      the\n    \n    \n      1\n      family\n      family\n    \n    \n      2\n      of\n      of\n    \n    \n      3\n      Dashwood\n      dashwood\n    \n    \n      4\n      had\n      had\n    \n  \n\n\n\n\n\nV.head()\n\n\n\n\n\n  \n    \n      \n      n\n      n_chars\n      p\n      i\n      h\n      s\n    \n    \n      term_str\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      3\n      1\n      0.000015\n      16.058901\n      0.000235\n      68267.0\n    \n    \n      15\n      1\n      2\n      0.000005\n      17.643863\n      0.000086\n      204801.0\n    \n    \n      16\n      1\n      2\n      0.000005\n      17.643863\n      0.000086\n      204801.0\n    \n    \n      1760\n      1\n      4\n      0.000005\n      17.643863\n      0.000086\n      204801.0\n    \n    \n      1784\n      1\n      4\n      0.000005\n      17.643863\n      0.000086\n      204801.0"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_04-Entropy-and-Term-Length.html#corpus-frequency-of-terms",
    "href": "lessons/M03_LanguageModels/M03_04-Entropy-and-Term-Length.html#corpus-frequency-of-terms",
    "title": "Entropy and Term Length",
    "section": "Corpus frequency of terms",
    "text": "Corpus frequency of terms\nSome words are very high frequency, but the vast majority have very low frequencies. We will explore the significane of this difference in the next module. NB: By “frequency” we mean relative frequency, which we use to estimate \\(p\\).\n\nV.p.sort_values(ascending=False).plot(style='.-', rot=45);"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_04-Entropy-and-Term-Length.html#frequency-of-frequencies",
    "href": "lessons/M03_LanguageModels/M03_04-Entropy-and-Term-Length.html#frequency-of-frequencies",
    "title": "Entropy and Term Length",
    "section": "Frequency of frequencies",
    "text": "Frequency of frequencies\nThe same data, but looked at in terms of the distribition of probabilities. This is so that it can be compared to the following graphs of \\(s\\), \\(i\\), and \\(h\\).\n\nV.p.round(4).value_counts().sort_index().plot(style='.-');"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_04-Entropy-and-Term-Length.html#create-a-dataframe-for-l",
    "href": "lessons/M03_LanguageModels/M03_04-Entropy-and-Term-Length.html#create-a-dataframe-for-l",
    "title": "Entropy and Term Length",
    "section": "Create a dataframe for \\(L\\)",
    "text": "Create a dataframe for \\(L\\)\n\nVG = V.groupby('n_chars')\n\n# Distribution of L over the vocabulary (terms)\nL = VG.n.count().to_frame('v_n')\nL['v_p'] = L.v_n / L.v_n.sum()\nL['v_s'] = 1 / L.v_p\nL['v_i'] = np.log2(L.v_s)\nL['v_h'] = L.v_p * L.v_i\n\n# Distribution of L over the corpus (tokens)\nL['k_n'] = VG.n.sum()\nL['k_p'] = L.k_n / L.k_n.sum()\nL['k_s'] = 1 / L.k_p\nL['k_i'] = np.log2(L.k_s)\nL['k_h'] = L.k_p * L.k_i\n\n# Aggregate probability features over tokens\nfor func in ['sum','mean']:\n    for x in 'psih':\n        L[f\"k{func}_{x}\"] = VG[x].agg(func)    \n        \n# L.index.name = 'n_chars'        \nL.columns = pd.Index([tuple(col.split('_')) for col in L.columns])\nL.T.index.names = ['pop','stat']\nLnorm = (L - L.mean()) / L.std()"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_04-Entropy-and-Term-Length.html#l-distributions",
    "href": "lessons/M03_LanguageModels/M03_04-Entropy-and-Term-Length.html#l-distributions",
    "title": "Entropy and Term Length",
    "section": "\\(L\\) distributions",
    "text": "\\(L\\) distributions\nCompare corpus and vocab frequencies. Why are the distributions different?\n\nfig, axes = plt.subplots(nrows=2, ncols=2, sharex=True, sharey='row', figsize=(10,5))\nL.v.p.plot.bar( ax=axes[0,0], title='L over Vocab')\nL.k.p.plot.bar( ax=axes[0,1], title='L over Corpus')\nLnorm.v.p.plot.bar( ax=axes[1,0], rot=0)\nLnorm.k.p.plot.bar( ax=axes[1,1], rot=0);\n\n\n\n\n\nL.style.background_gradient()\n\n\n\n\n  \n    \n      pop\n      v\n      k\n      ksum\n      kmean\n    \n    \n      stat\n      n\n      p\n      s\n      i\n      h\n      n\n      p\n      s\n      i\n      h\n      p\n      s\n      i\n      h\n      p\n      s\n      i\n      h\n    \n    \n      n_chars\n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n  \n  \n    \n      1\n      18\n      0.002185\n      457.666667\n      8.838153\n      0.019311\n      8118\n      0.039638\n      25.228012\n      4.656955\n      0.184595\n      0.039638\n      1608438.956525\n      258.487270\n      0.247346\n      0.002202\n      89357.719807\n      14.360404\n      0.013741\n    \n    \n      2\n      45\n      0.005462\n      183.066667\n      7.516225\n      0.041057\n      38051\n      0.185795\n      5.382276\n      2.428216\n      0.451150\n      0.185795\n      2117546.029359\n      509.297513\n      1.197241\n      0.004129\n      47056.578430\n      11.317723\n      0.026605\n    \n    \n      3\n      193\n      0.023428\n      42.683938\n      5.415621\n      0.126877\n      48161\n      0.235160\n      4.252424\n      2.088286\n      0.491081\n      0.235160\n      11143806.247405\n      2633.617269\n      1.605814\n      0.001218\n      57739.928743\n      13.645685\n      0.008320\n    \n    \n      4\n      620\n      0.075261\n      13.287097\n      3.731954\n      0.280871\n      36895\n      0.180150\n      5.550915\n      2.472726\n      0.445463\n      0.180150\n      45575529.593813\n      9010.289898\n      1.707454\n      0.000291\n      73508.918700\n      14.532726\n      0.002754\n    \n    \n      5\n      897\n      0.108886\n      9.183946\n      3.199114\n      0.348338\n      19846\n      0.096904\n      10.319510\n      3.367303\n      0.326304\n      0.096904\n      83133015.010730\n      13840.426679\n      1.045321\n      0.000108\n      92678.946500\n      15.429684\n      0.001165\n    \n    \n      6\n      1154\n      0.140083\n      7.138648\n      2.835651\n      0.397225\n      16045\n      0.078344\n      12.764163\n      3.674027\n      0.287839\n      0.078344\n      114267427.615405\n      18100.547960\n      0.930086\n      0.000068\n      99018.568124\n      15.685050\n      0.000806\n    \n    \n      7\n      1301\n      0.157927\n      6.332052\n      2.662673\n      0.420507\n      13795\n      0.067358\n      14.846031\n      3.892005\n      0.262158\n      0.067358\n      136598247.628890\n      20671.618172\n      0.835109\n      0.000052\n      104994.809861\n      15.889022\n      0.000642\n    \n    \n      8\n      1177\n      0.142874\n      6.999150\n      2.807180\n      0.401074\n      9031\n      0.044096\n      22.677555\n      4.503193\n      0.198575\n      0.044096\n      131142358.545741\n      18920.136277\n      0.575335\n      0.000037\n      111420.865374\n      16.074882\n      0.000489\n    \n    \n      9\n      1068\n      0.129643\n      7.713483\n      2.947382\n      0.382108\n      6627\n      0.032358\n      30.904029\n      4.949723\n      0.160164\n      0.032358\n      125694245.901375\n      17353.847265\n      0.438095\n      0.000030\n      117691.241481\n      16.248921\n      0.000410\n    \n    \n      10\n      770\n      0.093469\n      10.698701\n      3.419364\n      0.319605\n      4212\n      0.020566\n      48.623219\n      5.603574\n      0.115245\n      0.020566\n      89409020.985667\n      12538.374112\n      0.284873\n      0.000027\n      116115.611670\n      16.283603\n      0.000370\n    \n    \n      11\n      481\n      0.058388\n      17.126819\n      4.098185\n      0.239285\n      2045\n      0.009985\n      100.147188\n      6.645978\n      0.066362\n      0.009985\n      61999654.543214\n      7932.892196\n      0.144300\n      0.000021\n      128897.410693\n      16.492499\n      0.000300\n    \n    \n      12\n      284\n      0.034474\n      29.007042\n      4.858331\n      0.167488\n      1197\n      0.005845\n      171.095238\n      7.418656\n      0.043360\n      0.005845\n      38730443.020562\n      4725.127096\n      0.082405\n      0.000021\n      136374.799368\n      16.637771\n      0.000290\n    \n    \n      13\n      136\n      0.016509\n      60.573529\n      5.920616\n      0.097743\n      564\n      0.002754\n      363.122340\n      8.504312\n      0.023420\n      0.002754\n      17692419.721134\n      2247.716721\n      0.039933\n      0.000020\n      130091.321479\n      16.527329\n      0.000294\n    \n    \n      14\n      60\n      0.007283\n      137.300000\n      7.101188\n      0.051720\n      156\n      0.000762\n      1312.826923\n      10.358461\n      0.007890\n      0.000762\n      8626781.593651\n      1010.108566\n      0.011832\n      0.000013\n      143779.693228\n      16.835143\n      0.000197\n    \n    \n      15\n      27\n      0.003277\n      305.111111\n      8.253191\n      0.027050\n      49\n      0.000239\n      4179.612245\n      12.029153\n      0.002878\n      0.000239\n      4058473.150000\n      459.570526\n      0.003954\n      0.000009\n      150313.820370\n      17.021131\n      0.000146\n    \n    \n      16\n      4\n      0.000486\n      2059.500000\n      11.008078\n      0.005345\n      6\n      0.000029\n      34133.500000\n      15.058901\n      0.000441\n      0.000029\n      682670.000000\n      68.990490\n      0.000494\n      0.000007\n      170667.500000\n      17.247623\n      0.000123\n    \n    \n      17\n      3\n      0.000364\n      2746.000000\n      11.423116\n      0.004160\n      3\n      0.000015\n      68267.000000\n      16.058901\n      0.000235\n      0.000015\n      614403.000000\n      52.931590\n      0.000258\n      0.000005\n      204801.000000\n      17.643863\n      0.000086\n    \n  \n\n\n\n\nLnorm.v.plot(figsize=(20,5))\n\n<Axes: xlabel='n_chars'>\n\n\n\n\n\n\nLnorm.ksum.plot(figsize=(20,5));\n\n\n\n\nNOTE: Notice how easy it is to combare features in the shaded table vs the chart."
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_04-Entropy-and-Term-Length.html#plot-all",
    "href": "lessons/M03_LanguageModels/M03_04-Entropy-and-Term-Length.html#plot-all",
    "title": "Entropy and Term Length",
    "section": "Plot All",
    "text": "Plot All\n\nfig, axes = plt.subplots(ncols=4, nrows=6, figsize=(20,20), sharex=True)\nfig.tight_layout(pad=2.5)\nfor i, x in enumerate(list(\"psih\")):\n    \n#     ax = axes[0, i]\n    \n#     # Counts \n#     V[x].value_counts().plot(style='*-', ax=ax)\n#     ax.set_title(f\"${x}$\")    \n\n    ax = axes[0, i]\n    \n    # Means and points\n    V.plot('n_chars', x, style='.', ax=ax, legend=False)\n    L.kmean[x].plot(ax=ax);\n    ax.set_title(f'corpus mean {x}')\n\n    ax = axes[1, i]\n    \n    # Variances \n    sns.boxplot(data=V.reset_index(), x='n_chars', y=x, ax=ax)\n    ax.set_title(f'corpus variance {x}')\n\n    ax = axes[2, i]\n\n    # Ksum\n    L.ksum[x].plot(ax=ax)\n    ax.set_title(f'corpus sum {x}')\n    \n    ax = axes[3, i]\n    \n    # Kmean\n    L.kmean[x].plot(ax=ax)\n    ax.set_title(f\"corpus mean {x}\")\n\n    ax = axes[4, i]\n    \n    # K\n    L.k[x].plot(ax=ax)\n    ax.set_title(f\"corpus {x}\")\n\n    ax = axes[5, i]\n    \n    # V\n    L.v[x].plot(ax=ax)\n    ax.set_title(f\"vocab {x}\")\n\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n\n\n\n\n\n\ndef plot_chiasmus(df, kind='bar'):\n    fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(10,6), sharex=True)\n    fig.tight_layout()\n    df.p.plot(kind=kind, ax=axes[0,0], title='P')\n    df.s.plot(kind=kind, ax=axes[0,1], title='S')\n    df.i.plot(kind=kind, ax=axes[1,1], title='I', rot=0)\n    df.h.plot(kind=kind, ax=axes[1,0], title='H', rot=0)\n\n\n# plot_chiasmus(Lnorm.v, 'bar')\n\n\n# plot_chiasmus(Lnorm.k, 'bar')\n\n\n# plot_chiasmus(Lnorm.ksum, 'bar')\n\n\n# plot_chiasmus(Lnorm.kmean, 'bar')"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_04-Entropy-and-Term-Length.html#book-1-ss",
    "href": "lessons/M03_LanguageModels/M03_04-Entropy-and-Term-Length.html#book-1-ss",
    "title": "Entropy and Term Length",
    "section": "Book 1 (SS)",
    "text": "Book 1 (SS)\n\nT1.loc[1, 5:].style.background_gradient(axis=1)\n\n\n\n\n  \n    \n      n_chars\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n    \n    \n      chap_num\n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n  \n  \n    \n      1\n      166\n      128\n      106\n      99\n      54\n      32\n      22\n      9\n      2\n      3\n      0\n      0\n      0\n    \n    \n      2\n      196\n      139\n      133\n      87\n      51\n      32\n      20\n      10\n      4\n      0\n      0\n      0\n      0\n    \n    \n      3\n      143\n      112\n      108\n      93\n      51\n      35\n      20\n      18\n      9\n      1\n      0\n      0\n      0\n    \n    \n      4\n      191\n      164\n      133\n      84\n      74\n      48\n      24\n      19\n      9\n      3\n      0\n      0\n      0\n    \n    \n      5\n      107\n      69\n      66\n      58\n      38\n      25\n      17\n      6\n      3\n      2\n      1\n      0\n      0\n    \n    \n      6\n      160\n      106\n      101\n      66\n      40\n      23\n      13\n      10\n      3\n      0\n      0\n      0\n      0\n    \n    \n      7\n      137\n      98\n      77\n      73\n      61\n      30\n      17\n      9\n      6\n      0\n      0\n      0\n      0\n    \n    \n      8\n      106\n      85\n      102\n      59\n      55\n      20\n      14\n      8\n      6\n      1\n      0\n      1\n      0\n    \n    \n      9\n      195\n      123\n      129\n      118\n      56\n      34\n      21\n      9\n      4\n      0\n      0\n      0\n      0\n    \n    \n      10\n      168\n      168\n      157\n      112\n      82\n      64\n      29\n      18\n      9\n      3\n      1\n      0\n      0\n    \n    \n      11\n      140\n      112\n      100\n      82\n      54\n      45\n      29\n      16\n      4\n      1\n      0\n      0\n      0\n    \n    \n      12\n      159\n      131\n      98\n      80\n      50\n      32\n      15\n      9\n      4\n      2\n      0\n      0\n      0\n    \n    \n      13\n      210\n      134\n      114\n      103\n      50\n      41\n      13\n      12\n      2\n      1\n      0\n      0\n      0\n    \n    \n      14\n      170\n      105\n      108\n      64\n      46\n      38\n      12\n      10\n      8\n      1\n      0\n      0\n      0\n    \n    \n      15\n      208\n      181\n      165\n      93\n      92\n      91\n      33\n      16\n      9\n      4\n      1\n      1\n      0\n    \n    \n      16\n      201\n      179\n      149\n      112\n      65\n      50\n      19\n      9\n      6\n      3\n      0\n      0\n      0\n    \n    \n      17\n      159\n      148\n      109\n      80\n      54\n      24\n      15\n      6\n      4\n      1\n      1\n      0\n      0\n    \n    \n      18\n      132\n      118\n      112\n      80\n      48\n      30\n      28\n      12\n      4\n      1\n      2\n      0\n      0\n    \n    \n      19\n      274\n      210\n      219\n      132\n      79\n      56\n      28\n      11\n      8\n      1\n      0\n      0\n      0\n    \n    \n      20\n      211\n      186\n      121\n      73\n      52\n      44\n      13\n      12\n      3\n      1\n      0\n      0\n      0\n    \n    \n      21\n      312\n      229\n      192\n      134\n      114\n      48\n      28\n      20\n      9\n      2\n      1\n      0\n      0\n    \n    \n      22\n      248\n      226\n      185\n      96\n      94\n      57\n      29\n      19\n      8\n      1\n      0\n      0\n      0\n    \n    \n      23\n      239\n      190\n      158\n      116\n      88\n      61\n      32\n      22\n      9\n      3\n      1\n      0\n      0\n    \n    \n      24\n      221\n      184\n      124\n      81\n      56\n      43\n      27\n      6\n      4\n      1\n      0\n      0\n      1\n    \n    \n      25\n      181\n      157\n      117\n      104\n      51\n      54\n      18\n      17\n      5\n      2\n      0\n      0\n      0\n    \n    \n      26\n      244\n      205\n      169\n      146\n      83\n      54\n      31\n      22\n      3\n      4\n      0\n      0\n      1\n    \n    \n      27\n      219\n      201\n      183\n      130\n      88\n      63\n      28\n      15\n      6\n      2\n      0\n      0\n      0\n    \n    \n      28\n      150\n      113\n      94\n      88\n      52\n      37\n      14\n      7\n      8\n      1\n      2\n      0\n      0\n    \n    \n      29\n      368\n      294\n      223\n      193\n      101\n      80\n      32\n      21\n      9\n      2\n      2\n      0\n      0\n    \n    \n      30\n      292\n      223\n      166\n      125\n      48\n      52\n      21\n      9\n      6\n      1\n      2\n      0\n      0\n    \n    \n      31\n      338\n      294\n      281\n      186\n      115\n      98\n      53\n      15\n      6\n      1\n      1\n      0\n      0\n    \n    \n      32\n      249\n      222\n      171\n      126\n      82\n      57\n      22\n      13\n      7\n      2\n      0\n      0\n      0\n    \n    \n      33\n      275\n      222\n      194\n      136\n      103\n      48\n      36\n      18\n      7\n      3\n      0\n      0\n      0\n    \n    \n      34\n      255\n      223\n      220\n      120\n      81\n      51\n      35\n      26\n      9\n      1\n      1\n      0\n      1\n    \n    \n      35\n      198\n      225\n      134\n      95\n      68\n      53\n      12\n      14\n      6\n      1\n      0\n      0\n      0\n    \n    \n      36\n      337\n      234\n      241\n      118\n      101\n      70\n      41\n      22\n      9\n      2\n      2\n      0\n      0\n    \n    \n      37\n      438\n      346\n      291\n      184\n      134\n      91\n      30\n      10\n      13\n      2\n      1\n      0\n      0\n    \n    \n      38\n      283\n      250\n      155\n      127\n      61\n      37\n      20\n      11\n      3\n      0\n      0\n      0\n      0\n    \n    \n      39\n      200\n      177\n      142\n      93\n      68\n      39\n      25\n      4\n      8\n      1\n      0\n      0\n      0\n    \n    \n      40\n      231\n      194\n      155\n      92\n      68\n      48\n      29\n      16\n      10\n      2\n      0\n      0\n      0\n    \n    \n      41\n      236\n      228\n      179\n      109\n      68\n      46\n      24\n      21\n      4\n      0\n      1\n      0\n      0\n    \n    \n      42\n      209\n      135\n      144\n      109\n      70\n      35\n      20\n      11\n      4\n      0\n      1\n      0\n      0\n    \n    \n      43\n      354\n      304\n      260\n      201\n      131\n      67\n      40\n      20\n      13\n      4\n      0\n      0\n      0\n    \n    \n      44\n      497\n      417\n      350\n      214\n      164\n      114\n      53\n      35\n      14\n      5\n      0\n      0\n      0\n    \n    \n      45\n      205\n      160\n      157\n      117\n      78\n      59\n      28\n      9\n      7\n      2\n      0\n      0\n      0\n    \n    \n      46\n      260\n      242\n      212\n      173\n      87\n      85\n      30\n      16\n      5\n      2\n      1\n      0\n      0\n    \n    \n      47\n      209\n      189\n      171\n      136\n      69\n      49\n      33\n      8\n      7\n      5\n      1\n      0\n      0\n    \n    \n      48\n      124\n      120\n      100\n      46\n      46\n      26\n      14\n      5\n      2\n      1\n      0\n      0\n      0\n    \n    \n      49\n      392\n      379\n      327\n      186\n      123\n      101\n      45\n      26\n      18\n      2\n      0\n      0\n      0\n    \n    \n      50\n      229\n      228\n      202\n      153\n      93\n      79\n      26\n      24\n      8\n      3\n      1\n      0\n      0\n    \n  \n\n\n\n\nsns.boxplot(data=T2.loc[1]);\n\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n\n\n\n\n\n\nT2.loc[1, 1:].plot(figsize=(20,10), style='.-');\n\n\n\n\n\nHAC(T2.loc[1]).plot()\n\n<Figure size 640x480 with 0 Axes>"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_04-Entropy-and-Term-Length.html#book-2-p",
    "href": "lessons/M03_LanguageModels/M03_04-Entropy-and-Term-Length.html#book-2-p",
    "title": "Entropy and Term Length",
    "section": "Book 2 (P)",
    "text": "Book 2 (P)\n\nT1.loc[2, 5:].style.background_gradient(axis=1)\n\n\n\n\n  \n    \n      n_chars\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n    \n    \n      chap_num\n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n  \n  \n    \n      1\n      291\n      223\n      155\n      142\n      102\n      59\n      28\n      23\n      8\n      6\n      3\n      0\n      0\n    \n    \n      2\n      215\n      157\n      135\n      98\n      81\n      42\n      21\n      19\n      10\n      2\n      2\n      0\n      0\n    \n    \n      3\n      285\n      237\n      190\n      123\n      97\n      42\n      28\n      10\n      10\n      2\n      1\n      0\n      0\n    \n    \n      4\n      184\n      138\n      137\n      88\n      63\n      63\n      25\n      17\n      5\n      3\n      4\n      0\n      0\n    \n    \n      5\n      334\n      261\n      198\n      127\n      114\n      66\n      33\n      17\n      6\n      1\n      1\n      0\n      0\n    \n    \n      6\n      405\n      259\n      243\n      170\n      129\n      62\n      33\n      28\n      13\n      1\n      0\n      1\n      0\n    \n    \n      7\n      378\n      234\n      207\n      111\n      113\n      42\n      22\n      20\n      5\n      2\n      0\n      0\n      0\n    \n    \n      8\n      344\n      237\n      210\n      132\n      109\n      55\n      22\n      16\n      10\n      4\n      1\n      0\n      0\n    \n    \n      9\n      261\n      239\n      230\n      103\n      127\n      62\n      22\n      15\n      5\n      1\n      0\n      0\n      0\n    \n    \n      10\n      422\n      327\n      269\n      143\n      134\n      62\n      33\n      17\n      11\n      2\n      0\n      0\n      0\n    \n    \n      11\n      313\n      225\n      258\n      156\n      126\n      77\n      36\n      20\n      6\n      3\n      0\n      0\n      0\n    \n    \n      12\n      502\n      441\n      385\n      228\n      200\n      103\n      40\n      22\n      15\n      2\n      0\n      0\n      0\n    \n    \n      13\n      295\n      216\n      175\n      118\n      83\n      58\n      23\n      14\n      4\n      2\n      1\n      0\n      0\n    \n    \n      14\n      245\n      183\n      193\n      98\n      82\n      44\n      14\n      13\n      5\n      3\n      1\n      0\n      0\n    \n    \n      15\n      300\n      243\n      169\n      120\n      87\n      58\n      39\n      16\n      10\n      2\n      2\n      0\n      0\n    \n    \n      16\n      248\n      208\n      172\n      81\n      79\n      52\n      28\n      24\n      6\n      4\n      0\n      0\n      0\n    \n    \n      17\n      374\n      285\n      223\n      145\n      131\n      68\n      45\n      29\n      15\n      3\n      0\n      0\n      0\n    \n    \n      18\n      394\n      318\n      305\n      133\n      121\n      60\n      20\n      18\n      3\n      1\n      0\n      1\n      0\n    \n    \n      19\n      200\n      196\n      176\n      90\n      85\n      38\n      22\n      16\n      8\n      2\n      0\n      0\n      0\n    \n    \n      20\n      321\n      264\n      261\n      137\n      123\n      75\n      34\n      22\n      10\n      2\n      3\n      0\n      0\n    \n    \n      21\n      705\n      592\n      434\n      225\n      198\n      122\n      71\n      37\n      24\n      8\n      1\n      0\n      0\n    \n    \n      22\n      542\n      452\n      399\n      226\n      225\n      122\n      54\n      30\n      13\n      8\n      4\n      1\n      0\n    \n    \n      23\n      716\n      478\n      449\n      281\n      214\n      145\n      51\n      34\n      19\n      3\n      1\n      1\n      0\n    \n    \n      24\n      146\n      125\n      118\n      74\n      67\n      39\n      23\n      9\n      9\n      2\n      0\n      0\n      0\n    \n  \n\n\n\n\nsns.boxplot(data=T2.loc[2]);\n\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n\n\n\n\n\n\n# sns.relplot?\n\n\nsns.relplot(data=T2.loc[2], kind='line', aspect=2.5);\n\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\nHAC(T2.loc[2]).plot()\n\n<Figure size 640x480 with 0 Axes>"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_EZ.html",
    "href": "lessons/M03_LanguageModels/M03_EZ.html",
    "title": "How to Create an N-Gram Model from ScratchHow to Create an N-Gram Model from Scratch",
    "section": "",
    "text": "Set Up Environment\nFor our OOV terms, we select very short words that appear only once.\nWe apply the modified term list to our tokens.\nHere, we want to take our list of modified tokens and add a sentence boundary symbol <s> before the first token of each sentence.\nThen we want to bind offset versions of the this token list into a dataframe with as many columns as our ngram is wide.\nSo, for example, for a trigram table representing the novle Emma, we want something like this:\nNote how each succussive column is the same data but shifted up one row.\nNote also that this table contains all the data required to count instances of trigrams and all lower-order n-grams, i.e. bigram and unigram."
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_EZ.html#prepare-tokens-for-ngram-binding",
    "href": "lessons/M03_LanguageModels/M03_EZ.html#prepare-tokens-for-ngram-binding",
    "title": "How to Create an N-Gram Model from ScratchHow to Create an N-Gram Model from Scratch",
    "section": "Prepare tokens for ngram binding",
    "text": "Prepare tokens for ngram binding\nFirst, we filter all tokens that start sentences.\nOur OHCO index helps us here: any token with token_num == 0 is at the start of a sentence, since the renumbering starts with each sentence.\n\nS = TOKEN.query(\"token_num == 0\")[['modified_term_str']].index\n\nWe assign an absolute sentence number to each inital token and then label all the tokens in a given sentence by their sentence number.\nWe must do this because the current sent_num index column refers to the order of the sentence within the paragraph.\nWe want the order of the sentence within the corpus.\n\nTOKEN.loc[S, 'sent_num2'] = [n+1 for n in range(len(S))] # Assign ordinals \n\n\nTOKEN.sent_num2 = TOKEN.sent_num2.ffill().astype('int') # Label all members of sentence with ordinal\n\nThen we prepend each sentence with the <s> sign.\n\nTOKEN2 = TOKEN.groupby('sent_num2').modified_term_str\\\n    .apply(lambda x: pd.Series(['<s>'] + list(x))).reset_index(drop=True)\n\nInspect the results:\n\n' '.join(TOKEN2.head(20).to_list()) + ' ...'\n\n'<s> the family of dashwood had long been settled in sussex <s> their estate was large and their residence was ...'"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_EZ.html#bind-modified-token-sequence-into-an-ngram-table",
    "href": "lessons/M03_LanguageModels/M03_EZ.html#bind-modified-token-sequence-into-an-ngram-table",
    "title": "How to Create an N-Gram Model from ScratchHow to Create an N-Gram Model from Scratch",
    "section": "Bind modified token sequence into an ngram table",
    "text": "Bind modified token sequence into an ngram table\nNext, we choose the maximum ngram length for our models.\n\nngram = 3\n\nNow we concatenate a set of offset versions of our token list.\nFor this we use the pd.concat() method to bind our token lists, and\nwe use the df.shift() method to offset our token list for each position in the resulting ngram list.\n\nNGRAMS = pd.concat([TOKEN2.shift(0-i) for i in range(ngram)], axis=1)\nNGRAMS.index.name = 'ngram_num'\n\nWe create our word index names following the convention w0, w1, ..., wN.\n\nwidx = [f\"w{i}\" for i in range(ngram)]\nNGRAMS.columns = widx\n\n\nNGRAMS\n\n\n\n\n\n  \n    \n      \n      w0\n      w1\n      w2\n    \n    \n      ngram_num\n      \n      \n      \n    \n  \n  \n    \n      0\n      <s>\n      the\n      family\n    \n    \n      1\n      the\n      family\n      of\n    \n    \n      2\n      family\n      of\n      dashwood\n    \n    \n      3\n      of\n      dashwood\n      had\n    \n    \n      4\n      dashwood\n      had\n      long\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      217675\n      of\n      persuasion\n      by\n    \n    \n      217676\n      persuasion\n      by\n      jane\n    \n    \n      217677\n      by\n      jane\n      austen\n    \n    \n      217678\n      jane\n      austen\n      None\n    \n    \n      217679\n      austen\n      None\n      None\n    \n  \n\n217680 rows × 3 columns"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_EZ.html#generate-models",
    "href": "lessons/M03_LanguageModels/M03_EZ.html#generate-models",
    "title": "How to Create an N-Gram Model from ScratchHow to Create an N-Gram Model from Scratch",
    "section": "Generate models",
    "text": "Generate models\nNext we create a list of models.\nSince the model index will be zero-based (given Python), the trigram model will be M[2], for example.\nWe can use the maximum ngram table to derive all of the lower order models.\nFor each model, we will compute the relevant probabilities and information for each ngram.\n\nM = [None for i in range(ngram)]\nfor i in range(ngram):\n    if i == 0:\n        M[i] = NGRAMS.value_counts('w0').to_frame('n')\n        M[i]['p'] = M[i].n / M[i].n.sum()\n        M[i]['i'] = np.log2(1/M[i].p)\n    else:\n        M[i] = NGRAMS.value_counts(widx[:i+1]).to_frame('n')    \n        M[i]['cp'] = M[i].n / M[i-1].n\n        M[i]['ci'] = np.log2(1/M[i].cp)\n\n\nM[2]\n\n\n\n\n\n  \n    \n      \n      \n      \n      n\n      cp\n      ci\n    \n    \n      w0\n      w1\n      w2\n      \n      \n      \n    \n  \n  \n    \n      mrs\n      <s>\n      jennings\n      234\n      0.442344\n      1.176759\n    \n    \n      <s>\n      it\n      was\n      177\n      0.407834\n      1.293946\n    \n    \n      i\n      am\n      142\n      0.142714\n      2.808806\n    \n    \n      have\n      132\n      0.132663\n      2.914159\n    \n    \n      mrs\n      <s>\n      dashwood\n      121\n      0.228733\n      2.128261\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      happy\n      anne\n      could\n      1\n      1.000000\n      0.000000\n    \n    \n      ardour\n      of\n      1\n      1.000000\n      0.000000\n    \n    \n      as\n      all\n      1\n      0.333333\n      1.584963\n    \n    \n      to\n      1\n      0.333333\n      1.584963\n    \n    \n      zealously\n      discharging\n      all\n      1\n      1.000000\n      0.000000\n    \n  \n\n167055 rows × 3 columns"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_EZ.html#generate-training-data",
    "href": "lessons/M03_LanguageModels/M03_EZ.html#generate-training-data",
    "title": "How to Create an N-Gram Model from ScratchHow to Create an N-Gram Model from Scratch",
    "section": "Generate Training Data",
    "text": "Generate Training Data\nImport and pad the test sentences.\n\nTEST_SENTS = pd.read_csv(\"test_sentences.txt\", header=None, names=['sent_str'])\nTEST_SENTS.sent_str = '<s> ' + TEST_SENTS.sent_str \nTEST_SENTS.index.name = 'sent_num'\n\n\nTEST_SENTS.head()\n\n\n\n\n\n  \n    \n      \n      sent_str\n    \n    \n      sent_num\n      \n    \n  \n  \n    \n      0\n      <s> the quick brown fox jumped over the lazy dogs\n    \n    \n      1\n      <s> The event had every promise of happiness f...\n    \n    \n      2\n      <s> Mr Weston was a man of unexceptionable cha...\n    \n    \n      3\n      <s> ChatGPT is not the singularity\n    \n    \n      4\n      <s> but it was a black morning's work for her\n    \n  \n\n\n\n\nTokenize and normalize.\n\nTEST_TOKENS = TEST_SENTS.sent_str.apply(lambda x: pd.Series(x.split())).stack().to_frame('token_str')\nTEST_TOKENS['term_str'] = TEST_TOKENS.token_str.str.replace(r\"[\\W_]+\", \"\", regex=True).str.lower()\nTEST_TOKENS.index.names = ['sent_num', 'token_num']\nTEST_TOKENS.loc[TEST_TOKENS.token_str == '<s>', 'term_str'] = '<s>' # These will have filtered out in the normalization\n\n\nTEST_TOKENS.head()\n\n\n\n\n\n  \n    \n      \n      \n      token_str\n      term_str\n    \n    \n      sent_num\n      token_num\n      \n      \n    \n  \n  \n    \n      0\n      0\n      <s>\n      <s>\n    \n    \n      1\n      the\n      the\n    \n    \n      2\n      quick\n      quick\n    \n    \n      3\n      brown\n      brown\n    \n    \n      4\n      fox\n      fox"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_EZ.html#identify-oov-terms",
    "href": "lessons/M03_LanguageModels/M03_EZ.html#identify-oov-terms",
    "title": "How to Create an N-Gram Model from ScratchHow to Create an N-Gram Model from Scratch",
    "section": "Identify OOV terms",
    "text": "Identify OOV terms\nCreate a Series of OOV terms from our unigram model.\nWe prefer this to our VOCAB table since the former includes the <s> symbol.\n\nV = M[0].sort_index().reset_index().w0\n\n\nV.sample(10)\n\n3878        inforced\n6886         spurned\n4014      intimation\n6839          speech\n7653    unfathomable\n6951         steward\n4898        obedient\n2051      directions\n4213    lamentations\n4053        involved\nName: w0, dtype: object\n\n\n\nTEST_TOKENS.loc[~TEST_TOKENS.term_str.isin(V), 'term_str'] = \"<UNK>\"\n\n\nTEST_TOKENS[TEST_TOKENS.term_str == '<UNK>'].value_counts('token_str')\n\ntoken_str\nWeston            2\nChatGPT           1\nHartfield         1\nIsabella          1\nIsabella's        1\nWoodhouse         1\nchildhood         1\nillnesses         1\nintellectual      1\nlazy              1\ntenderer          1\nvaletudinarian    1\nName: count, dtype: int64"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_EZ.html#convert-test-tokens-into-ngrams",
    "href": "lessons/M03_LanguageModels/M03_EZ.html#convert-test-tokens-into-ngrams",
    "title": "How to Create an N-Gram Model from ScratchHow to Create an N-Gram Model from Scratch",
    "section": "Convert test tokens into ngrams",
    "text": "Convert test tokens into ngrams\nUse offset and bind method using pd.concat() and df.shift(0), as above.\n\nTEST = pd.concat([TEST_TOKENS.shift(0-i)[['term_str']] for i in range(ngram)], axis=1).fillna('<s>')\nTEST.columns = widx\nTEST = TEST.reset_index()\n\n\nTEST.tail()\n\n\n\n\n\n  \n    \n      \n      sent_num\n      token_num\n      w0\n      w1\n      w2\n    \n  \n  \n    \n      474\n      42\n      10\n      give\n      her\n      pleasant\n    \n    \n      475\n      42\n      11\n      her\n      pleasant\n      society\n    \n    \n      476\n      42\n      12\n      pleasant\n      society\n      again\n    \n    \n      477\n      42\n      13\n      society\n      again\n      <s>\n    \n    \n      478\n      42\n      14\n      again\n      <s>\n      <s>"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_EZ.html#bind-the-models-to-test-data",
    "href": "lessons/M03_LanguageModels/M03_EZ.html#bind-the-models-to-test-data",
    "title": "How to Create an N-Gram Model from ScratchHow to Create an N-Gram Model from Scratch",
    "section": "Bind the models to test data",
    "text": "Bind the models to test data\nWe add our model data to the test data as a set of features.\nNote: if you re-run this cell, you will grow the table unnecessarily.\nBe sure to regenerate the TEST dataframe first.\n\nTEST = TEST.merge(M[0].i, on=['w0'], how='left').rename(columns={'i':'i0'})\nTEST = TEST.merge(M[1].ci, on=['w0','w1'], how='left').rename(columns={'ci':'i1'})\nTEST = TEST.merge(M[2].ci, on=['w0','w1','w2'], how='left').rename(columns={'ci':'i2'})\n\n\nTEST\n\n\n\n\n\n  \n    \n      \n      sent_num\n      token_num\n      w0\n      w1\n      w2\n      i0\n      i1\n      i2\n    \n  \n  \n    \n      0\n      0\n      0\n      <s>\n      the\n      quick\n      4.079116\n      4.692731\n      NaN\n    \n    \n      1\n      0\n      1\n      the\n      quick\n      brown\n      4.871732\n      12.860117\n      NaN\n    \n    \n      2\n      0\n      2\n      quick\n      brown\n      fox\n      13.483922\n      NaN\n      NaN\n    \n    \n      3\n      0\n      3\n      brown\n      fox\n      jumped\n      15.731849\n      NaN\n      NaN\n    \n    \n      4\n      0\n      4\n      fox\n      jumped\n      over\n      17.731849\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      474\n      42\n      10\n      give\n      her\n      pleasant\n      10.052369\n      2.979040\n      NaN\n    \n    \n      475\n      42\n      11\n      her\n      pleasant\n      society\n      5.860329\n      NaN\n      NaN\n    \n    \n      476\n      42\n      12\n      pleasant\n      society\n      again\n      12.924494\n      NaN\n      NaN\n    \n    \n      477\n      42\n      13\n      society\n      again\n      <s>\n      12.522396\n      NaN\n      NaN\n    \n    \n      478\n      42\n      14\n      again\n      <s>\n      <s>\n      9.607728\n      1.876194\n      NaN\n    \n  \n\n479 rows × 8 columns"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_EZ.html#apply-fallback",
    "href": "lessons/M03_LanguageModels/M03_EZ.html#apply-fallback",
    "title": "How to Create an N-Gram Model from ScratchHow to Create an N-Gram Model from Scratch",
    "section": "Apply fallback",
    "text": "Apply fallback\nNote that some values are null values, i.e. NaN – these are cases of ngram combinations that do not appear in the training data.\nWe we replace null values using “stupid fallback.” This means that if is bigram does not exist, we replace it with the unigram value of the last term, and if a trigram does not exist, we replace it with the bigram value of the last two terms.\nThis is not the best method of interpolation but it has the virtue of being very easy to implement.\nWe could write a general routine that iterates through i for range(ngram), but we just implement the two cases for trigrams here.\n\nTEST.loc[TEST.i1.isna(), 'i1'] = TEST.w1.map(M[0].i)\n\n\nTEST = TEST.merge(M[1].ci, left_on=['w1','w2'], right_on=['w0','w1'])\nTEST.loc[TEST.i2.isna(), 'i2'] = TEST.ci\nTEST = TEST.drop('ci', axis=1)\n\n\nTEST.sort_values('i0')\n\n\n\n\n\n  \n    \n      \n      sent_num\n      token_num\n      w0\n      w1\n      w2\n      i0\n      i1\n      i2\n    \n  \n  \n    \n      0\n      0\n      0\n      <s>\n      the\n      quick\n      4.079116\n      4.692731\n      12.860117\n    \n    \n      120\n      12\n      0\n      <s>\n      the\n      equal\n      4.079116\n      4.692731\n      12.860117\n    \n    \n      247\n      29\n      0\n      <s>\n      he\n      could\n      4.079116\n      4.546824\n      5.520946\n    \n    \n      112\n      11\n      0\n      <s>\n      but\n      the\n      4.079116\n      3.859943\n      4.207828\n    \n    \n      310\n      39\n      0\n      <s>\n      being\n      settled\n      4.079116\n      13.652733\n      7.588715\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      31\n      3\n      5\n      singularity\n      <s>\n      but\n      17.731849\n      4.079116\n      3.859943\n    \n    \n      102\n      9\n      3\n      nursed\n      her\n      through\n      17.731849\n      0.000000\n      10.871520\n    \n    \n      111\n      10\n      7\n      owing\n      here\n      <s>\n      17.731849\n      10.777653\n      2.494765\n    \n    \n      207\n      22\n      2\n      emma\n      was\n      aware\n      17.731849\n      6.088895\n      11.642954\n    \n    \n      97\n      30\n      9\n      ages\n      <s>\n      and\n      17.731849\n      4.079116\n      3.270109\n    \n  \n\n339 rows × 8 columns"
  },
  {
    "objectID": "lessons/M03_LanguageModels/M03_EZ.html#compute-sentence-scores",
    "href": "lessons/M03_LanguageModels/M03_EZ.html#compute-sentence-scores",
    "title": "How to Create an N-Gram Model from ScratchHow to Create an N-Gram Model from Scratch",
    "section": "Compute sentence scores",
    "text": "Compute sentence scores\n\nfor n in range(ngram):\n    TEST_SENTS[f'pp{n}'] = np.exp2(TEST.groupby('sent_num')[f'i{n}'].mean())\n\n\nTEST_SENTS.sort_values('pp0')\n\n\n\n\n\n  \n    \n      \n      sent_str\n      pp0\n      pp1\n      pp2\n    \n    \n      sent_num\n      \n      \n      \n      \n    \n  \n  \n    \n      28\n      <s> but he was no companion for her\n      145.356229\n      26.750155\n      18.182994\n    \n    \n      4\n      <s> but it was a black morning's work for her\n      148.603536\n      18.804269\n      10.627530\n    \n    \n      20\n      <s> How was she to bear the change\n      174.540021\n      121.726344\n      46.473587\n    \n    \n      15\n      <s> knowing all the ways of the family\n      203.954617\n      78.585142\n      15.826059\n    \n    \n      24\n      <s> and a Miss Taylor in the house\n      216.450798\n      19.743031\n      40.828353\n    \n    \n      19\n      <s> and who had such an affection for her as c...\n      295.444852\n      55.404579\n      34.041964\n    \n    \n      25\n      <s> and with all her advantages natural and do...\n      327.763999\n      27.775623\n      53.561150\n    \n    \n      18\n      <s> one to whom she could speak every thought ...\n      333.072366\n      87.039292\n      45.592088\n    \n    \n      14\n      <s> She had been a friend and companion such a...\n      344.039462\n      27.441482\n      64.903447\n    \n    \n      21\n      <s> It was true that her friend was going only...\n      358.001018\n      43.181856\n      27.713239\n    \n    \n      29\n      <s> He could not meet her in conversation rati...\n      378.801093\n      38.265239\n      49.525454\n    \n    \n      33\n      <s> for having been a valetudinarian all his l...\n      383.667904\n      40.771211\n      80.694590\n    \n    \n      40\n      <s> and many a long October and November eveni...\n      389.406420\n      136.349662\n      54.718426\n    \n    \n      42\n      <s> and their little children to fill the hous...\n      391.223616\n      113.877552\n      42.397179\n    \n    \n      1\n      <s> The event had every promise of happiness f...\n      393.759284\n      60.528665\n      67.116953\n    \n    \n      23\n      <s> only half a mile from them\n      394.854646\n      46.774250\n      10.894267\n    \n    \n      13\n      <s> on their being left to each other was yet ...\n      411.111651\n      119.071898\n      28.912701\n    \n    \n      11\n      <s> but the intercourse of the last seven years\n      418.390104\n      48.586749\n      20.959050\n    \n    \n      35\n      <s> he was a much older man in ways than in ye...\n      427.202624\n      93.165547\n      38.228065\n    \n    \n      37\n      <s> his talents could not have recommended him...\n      501.170933\n      69.430468\n      26.787393\n    \n    \n      26\n      <s> she was now in great danger of suffering f...\n      512.020374\n      135.538631\n      18.431313\n    \n    \n      5\n      <s> The want of Miss Taylor would be felt ever...\n      535.364012\n      39.960976\n      27.494460\n    \n    \n      32\n      <s> was much increased by his constitution and...\n      569.744472\n      102.931097\n      63.524772\n    \n    \n      41\n      <s> before Christmas brought the next visit fr...\n      572.355290\n      99.000372\n      43.546936\n    \n    \n      8\n      <s> how she had devoted all her powers to atta...\n      589.100181\n      105.508183\n      50.872073\n    \n    \n      36\n      <s> and though everywhere beloved for the frie...\n      594.848425\n      52.417260\n      26.515802\n    \n    \n      7\n      <s> how she had taught and how she had played ...\n      604.938770\n      45.573731\n      49.321784\n    \n    \n      27\n      <s> She dearly loved her father\n      663.003856\n      28.601827\n      28.635560\n    \n    \n      16\n      <s> interested in all its concerns\n      776.264971\n      185.023233\n      16.641210\n    \n    \n      9\n      <s> and how nursed her through the various ill...\n      804.234190\n      137.937208\n      47.038614\n    \n    \n      31\n      <s> and Mr Woodhouse had not married early\n      808.640927\n      64.835521\n      135.226451\n    \n    \n      6\n      <s> She recalled her past kindness the kindnes...\n      872.746711\n      268.506479\n      50.863875\n    \n    \n      22\n      <s> but Emma was aware that great must be the ...\n      961.017774\n      124.048832\n      65.118492\n    \n    \n      17\n      <s> and peculiarly interested in herself in ev...\n      982.605937\n      109.948370\n      83.233794\n    \n    \n      12\n      <s> the equal footing and perfect unreserve wh...\n      990.162956\n      103.359760\n      195.536922\n    \n    \n      30\n      <s> The evil of the actual disparity in their ...\n      1223.095280\n      31.453849\n      42.645729\n    \n    \n      38\n      <s> Her sister though comparatively but little...\n      1357.426160\n      490.423261\n      108.557770\n    \n    \n      34\n      <s> without activity of mind or body\n      1483.243374\n      110.746912\n      36.256001\n    \n    \n      39\n      <s> being settled in London only sixteen miles...\n      1572.934773\n      262.014959\n      51.376731\n    \n    \n      2\n      <s> Mr Weston was a man of unexceptionable cha...\n      1949.212515\n      141.911737\n      54.870981\n    \n    \n      10\n      <s> A large debt of gratitude was owing here\n      4329.897084\n      74.565473\n      65.622680\n    \n    \n      0\n      <s> the quick brown fox jumped over the lazy dogs\n      5110.625578\n      80.265694\n      103.403175\n    \n    \n      3\n      <s> ChatGPT is not the singularity\n      8600.816138\n      31.931954\n      13.392336\n    \n  \n\n\n\n\n\nTEST_SENTS.sort_values('pp1')\n\n\n\n\n\n  \n    \n      \n      sent_str\n      pp0\n      pp1\n      pp2\n    \n    \n      sent_num\n      \n      \n      \n      \n    \n  \n  \n    \n      4\n      <s> but it was a black morning's work for her\n      148.603536\n      18.804269\n      10.627530\n    \n    \n      24\n      <s> and a Miss Taylor in the house\n      216.450798\n      19.743031\n      40.828353\n    \n    \n      28\n      <s> but he was no companion for her\n      145.356229\n      26.750155\n      18.182994\n    \n    \n      14\n      <s> She had been a friend and companion such a...\n      344.039462\n      27.441482\n      64.903447\n    \n    \n      25\n      <s> and with all her advantages natural and do...\n      327.763999\n      27.775623\n      53.561150\n    \n    \n      27\n      <s> She dearly loved her father\n      663.003856\n      28.601827\n      28.635560\n    \n    \n      30\n      <s> The evil of the actual disparity in their ...\n      1223.095280\n      31.453849\n      42.645729\n    \n    \n      3\n      <s> ChatGPT is not the singularity\n      8600.816138\n      31.931954\n      13.392336\n    \n    \n      29\n      <s> He could not meet her in conversation rati...\n      378.801093\n      38.265239\n      49.525454\n    \n    \n      5\n      <s> The want of Miss Taylor would be felt ever...\n      535.364012\n      39.960976\n      27.494460\n    \n    \n      33\n      <s> for having been a valetudinarian all his l...\n      383.667904\n      40.771211\n      80.694590\n    \n    \n      21\n      <s> It was true that her friend was going only...\n      358.001018\n      43.181856\n      27.713239\n    \n    \n      7\n      <s> how she had taught and how she had played ...\n      604.938770\n      45.573731\n      49.321784\n    \n    \n      23\n      <s> only half a mile from them\n      394.854646\n      46.774250\n      10.894267\n    \n    \n      11\n      <s> but the intercourse of the last seven years\n      418.390104\n      48.586749\n      20.959050\n    \n    \n      36\n      <s> and though everywhere beloved for the frie...\n      594.848425\n      52.417260\n      26.515802\n    \n    \n      19\n      <s> and who had such an affection for her as c...\n      295.444852\n      55.404579\n      34.041964\n    \n    \n      1\n      <s> The event had every promise of happiness f...\n      393.759284\n      60.528665\n      67.116953\n    \n    \n      31\n      <s> and Mr Woodhouse had not married early\n      808.640927\n      64.835521\n      135.226451\n    \n    \n      37\n      <s> his talents could not have recommended him...\n      501.170933\n      69.430468\n      26.787393\n    \n    \n      10\n      <s> A large debt of gratitude was owing here\n      4329.897084\n      74.565473\n      65.622680\n    \n    \n      15\n      <s> knowing all the ways of the family\n      203.954617\n      78.585142\n      15.826059\n    \n    \n      0\n      <s> the quick brown fox jumped over the lazy dogs\n      5110.625578\n      80.265694\n      103.403175\n    \n    \n      18\n      <s> one to whom she could speak every thought ...\n      333.072366\n      87.039292\n      45.592088\n    \n    \n      35\n      <s> he was a much older man in ways than in ye...\n      427.202624\n      93.165547\n      38.228065\n    \n    \n      41\n      <s> before Christmas brought the next visit fr...\n      572.355290\n      99.000372\n      43.546936\n    \n    \n      32\n      <s> was much increased by his constitution and...\n      569.744472\n      102.931097\n      63.524772\n    \n    \n      12\n      <s> the equal footing and perfect unreserve wh...\n      990.162956\n      103.359760\n      195.536922\n    \n    \n      8\n      <s> how she had devoted all her powers to atta...\n      589.100181\n      105.508183\n      50.872073\n    \n    \n      17\n      <s> and peculiarly interested in herself in ev...\n      982.605937\n      109.948370\n      83.233794\n    \n    \n      34\n      <s> without activity of mind or body\n      1483.243374\n      110.746912\n      36.256001\n    \n    \n      42\n      <s> and their little children to fill the hous...\n      391.223616\n      113.877552\n      42.397179\n    \n    \n      13\n      <s> on their being left to each other was yet ...\n      411.111651\n      119.071898\n      28.912701\n    \n    \n      20\n      <s> How was she to bear the change\n      174.540021\n      121.726344\n      46.473587\n    \n    \n      22\n      <s> but Emma was aware that great must be the ...\n      961.017774\n      124.048832\n      65.118492\n    \n    \n      26\n      <s> she was now in great danger of suffering f...\n      512.020374\n      135.538631\n      18.431313\n    \n    \n      40\n      <s> and many a long October and November eveni...\n      389.406420\n      136.349662\n      54.718426\n    \n    \n      9\n      <s> and how nursed her through the various ill...\n      804.234190\n      137.937208\n      47.038614\n    \n    \n      2\n      <s> Mr Weston was a man of unexceptionable cha...\n      1949.212515\n      141.911737\n      54.870981\n    \n    \n      16\n      <s> interested in all its concerns\n      776.264971\n      185.023233\n      16.641210\n    \n    \n      39\n      <s> being settled in London only sixteen miles...\n      1572.934773\n      262.014959\n      51.376731\n    \n    \n      6\n      <s> She recalled her past kindness the kindnes...\n      872.746711\n      268.506479\n      50.863875\n    \n    \n      38\n      <s> Her sister though comparatively but little...\n      1357.426160\n      490.423261\n      108.557770\n    \n  \n\n\n\n\n\nTEST_SENTS.sort_values('pp2')\n\n\n\n\n\n  \n    \n      \n      sent_str\n      pp0\n      pp1\n      pp2\n    \n    \n      sent_num\n      \n      \n      \n      \n    \n  \n  \n    \n      4\n      <s> but it was a black morning's work for her\n      148.603536\n      18.804269\n      10.627530\n    \n    \n      23\n      <s> only half a mile from them\n      394.854646\n      46.774250\n      10.894267\n    \n    \n      3\n      <s> ChatGPT is not the singularity\n      8600.816138\n      31.931954\n      13.392336\n    \n    \n      15\n      <s> knowing all the ways of the family\n      203.954617\n      78.585142\n      15.826059\n    \n    \n      16\n      <s> interested in all its concerns\n      776.264971\n      185.023233\n      16.641210\n    \n    \n      28\n      <s> but he was no companion for her\n      145.356229\n      26.750155\n      18.182994\n    \n    \n      26\n      <s> she was now in great danger of suffering f...\n      512.020374\n      135.538631\n      18.431313\n    \n    \n      11\n      <s> but the intercourse of the last seven years\n      418.390104\n      48.586749\n      20.959050\n    \n    \n      36\n      <s> and though everywhere beloved for the frie...\n      594.848425\n      52.417260\n      26.515802\n    \n    \n      37\n      <s> his talents could not have recommended him...\n      501.170933\n      69.430468\n      26.787393\n    \n    \n      5\n      <s> The want of Miss Taylor would be felt ever...\n      535.364012\n      39.960976\n      27.494460\n    \n    \n      21\n      <s> It was true that her friend was going only...\n      358.001018\n      43.181856\n      27.713239\n    \n    \n      27\n      <s> She dearly loved her father\n      663.003856\n      28.601827\n      28.635560\n    \n    \n      13\n      <s> on their being left to each other was yet ...\n      411.111651\n      119.071898\n      28.912701\n    \n    \n      19\n      <s> and who had such an affection for her as c...\n      295.444852\n      55.404579\n      34.041964\n    \n    \n      34\n      <s> without activity of mind or body\n      1483.243374\n      110.746912\n      36.256001\n    \n    \n      35\n      <s> he was a much older man in ways than in ye...\n      427.202624\n      93.165547\n      38.228065\n    \n    \n      24\n      <s> and a Miss Taylor in the house\n      216.450798\n      19.743031\n      40.828353\n    \n    \n      42\n      <s> and their little children to fill the hous...\n      391.223616\n      113.877552\n      42.397179\n    \n    \n      30\n      <s> The evil of the actual disparity in their ...\n      1223.095280\n      31.453849\n      42.645729\n    \n    \n      41\n      <s> before Christmas brought the next visit fr...\n      572.355290\n      99.000372\n      43.546936\n    \n    \n      18\n      <s> one to whom she could speak every thought ...\n      333.072366\n      87.039292\n      45.592088\n    \n    \n      20\n      <s> How was she to bear the change\n      174.540021\n      121.726344\n      46.473587\n    \n    \n      9\n      <s> and how nursed her through the various ill...\n      804.234190\n      137.937208\n      47.038614\n    \n    \n      7\n      <s> how she had taught and how she had played ...\n      604.938770\n      45.573731\n      49.321784\n    \n    \n      29\n      <s> He could not meet her in conversation rati...\n      378.801093\n      38.265239\n      49.525454\n    \n    \n      6\n      <s> She recalled her past kindness the kindnes...\n      872.746711\n      268.506479\n      50.863875\n    \n    \n      8\n      <s> how she had devoted all her powers to atta...\n      589.100181\n      105.508183\n      50.872073\n    \n    \n      39\n      <s> being settled in London only sixteen miles...\n      1572.934773\n      262.014959\n      51.376731\n    \n    \n      25\n      <s> and with all her advantages natural and do...\n      327.763999\n      27.775623\n      53.561150\n    \n    \n      40\n      <s> and many a long October and November eveni...\n      389.406420\n      136.349662\n      54.718426\n    \n    \n      2\n      <s> Mr Weston was a man of unexceptionable cha...\n      1949.212515\n      141.911737\n      54.870981\n    \n    \n      32\n      <s> was much increased by his constitution and...\n      569.744472\n      102.931097\n      63.524772\n    \n    \n      14\n      <s> She had been a friend and companion such a...\n      344.039462\n      27.441482\n      64.903447\n    \n    \n      22\n      <s> but Emma was aware that great must be the ...\n      961.017774\n      124.048832\n      65.118492\n    \n    \n      10\n      <s> A large debt of gratitude was owing here\n      4329.897084\n      74.565473\n      65.622680\n    \n    \n      1\n      <s> The event had every promise of happiness f...\n      393.759284\n      60.528665\n      67.116953\n    \n    \n      33\n      <s> for having been a valetudinarian all his l...\n      383.667904\n      40.771211\n      80.694590\n    \n    \n      17\n      <s> and peculiarly interested in herself in ev...\n      982.605937\n      109.948370\n      83.233794\n    \n    \n      0\n      <s> the quick brown fox jumped over the lazy dogs\n      5110.625578\n      80.265694\n      103.403175\n    \n    \n      38\n      <s> Her sister though comparatively but little...\n      1357.426160\n      490.423261\n      108.557770\n    \n    \n      31\n      <s> and Mr Woodhouse had not married early\n      808.640927\n      64.835521\n      135.226451\n    \n    \n      12\n      <s> the equal footing and perfect unreserve wh...\n      990.162956\n      103.359760\n      195.536922\n    \n  \n\n\n\n\n\nTEST_SENTS.mean(numeric_only=True).plot.bar();"
  },
  {
    "objectID": "lessons/M04_NLP/M04_00_NLTK_Intro.html",
    "href": "lessons/M04_NLP/M04_00_NLTK_Intro.html",
    "title": "NLTK Parsers",
    "section": "",
    "text": "Set Up\nWe import our text from scratch in order to demonstrate the use of NLTKs model-driven parsers.\nWe took a source file and showed how to use NLTK’s tools to do intelligent parsing of sentences and tokens, and then to annotate the resulting TOKENS and VOCAB tables. We also created a POS table to capture information about pos usage in the corpus."
  },
  {
    "objectID": "lessons/M04_NLP/M04_00_NLTK_Intro.html#config",
    "href": "lessons/M04_NLP/M04_00_NLTK_Intro.html#config",
    "title": "NLTK Parsers",
    "section": "Config",
    "text": "Config\n\nimport configparser\n\n\nconfig = configparser.ConfigParser()\nconfig.read(\"../env.ini\")\ndata_dir = config['DEFAULT']['data_home']\noutput_dir = config['DEFAULT']['output_dir']\n\n\nsrc_file = f\"{data_dir}/gutenberg/pg161.txt\"\nOHCO = ['chap_num', 'para_num', 'sent_num', 'token_num']\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport nltk\nimport re\n\n\nnp.__version__\n\n'1.24.3'\n\n\n\nimport scipy as sp\nsp.__version__\n\n'1.11.3'"
  },
  {
    "objectID": "lessons/M04_NLP/M04_00_NLTK_Intro.html#import-nltk-and-download-resources",
    "href": "lessons/M04_NLP/M04_00_NLTK_Intro.html#import-nltk-and-download-resources",
    "title": "NLTK Parsers",
    "section": "Import NLTK and download resources",
    "text": "Import NLTK and download resources\nIf you need to install NLTK, see the instructions here. You can also install this with Anaconda, like so:\nconda install nltk or conda install -c anaconda nltk (See https://anaconda.org/anaconda/nltk)\nOnce you have installed NLTK, you will need to download resources, which will happen when you run the following cell. If the interactive window opens, you may need to set your NLTK Data Directory, as described in the instructions here. To set the directory, click on the File menu and select Change Download Directory. For central installation, set this to C:\\nltk_data (Windows),/usr/local/share/nltk_data (Mac), or /usr/share/nltk_data (Unix).\n\nIf you did not install the data to one of the above central locations, you will need to set the NLTK_DATA environment variable to specify the location of the data. (On a Windows machine, right click on “My Computer” then select Properties > Advanced > Environment Variables > User Variables > New…)\n\n\nnltk_resources = [\n    'tokenizers/punkt', \n    'taggers/averaged_perceptron_tagger', \n    'corpora/stopwords', \n    'help/tagsets'\n]\n\n\nfor rsc in nltk_resources:\n    try:\n        nltk.data.find(rsc)\n    except IndexError:\n        nltk.download(rsc)\n\nIf the above does not work, try going to the command line and download these resources from the interactive python shell.\nYour session should look something like this:\n$ ipython\nPython 3.9.6 (default, Aug 18 2021, 19:38:01) \nType 'copyright', 'credits' or 'license' for more information\nIPython 7.26.0 -- An enhanced Interactive Python. Type '?' for help.\nIn [1]: import nltk\nIn [2]: nltk.download('averaged_perceptron_tagger')\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/rca2t/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\nOut[2]: True\nIn [3]: nltk.download('tagsets')\n[nltk_data] Downloading package tagsets to /home/rca2t/nltk_data...\n[nltk_data]   Unzipping help/tagsets.zip.\nOut[3]: True"
  },
  {
    "objectID": "lessons/M04_NLP/M04_00_NLTK_Intro.html#demonstrate-nltk.sent_tokenize",
    "href": "lessons/M04_NLP/M04_00_NLTK_Intro.html#demonstrate-nltk.sent_tokenize",
    "title": "NLTK Parsers",
    "section": "Demonstrate nltk.sent_tokenize()",
    "text": "Demonstrate nltk.sent_tokenize()\n\nsample_para = PARAS.iloc[1].para_str\n\n\nsample_para\n\n\"By a former marriage, Mr. Henry Dashwood had one son: by his present lady, three daughters.  The son, a steady respectable young man, was amply provided for by the fortune of his mother, which had been large, and half of which devolved on him on his coming of age.  By his own marriage, likewise, which happened soon afterwards, he added to his wealth.  To him therefore the succession to the Norland estate was not so really important as to his sisters; for their fortune, independent of what might arise to them from their father's inheriting that property, could be but small.  Their mother had nothing, and their father only seven thousand pounds in his own disposal; for the remaining moiety of his first wife's fortune was also secured to her child, and he had only a life-interest in it.\"\n\n\n\nnltk.sent_tokenize(sample_para)\n\n['By a former marriage, Mr. Henry Dashwood had one son: by his present lady, three daughters.',\n 'The son, a steady respectable young man, was amply provided for by the fortune of his mother, which had been large, and half of which devolved on him on his coming of age.',\n 'By his own marriage, likewise, which happened soon afterwards, he added to his wealth.',\n \"To him therefore the succession to the Norland estate was not so really important as to his sisters; for their fortune, independent of what might arise to them from their father's inheriting that property, could be but small.\",\n \"Their mother had nothing, and their father only seven thousand pounds in his own disposal; for the remaining moiety of his first wife's fortune was also secured to her child, and he had only a life-interest in it.\"]"
  },
  {
    "objectID": "lessons/M04_NLP/M04_00_NLTK_Intro.html#apply-to-paras-dataframe",
    "href": "lessons/M04_NLP/M04_00_NLTK_Intro.html#apply-to-paras-dataframe",
    "title": "NLTK Parsers",
    "section": "Apply to PARAS dataframe",
    "text": "Apply to PARAS dataframe\n\nSENTS = PARAS.para_str.apply(lambda x: pd.Series(nltk.sent_tokenize(x)))\\\n        .stack()\\\n        .to_frame('sent_str')\nSENTS.index.names = OHCO[:3]\n\n\nSENTS\n\n\n\n\n\n  \n    \n      \n      \n      \n      sent_str\n    \n    \n      chap_num\n      para_num\n      sent_num\n      \n    \n  \n  \n    \n      1\n      0\n      0\n      The family of Dashwood had long been settled i...\n    \n    \n      1\n      Their estate was large, and their residence wa...\n    \n    \n      2\n      The late owner of this estate was a single man...\n    \n    \n      3\n      But her death, which happened ten years before...\n    \n    \n      4\n      In the society of his nephew and niece, and th...\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      50\n      17\n      3\n      He lived to exert, and frequently to enjoy him...\n    \n    \n      4\n      His wife was not always out of humour, nor his...\n    \n    \n      18\n      0\n      For Marianne, however--in spite of his incivil...\n    \n    \n      19\n      0\n      Mrs. Dashwood was prudent enough to remain at ...\n    \n    \n      20\n      0\n      Between Barton and Delaford, there was that co...\n    \n  \n\n4947 rows × 1 columns"
  },
  {
    "objectID": "lessons/M04_NLP/M04_00_NLTK_Intro.html#demonstrate-nltk.word_tokenize",
    "href": "lessons/M04_NLP/M04_00_NLTK_Intro.html#demonstrate-nltk.word_tokenize",
    "title": "NLTK Parsers",
    "section": "Demonstrate nltk.word_tokenize()",
    "text": "Demonstrate nltk.word_tokenize()\n\nsample_sent = SENTS.iloc[3].sent_str\n\n\nsample_sent\n\n'But her death, which happened ten years before his own, produced a great alteration in his home; for to supply her loss, he invited and received into his house the family of his nephew Mr. Henry Dashwood, the legal inheritor of the Norland estate, and the person to whom he intended to bequeath it.'\n\n\n\nsample_tokens = nltk.word_tokenize(sample_sent)\n\n\nprint(sample_tokens)\n\n['But', 'her', 'death', ',', 'which', 'happened', 'ten', 'years', 'before', 'his', 'own', ',', 'produced', 'a', 'great', 'alteration', 'in', 'his', 'home', ';', 'for', 'to', 'supply', 'her', 'loss', ',', 'he', 'invited', 'and', 'received', 'into', 'his', 'house', 'the', 'family', 'of', 'his', 'nephew', 'Mr.', 'Henry', 'Dashwood', ',', 'the', 'legal', 'inheritor', 'of', 'the', 'Norland', 'estate', ',', 'and', 'the', 'person', 'to', 'whom', 'he', 'intended', 'to', 'bequeath', 'it', '.']"
  },
  {
    "objectID": "lessons/M04_NLP/M04_00_NLTK_Intro.html#demostrate-pos-tagging",
    "href": "lessons/M04_NLP/M04_00_NLTK_Intro.html#demostrate-pos-tagging",
    "title": "NLTK Parsers",
    "section": "Demostrate POS tagging",
    "text": "Demostrate POS tagging\n\nsample_tagged_tokens = nltk.pos_tag(nltk.word_tokenize(sample_sent))\n\n\nprint(sample_tagged_tokens)\n\n[('But', 'CC'), ('her', 'PRP$'), ('death', 'NN'), (',', ','), ('which', 'WDT'), ('happened', 'VBD'), ('ten', 'CD'), ('years', 'NNS'), ('before', 'IN'), ('his', 'PRP$'), ('own', 'JJ'), (',', ','), ('produced', 'VBD'), ('a', 'DT'), ('great', 'JJ'), ('alteration', 'NN'), ('in', 'IN'), ('his', 'PRP$'), ('home', 'NN'), (';', ':'), ('for', 'IN'), ('to', 'TO'), ('supply', 'VB'), ('her', 'PRP$'), ('loss', 'NN'), (',', ','), ('he', 'PRP'), ('invited', 'VBD'), ('and', 'CC'), ('received', 'VBD'), ('into', 'IN'), ('his', 'PRP$'), ('house', 'NN'), ('the', 'DT'), ('family', 'NN'), ('of', 'IN'), ('his', 'PRP$'), ('nephew', 'JJ'), ('Mr.', 'NNP'), ('Henry', 'NNP'), ('Dashwood', 'NNP'), (',', ','), ('the', 'DT'), ('legal', 'JJ'), ('inheritor', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Norland', 'NNP'), ('estate', 'NN'), (',', ','), ('and', 'CC'), ('the', 'DT'), ('person', 'NN'), ('to', 'TO'), ('whom', 'WP'), ('he', 'PRP'), ('intended', 'VBD'), ('to', 'TO'), ('bequeath', 'VB'), ('it', 'PRP'), ('.', '.')]"
  },
  {
    "objectID": "lessons/M04_NLP/M04_00_NLTK_Intro.html#apply-to-sents-dataframe",
    "href": "lessons/M04_NLP/M04_00_NLTK_Intro.html#apply-to-sents-dataframe",
    "title": "NLTK Parsers",
    "section": "Apply to SENTS dataframe",
    "text": "Apply to SENTS dataframe\n\nkeep_whitespace = True\n\n\nif keep_whitespace:\n    TOKENS = SENTS.sent_str\\\n            .apply(lambda x: pd.Series(nltk.pos_tag(nltk.word_tokenize(x))))\\\n            .stack()\\\n            .to_frame('pos_tuple')\nelse:\n    TOKENS = SENTS.sent_str\\\n            .apply(lambda x: pd.Series(nltk.pos_tag(nltk.WhitespaceTokenizer().tokenize(x))))\\\n            .stack()\\\n            .to_frame('pos_tuple')\n\n\nTOKENS.index.names = OHCO\n\n\nTOKENS\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      pos_tuple\n    \n    \n      chap_num\n      para_num\n      sent_num\n      token_num\n      \n    \n  \n  \n    \n      1\n      0\n      0\n      0\n      (The, DT)\n    \n    \n      1\n      (family, NN)\n    \n    \n      2\n      (of, IN)\n    \n    \n      3\n      (Dashwood, NNP)\n    \n    \n      4\n      (had, VBD)\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      50\n      20\n      0\n      64\n      (coolness, NN)\n    \n    \n      65\n      (between, IN)\n    \n    \n      66\n      (their, PRP$)\n    \n    \n      67\n      (husbands, NNS)\n    \n    \n      68\n      (., .)\n    \n  \n\n141320 rows × 1 columns\n\n\n\n\nTOKENS['pos'] = TOKENS.pos_tuple.apply(lambda x: x[1])\nTOKENS['token_str'] = TOKENS.pos_tuple.apply(lambda x: x[0])\nTOKENS['term_str'] = TOKENS.token_str.str.lower()\n\n\nTOKENS\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      pos_tuple\n      pos\n      token_str\n      term_str\n    \n    \n      chap_num\n      para_num\n      sent_num\n      token_num\n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      0\n      0\n      0\n      (The, DT)\n      DT\n      The\n      the\n    \n    \n      1\n      (family, NN)\n      NN\n      family\n      family\n    \n    \n      2\n      (of, IN)\n      IN\n      of\n      of\n    \n    \n      3\n      (Dashwood, NNP)\n      NNP\n      Dashwood\n      dashwood\n    \n    \n      4\n      (had, VBD)\n      VBD\n      had\n      had\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      50\n      20\n      0\n      64\n      (coolness, NN)\n      NN\n      coolness\n      coolness\n    \n    \n      65\n      (between, IN)\n      IN\n      between\n      between\n    \n    \n      66\n      (their, PRP$)\n      PRP$\n      their\n      their\n    \n    \n      67\n      (husbands, NNS)\n      NNS\n      husbands\n      husbands\n    \n    \n      68\n      (., .)\n      .\n      .\n      .\n    \n  \n\n141320 rows × 4 columns"
  },
  {
    "objectID": "lessons/M04_NLP/M04_00_NLTK_Intro.html#extract-vocab",
    "href": "lessons/M04_NLP/M04_00_NLTK_Intro.html#extract-vocab",
    "title": "NLTK Parsers",
    "section": "Extract VOCAB",
    "text": "Extract VOCAB\n\nVOCAB = TOKENS.term_str.value_counts().to_frame('n')\nVOCAB.index.name = 'term_str'\nVOCAB['p'] = VOCAB.n / VOCAB.n.sum()\nVOCAB['i'] = -np.log2(VOCAB.p)\nVOCAB['n_chars'] = VOCAB.index.str.len()\n\n\nVOCAB\n\n\n\n\n\n  \n    \n      \n      n\n      p\n      i\n      n_chars\n    \n    \n      term_str\n      \n      \n      \n      \n    \n  \n  \n    \n      ,\n      9900\n      0.070054\n      3.835393\n      1\n    \n    \n      the\n      4101\n      0.029019\n      5.106846\n      3\n    \n    \n      to\n      4101\n      0.029019\n      5.106846\n      2\n    \n    \n      .\n      4028\n      0.028503\n      5.132758\n      1\n    \n    \n      of\n      3571\n      0.025269\n      5.306494\n      2\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      festival\n      1\n      0.000007\n      17.108606\n      8\n    \n    \n      proclaim\n      1\n      0.000007\n      17.108606\n      8\n    \n    \n      spending\n      1\n      0.000007\n      17.108606\n      8\n    \n    \n      habitation\n      1\n      0.000007\n      17.108606\n      10\n    \n    \n      producing\n      1\n      0.000007\n      17.108606\n      9\n    \n  \n\n6586 rows × 4 columns"
  },
  {
    "objectID": "lessons/M04_NLP/M04_00_NLTK_Intro.html#add-max-pos",
    "href": "lessons/M04_NLP/M04_00_NLTK_Intro.html#add-max-pos",
    "title": "NLTK Parsers",
    "section": "Add Max POS",
    "text": "Add Max POS\n\nTOKENS[['term_str','pos']].value_counts().sort_index().loc['love']\n\npos\nIN      1\nNN     47\nVB     18\nVBP    10\nName: count, dtype: int64\n\n\n\nVOCAB['max_pos'] = TOKENS[['term_str','pos']].value_counts().unstack(fill_value=0).idxmax(1)\n\n\nVOCAB\n\n\n\n\n\n  \n    \n      \n      n\n      p\n      i\n      n_chars\n      max_pos\n    \n    \n      term_str\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      ,\n      9900\n      0.070054\n      3.835393\n      1\n      ,\n    \n    \n      the\n      4101\n      0.029019\n      5.106846\n      3\n      DT\n    \n    \n      to\n      4101\n      0.029019\n      5.106846\n      2\n      TO\n    \n    \n      .\n      4028\n      0.028503\n      5.132758\n      1\n      .\n    \n    \n      of\n      3571\n      0.025269\n      5.306494\n      2\n      IN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      festival\n      1\n      0.000007\n      17.108606\n      8\n      NN\n    \n    \n      proclaim\n      1\n      0.000007\n      17.108606\n      8\n      VB\n    \n    \n      spending\n      1\n      0.000007\n      17.108606\n      8\n      VBG\n    \n    \n      habitation\n      1\n      0.000007\n      17.108606\n      10\n      NN\n    \n    \n      producing\n      1\n      0.000007\n      17.108606\n      9\n      VBG\n    \n  \n\n6586 rows × 5 columns"
  },
  {
    "objectID": "lessons/M04_NLP/M04_00_NLTK_Intro.html#the-u-penn-tree-bank",
    "href": "lessons/M04_NLP/M04_00_NLTK_Intro.html#the-u-penn-tree-bank",
    "title": "NLTK Parsers",
    "section": "The U Penn Tree Bank",
    "text": "The U Penn Tree Bank"
  },
  {
    "objectID": "lessons/M04_NLP/M04_00_NLTK_Intro.html#extract-pos",
    "href": "lessons/M04_NLP/M04_00_NLTK_Intro.html#extract-pos",
    "title": "NLTK Parsers",
    "section": "Extract POS",
    "text": "Extract POS\nGrab UPenn Codes\nWe get a text version of the UPenn Codes, or tag set, by calling nltk.help.upenn_tagset(). There is a script in data/misc that will generate this file for you, although a copy of it has already been generated. We use this to add a definition column to our table. This can be used later in visualizations, etc.\n\ntags_csv = [(line.split()[0], ' '.join(line.split()[1:])) \n            for line in open(f'{data_dir}/misc/upenn_tagset.txt', 'r').readlines()]\n\n\nTOKENS\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      pos_tuple\n      pos\n      token_str\n      term_str\n    \n    \n      chap_num\n      para_num\n      sent_num\n      token_num\n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      0\n      0\n      0\n      (The, DT)\n      DT\n      The\n      the\n    \n    \n      1\n      (family, NN)\n      NN\n      family\n      family\n    \n    \n      2\n      (of, IN)\n      IN\n      of\n      of\n    \n    \n      3\n      (Dashwood, NNP)\n      NNP\n      Dashwood\n      dashwood\n    \n    \n      4\n      (had, VBD)\n      VBD\n      had\n      had\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      50\n      20\n      0\n      64\n      (coolness, NN)\n      NN\n      coolness\n      coolness\n    \n    \n      65\n      (between, IN)\n      IN\n      between\n      between\n    \n    \n      66\n      (their, PRP$)\n      PRP$\n      their\n      their\n    \n    \n      67\n      (husbands, NNS)\n      NNS\n      husbands\n      husbands\n    \n    \n      68\n      (., .)\n      .\n      .\n      .\n    \n  \n\n141320 rows × 4 columns\n\n\n\n\nPOS = pd.DataFrame(tags_csv)\nPOS.columns = ['pos_code','pos_def']\nPOS = POS.set_index('pos_code')\nPOS['n'] = TOKENS.pos.value_counts()\nPOS['n'] = POS['n'].fillna(0).astype('int')\nPOS['group'] = POS.apply(lambda x: x.name[:2], 1)\nPOS['punc'] = POS.apply(lambda x: bool(re.match(r\"^\\W\", x.name)), 1)\n\n\nPOS[POS.punc].index.to_list()\n\n['$', \"''\", '(', ')', ',', '--', '.', ':', '``']\n\n\n\nPOS.groupby('group').n.sum().sort_values().plot.bar(figsize=(10,5), rot=0);"
  },
  {
    "objectID": "lessons/M04_NLP/M04_00_NLTK_Intro.html#compute-pos-ambiguity",
    "href": "lessons/M04_NLP/M04_00_NLTK_Intro.html#compute-pos-ambiguity",
    "title": "NLTK Parsers",
    "section": "Compute POS ambiguity",
    "text": "Compute POS ambiguity\n\nTPM = TOKENS[['term_str','pos']].value_counts().unstack()\n\n\nVOCAB['n_pos'] = TPM.count(1)\n\n\nVOCAB.sort_values('n_pos')\n\n\n\n\n\n  \n    \n      \n      n\n      p\n      i\n      n_chars\n      max_pos\n      n_pos\n    \n    \n      term_str\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      ,\n      9900\n      0.070054\n      3.835393\n      1\n      ,\n      1\n    \n    \n      projects\n      1\n      0.000007\n      17.108606\n      8\n      NNS\n      1\n    \n    \n      latest\n      1\n      0.000007\n      17.108606\n      6\n      JJS\n      1\n    \n    \n      favour.\n      1\n      0.000007\n      17.108606\n      7\n      NN\n      1\n    \n    \n      hating\n      1\n      0.000007\n      17.108606\n      6\n      VBG\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      round\n      27\n      0.000191\n      12.353719\n      5\n      NN\n      7\n    \n    \n      heard\n      78\n      0.000552\n      10.823204\n      5\n      VBN\n      7\n    \n    \n      half\n      55\n      0.000389\n      11.327246\n      4\n      PDT\n      7\n    \n    \n      till\n      83\n      0.000587\n      10.733567\n      4\n      VB\n      8\n    \n    \n      ma'am\n      37\n      0.000262\n      11.899153\n      5\n      NN\n      8\n    \n  \n\n6586 rows × 6 columns\n\n\n\n\nVOCAB.n_pos.hist();\n\n\n\n\n\nVOCAB.plot.scatter('n_chars', 'n_pos');"
  },
  {
    "objectID": "lessons/M04_NLP/M04_00_NLTK_Intro.html#extract-max-pos",
    "href": "lessons/M04_NLP/M04_00_NLTK_Intro.html#extract-max-pos",
    "title": "NLTK Parsers",
    "section": "Extract Max POS",
    "text": "Extract Max POS\n\nVOCAB['max_pos'] = TPM.idxmax(1)\n\n\nVOCAB\n\n\n\n\n\n  \n    \n      \n      n\n      p\n      i\n      n_chars\n      max_pos\n      n_pos\n    \n    \n      term_str\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      ,\n      9900\n      0.070054\n      3.835393\n      1\n      ,\n      1\n    \n    \n      the\n      4101\n      0.029019\n      5.106846\n      3\n      DT\n      1\n    \n    \n      to\n      4101\n      0.029019\n      5.106846\n      2\n      TO\n      1\n    \n    \n      .\n      4028\n      0.028503\n      5.132758\n      1\n      .\n      1\n    \n    \n      of\n      3571\n      0.025269\n      5.306494\n      2\n      IN\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      festival\n      1\n      0.000007\n      17.108606\n      8\n      NN\n      1\n    \n    \n      proclaim\n      1\n      0.000007\n      17.108606\n      8\n      VB\n      1\n    \n    \n      spending\n      1\n      0.000007\n      17.108606\n      8\n      VBG\n      1\n    \n    \n      habitation\n      1\n      0.000007\n      17.108606\n      10\n      NN\n      1\n    \n    \n      producing\n      1\n      0.000007\n      17.108606\n      9\n      VBG\n      1\n    \n  \n\n6586 rows × 6 columns"
  },
  {
    "objectID": "lessons/M04_NLP/M04_00_NLTK_Intro.html#identify-stopwords",
    "href": "lessons/M04_NLP/M04_00_NLTK_Intro.html#identify-stopwords",
    "title": "NLTK Parsers",
    "section": "Identify Stopwords",
    "text": "Identify Stopwords\nWe use NLTK’s built in stopword list for English. Note that we can add and subtract from this list, or just create our own list and keep it in our data model.\n\nsw = pd.DataFrame({'stop': 1}, index=nltk.corpus.stopwords.words('english'))\nsw.index.name='term_str'\n\n\nsw.head()\n\n\n\n\n\n  \n    \n      \n      stop\n    \n    \n      term_str\n      \n    \n  \n  \n    \n      i\n      1\n    \n    \n      me\n      1\n    \n    \n      my\n      1\n    \n    \n      myself\n      1\n    \n    \n      we\n      1\n    \n  \n\n\n\n\n\nif 'stop' not in VOCAB.columns:\n    VOCAB = VOCAB.join(sw)\n    VOCAB['stop'] = VOCAB['stop'].fillna(0).astype('int')\n\n\nVOCAB\n\n\n\n\n\n  \n    \n      \n      n\n      p\n      i\n      n_chars\n      max_pos\n      n_pos\n      stop\n    \n    \n      term_str\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      ,\n      9900\n      0.070054\n      3.835393\n      1\n      ,\n      1\n      0\n    \n    \n      the\n      4101\n      0.029019\n      5.106846\n      3\n      DT\n      1\n      1\n    \n    \n      to\n      4101\n      0.029019\n      5.106846\n      2\n      TO\n      1\n      1\n    \n    \n      .\n      4028\n      0.028503\n      5.132758\n      1\n      .\n      1\n      0\n    \n    \n      of\n      3571\n      0.025269\n      5.306494\n      2\n      IN\n      1\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      festival\n      1\n      0.000007\n      17.108606\n      8\n      NN\n      1\n      0\n    \n    \n      proclaim\n      1\n      0.000007\n      17.108606\n      8\n      VB\n      1\n      0\n    \n    \n      spending\n      1\n      0.000007\n      17.108606\n      8\n      VBG\n      1\n      0\n    \n    \n      habitation\n      1\n      0.000007\n      17.108606\n      10\n      NN\n      1\n      0\n    \n    \n      producing\n      1\n      0.000007\n      17.108606\n      9\n      VBG\n      1\n      0\n    \n  \n\n6586 rows × 7 columns"
  },
  {
    "objectID": "lessons/M04_NLP/M04_00_NLTK_Intro.html#add-stems",
    "href": "lessons/M04_NLP/M04_00_NLTK_Intro.html#add-stems",
    "title": "NLTK Parsers",
    "section": "Add Stems",
    "text": "Add Stems\n\nfrom nltk.stem.porter import PorterStemmer\nstemmer = PorterStemmer()\nVOCAB['p_stem'] = VOCAB.apply(lambda x: stemmer.stem(x.name), 1)\n\n\nVOCAB\n\n\n\n\n\n  \n    \n      \n      n\n      p\n      i\n      n_chars\n      max_pos\n      n_pos\n      stop\n      p_stem\n    \n    \n      term_str\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      ,\n      9900\n      0.070054\n      3.835393\n      1\n      ,\n      1\n      0\n      ,\n    \n    \n      the\n      4101\n      0.029019\n      5.106846\n      3\n      DT\n      1\n      1\n      the\n    \n    \n      to\n      4101\n      0.029019\n      5.106846\n      2\n      TO\n      1\n      1\n      to\n    \n    \n      .\n      4028\n      0.028503\n      5.132758\n      1\n      .\n      1\n      0\n      .\n    \n    \n      of\n      3571\n      0.025269\n      5.306494\n      2\n      IN\n      1\n      1\n      of\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      festival\n      1\n      0.000007\n      17.108606\n      8\n      NN\n      1\n      0\n      festiv\n    \n    \n      proclaim\n      1\n      0.000007\n      17.108606\n      8\n      VB\n      1\n      0\n      proclaim\n    \n    \n      spending\n      1\n      0.000007\n      17.108606\n      8\n      VBG\n      1\n      0\n      spend\n    \n    \n      habitation\n      1\n      0.000007\n      17.108606\n      10\n      NN\n      1\n      0\n      habit\n    \n    \n      producing\n      1\n      0.000007\n      17.108606\n      9\n      VBG\n      1\n      0\n      produc\n    \n  \n\n6586 rows × 8 columns"
  },
  {
    "objectID": "lessons/M04_NLP/M04_01_Pipeline.html",
    "href": "lessons/M04_NLP/M04_01_Pipeline.html",
    "title": "NLP and the Pipeline",
    "section": "",
    "text": "Set Up\nSince Project Gutenberg texts vary widely in their markup, we define our chunking patterns by hand.\nWe get each file and add to a library LIB.\nExtract a vocabulary from the CORPUS as a whole"
  },
  {
    "objectID": "lessons/M04_NLP/M04_01_Pipeline.html#config",
    "href": "lessons/M04_NLP/M04_01_Pipeline.html#config",
    "title": "NLP and the Pipeline",
    "section": "Config",
    "text": "Config\n\nimport pandas as pd\nimport numpy as np\nfrom glob import glob\nimport re\nimport nltk\nimport plotly_express as px\n\n\nimport configparser\n\n\nconfig = configparser.ConfigParser()\nconfig.read(\"../env.ini\")\ndata_home = config['DEFAULT']['data_home']\noutput_dir = config['DEFAULT']['output_dir']\nlocal_lib = config['DEFAULT']['local_lib']\n\n\nsource_files = f'{data_home}/gutenberg/austen-melville-set'\ndata_prefix = 'austen-melville'\n\n\nOHCO = ['book_id', 'chap_num', 'para_num', 'sent_num', 'token_num']\n\n\nimport sys\nsys.path.append(local_lib)\n\n\nfrom textparser import TextParser"
  },
  {
    "objectID": "lessons/M04_NLP/M04_01_Pipeline.html#save-chapter-regexes",
    "href": "lessons/M04_NLP/M04_01_Pipeline.html#save-chapter-regexes",
    "title": "NLP and the Pipeline",
    "section": "Save Chapter regexes",
    "text": "Save Chapter regexes\n\nLIB['chap_regex'] = LIB.index.map(pd.Series({x[0]:x[1] for x in ohco_pat_list}))\n\n\nLIB\n\n\n\n\n\n  \n    \n      \n      source_file_path\n      author\n      title\n      chap_regex\n    \n    \n      book_id\n      \n      \n      \n      \n    \n  \n  \n    \n      105\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      AUSTEN, JANE\n      PERSUASION\n      ^Chapter\\s+\\d+$\n    \n    \n      121\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      AUSTEN, JANE\n      NORTHANGER ABBEY\n      ^CHAPTER\\s+\\d+$\n    \n    \n      141\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      AUSTEN, JANE\n      MANSFIELD PARK\n      ^CHAPTER\\s+[IVXLCM]+$\n    \n    \n      158\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      AUSTEN, JANE\n      EMMA\n      ^\\s*CHAPTER\\s+[IVXLCM]+\\s*$\n    \n    \n      161\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      AUSTEN, JANE\n      SENSE AND SENSIBILITY\n      ^CHAPTER\\s+\\d+$\n    \n    \n      946\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      AUSTEN, JANE\n      LADY SUSAN\n      ^\\s*[IVXLCM]+\\s*$\n    \n    \n      1212\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      AUSTEN, JANE\n      LOVE AND FREINDSHIP SIC\n      ^\\s*LETTER .* to .*$\n    \n    \n      1342\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      AUSTEN, JANE\n      PRIDE AND PREJUDICE\n      ^Chapter\\s+\\d+$\n    \n    \n      1900\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      MELVILLE, HERMAN\n      TYPEE A ROMANCE OF THE SOUTH SEAS\n      ^CHAPTER\n    \n    \n      2701\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      MELVILLE, HERMAN\n      MOBY DICK OR THE WHALE\n      ^(?:ETYMOLOGY|EXTRACTS|CHAPTER)\n    \n    \n      4045\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      MELVILLE, HERMAN\n      OMOO ADVENTURES IN THE SOUTH SEAS\n      ^\\s*CHAPTER\\s+[IVXLCM]+\\.\\s*$\n    \n    \n      8118\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      MELVILLE, HERMAN\n      REDBURN HIS FIRST VOYAGE BEING THE SAILOR BOY ...\n      ^\\s*[IVXLCM]+\\. .*$\n    \n    \n      10712\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      MELVILLE, HERMAN\n      WHITE JACKET OR THE WORLD ON A MAN OF WAR\n      ^CHAPTER\\s+[IVXLCM]+\\.\\s*$\n    \n    \n      13720\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      MELVILLE, HERMAN\n      MARDI AND A VOYAGE THITHER VOL I\n      ^\\s*CHAPTER\\s+[IVXLCM]+\\s*$\n    \n    \n      13721\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      MELVILLE, HERMAN\n      MARDI AND A VOYAGE THITHER VOL II\n      ^\\s*CHAPTER\\s+[IVXLCM]+\\s*$\n    \n    \n      15422\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      MELVILLE, HERMAN\n      ISRAEL POTTER HIS FIFTY YEARS OF EXILE\n      ^\\s*CHAPTER\\s+[IVXLCM]+\\.\n    \n    \n      15859\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      MELVILLE, HERMAN\n      THE PIAZZA TALES\n      ^\\s*[A-Z,;-]+\\.\\s*$\n    \n    \n      21816\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      MELVILLE, HERMAN\n      THE CONFIDENCE MAN HIS MASQUERADE\n      ^CHAPTER\\s+[IVXLCM]+\\.?$\n    \n    \n      34970\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      MELVILLE, HERMAN\n      PIERRE OR THE AMBIGUITIES\n      ^\\s*[IVXLCM]+\\.\\s*$\n    \n  \n\n\n\n\n\n# {x[0]:x[1] for x in ohco_pat_list}"
  },
  {
    "objectID": "lessons/M04_NLP/M04_01_Pipeline.html#tokenize-corpus",
    "href": "lessons/M04_NLP/M04_01_Pipeline.html#tokenize-corpus",
    "title": "NLP and the Pipeline",
    "section": "Tokenize Corpus",
    "text": "Tokenize Corpus\nWe tokenize each book and add each TOKENS table to a list to be concatenated into a single CORPUS.\n\ndef tokenize_collection(LIB):\n\n    clip_pats = [\n        r\"\\*\\*\\*\\s*START OF\",\n        r\"\\*\\*\\*\\s*END OF\"\n    ]\n\n    books = []\n    for book_id in LIB.index:\n\n        # Announce\n        print(\"Tokenizing\", book_id, LIB.loc[book_id].title)\n\n        # Define vars\n        chap_regex = LIB.loc[book_id].chap_regex\n        ohco_pats = [('chap', chap_regex, 'm')]\n        src_file_path = LIB.loc[book_id].source_file_path\n\n        # Create object\n        text = TextParser(src_file_path, ohco_pats=ohco_pats, clip_pats=clip_pats, use_nltk=True)\n\n        # Define parameters\n        text.verbose = True\n        text.strip_hyphens = True\n        text.strip_whitespace = True\n\n        # Parse\n        text.import_source().parse_tokens();\n\n        # Name things\n        text.TOKENS['book_id'] = book_id\n        text.TOKENS = text.TOKENS.reset_index().set_index(['book_id'] + text.OHCO)\n\n        # Add to list\n        books.append(text.TOKENS)\n        \n    # Combine into a single dataframe\n    CORPUS = pd.concat(books).sort_index()\n\n    # Clean up\n    del(books)\n    del(text)\n        \n    print(\"Done\")\n        \n    return CORPUS\n\n\nLIB.loc[15859].chap_regex\n\n'^\\\\s*[A-Z,;-]+\\\\.\\\\s*$'\n\n\n\nCORPUS = tokenize_collection(LIB)\n\nTokenizing 105 PERSUASION\nImporting  /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001_2024_01_R/data/gutenberg/austen-melville-set/AUSTEN_JANE_PERSUASION-pg105.txt\nClipping text\nParsing OHCO level 0 chap_id by milestone ^Chapter\\s+\\d+$\nline_str chap_str\nIndex(['chap_str'], dtype='object')\nParsing OHCO level 1 para_num by delimitter \\n\\n\nParsing OHCO level 2 sent_num by NLTK model\nParsing OHCO level 3 token_num by NLTK model\nTokenizing 121 NORTHANGER ABBEY\nImporting  /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001_2024_01_R/data/gutenberg/austen-melville-set/AUSTEN_JANE_NORTHANGER_ABBEY-pg121.txt\nClipping text\nParsing OHCO level 0 chap_id by milestone ^CHAPTER\\s+\\d+$\nline_str chap_str\nIndex(['chap_str'], dtype='object')\nParsing OHCO level 1 para_num by delimitter \\n\\n\nParsing OHCO level 2 sent_num by NLTK model\nParsing OHCO level 3 token_num by NLTK model\nTokenizing 141 MANSFIELD PARK\nImporting  /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001_2024_01_R/data/gutenberg/austen-melville-set/AUSTEN_JANE_MANSFIELD_PARK-pg141.txt\nClipping text\nParsing OHCO level 0 chap_id by milestone ^CHAPTER\\s+[IVXLCM]+$\nline_str chap_str\nIndex(['chap_str'], dtype='object')\nParsing OHCO level 1 para_num by delimitter \\n\\n\nParsing OHCO level 2 sent_num by NLTK model\nParsing OHCO level 3 token_num by NLTK model\nTokenizing 158 EMMA\nImporting  /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001_2024_01_R/data/gutenberg/austen-melville-set/AUSTEN_JANE_EMMA-pg158.txt\nClipping text\nParsing OHCO level 0 chap_id by milestone ^\\s*CHAPTER\\s+[IVXLCM]+\\s*$\nline_str chap_str\nIndex(['chap_str'], dtype='object')\nParsing OHCO level 1 para_num by delimitter \\n\\n\nParsing OHCO level 2 sent_num by NLTK model\nParsing OHCO level 3 token_num by NLTK model\nTokenizing 161 SENSE AND SENSIBILITY\nImporting  /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001_2024_01_R/data/gutenberg/austen-melville-set/AUSTEN_JANE_SENSE_AND_SENSIBILITY-pg161.txt\nClipping text\nParsing OHCO level 0 chap_id by milestone ^CHAPTER\\s+\\d+$\nline_str chap_str\nIndex(['chap_str'], dtype='object')\nParsing OHCO level 1 para_num by delimitter \\n\\n\nParsing OHCO level 2 sent_num by NLTK model\nParsing OHCO level 3 token_num by NLTK model\nTokenizing 946 LADY SUSAN\nImporting  /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001_2024_01_R/data/gutenberg/austen-melville-set/AUSTEN_JANE_LADY_SUSAN-pg946.txt\nClipping text\nParsing OHCO level 0 chap_id by milestone ^\\s*[IVXLCM]+\\s*$\nline_str chap_str\nIndex(['chap_str'], dtype='object')\nParsing OHCO level 1 para_num by delimitter \\n\\n\nParsing OHCO level 2 sent_num by NLTK model\nParsing OHCO level 3 token_num by NLTK model\nTokenizing 1212 LOVE AND FREINDSHIP SIC\nImporting  /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001_2024_01_R/data/gutenberg/austen-melville-set/AUSTEN_JANE_LOVE_AND_FREINDSHIP_SIC_-pg1212.txt\nClipping text\nParsing OHCO level 0 chap_id by milestone ^\\s*LETTER .* to .*$\nline_str chap_str\nIndex(['chap_str'], dtype='object')\nParsing OHCO level 1 para_num by delimitter \\n\\n\nParsing OHCO level 2 sent_num by NLTK model\nParsing OHCO level 3 token_num by NLTK model\nTokenizing 1342 PRIDE AND PREJUDICE\nImporting  /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001_2024_01_R/data/gutenberg/austen-melville-set/AUSTEN_JANE_PRIDE_AND_PREJUDICE-pg1342.txt\nClipping text\nParsing OHCO level 0 chap_id by milestone ^Chapter\\s+\\d+$\nline_str chap_str\nIndex(['chap_str'], dtype='object')\nParsing OHCO level 1 para_num by delimitter \\n\\n\nParsing OHCO level 2 sent_num by NLTK model\nParsing OHCO level 3 token_num by NLTK model\nTokenizing 1900 TYPEE A ROMANCE OF THE SOUTH SEAS\nImporting  /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001_2024_01_R/data/gutenberg/austen-melville-set/MELVILLE_HERMAN_TYPEE_A_ROMANCE_OF_THE_SOUTH_SEAS-pg1900.txt\nClipping text\nParsing OHCO level 0 chap_id by milestone ^CHAPTER \nline_str chap_str\nIndex(['chap_str'], dtype='object')\nParsing OHCO level 1 para_num by delimitter \\n\\n\nParsing OHCO level 2 sent_num by NLTK model\nParsing OHCO level 3 token_num by NLTK model\nTokenizing 2701 MOBY DICK OR THE WHALE\nImporting  /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001_2024_01_R/data/gutenberg/austen-melville-set/MELVILLE_HERMAN_MOBY_DICK_OR_THE_WHALE-pg2701.txt\nClipping text\nParsing OHCO level 0 chap_id by milestone ^(?:ETYMOLOGY|EXTRACTS|CHAPTER)\nline_str chap_str\nIndex(['chap_str'], dtype='object')\nParsing OHCO level 1 para_num by delimitter \\n\\n\nParsing OHCO level 2 sent_num by NLTK model\nParsing OHCO level 3 token_num by NLTK model\nTokenizing 4045 OMOO ADVENTURES IN THE SOUTH SEAS\nImporting  /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001_2024_01_R/data/gutenberg/austen-melville-set/MELVILLE_HERMAN_OMOO_ADVENTURES_IN_THE_SOUTH_SEAS-pg4045.txt\nClipping text\nParsing OHCO level 0 chap_id by milestone ^\\s*CHAPTER\\s+[IVXLCM]+\\.\\s*$\nline_str chap_str\nIndex(['chap_str'], dtype='object')\nParsing OHCO level 1 para_num by delimitter \\n\\n\nParsing OHCO level 2 sent_num by NLTK model\nParsing OHCO level 3 token_num by NLTK model\nTokenizing 8118 REDBURN HIS FIRST VOYAGE BEING THE SAILOR BOY CONFESSIONS AND REMINISCENCES OF THE SON OF A GENTLEMAN IN THE MERCHANT NAVY\nImporting  /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001_2024_01_R/data/gutenberg/austen-melville-set/MELVILLE_HERMAN_REDBURN_HIS_FIRST_VOYAGE_BEING_THE_SAILOR_BOY_CONFESSIONS_AND_REMINISCENCES_OF_THE_SON_OF_A_GENTLEMAN_IN_THE_MERCHANT_NAVY-pg8118.txt\nClipping text\nParsing OHCO level 0 chap_id by milestone ^\\s*[IVXLCM]+\\. .*$\nline_str chap_str\nIndex(['chap_str'], dtype='object')\nParsing OHCO level 1 para_num by delimitter \\n\\n\nParsing OHCO level 2 sent_num by NLTK model\nParsing OHCO level 3 token_num by NLTK model\nTokenizing 10712 WHITE JACKET OR THE WORLD ON A MAN OF WAR\nImporting  /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001_2024_01_R/data/gutenberg/austen-melville-set/MELVILLE_HERMAN_WHITE_JACKET_OR_THE_WORLD_ON_A_MAN_OF_WAR-pg10712.txt\nClipping text\nParsing OHCO level 0 chap_id by milestone ^CHAPTER\\s+[IVXLCM]+\\.\\s*$\nline_str chap_str\nIndex(['chap_str'], dtype='object')\nParsing OHCO level 1 para_num by delimitter \\n\\n\nParsing OHCO level 2 sent_num by NLTK model\nParsing OHCO level 3 token_num by NLTK model\nTokenizing 13720 MARDI AND A VOYAGE THITHER VOL I\nImporting  /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001_2024_01_R/data/gutenberg/austen-melville-set/MELVILLE_HERMAN_MARDI_AND_A_VOYAGE_THITHER_VOL_I-pg13720.txt\nClipping text\nParsing OHCO level 0 chap_id by milestone ^\\s*CHAPTER\\s+[IVXLCM]+\\s*$\nline_str chap_str\nIndex(['chap_str'], dtype='object')\nParsing OHCO level 1 para_num by delimitter \\n\\n\nParsing OHCO level 2 sent_num by NLTK model\nParsing OHCO level 3 token_num by NLTK model\nTokenizing 13721 MARDI AND A VOYAGE THITHER VOL II\nImporting  /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001_2024_01_R/data/gutenberg/austen-melville-set/MELVILLE_HERMAN_MARDI_AND_A_VOYAGE_THITHER_VOL_II-pg13721.txt\nClipping text\nParsing OHCO level 0 chap_id by milestone ^\\s*CHAPTER\\s+[IVXLCM]+\\s*$\nline_str chap_str\nIndex(['chap_str'], dtype='object')\nParsing OHCO level 1 para_num by delimitter \\n\\n\nParsing OHCO level 2 sent_num by NLTK model\nParsing OHCO level 3 token_num by NLTK model\nTokenizing 15422 ISRAEL POTTER HIS FIFTY YEARS OF EXILE\nImporting  /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001_2024_01_R/data/gutenberg/austen-melville-set/MELVILLE_HERMAN_ISRAEL_POTTER_HIS_FIFTY_YEARS_OF_EXILE-pg15422.txt\nClipping text\nParsing OHCO level 0 chap_id by milestone ^\\s*CHAPTER\\s+[IVXLCM]+\\.\nline_str chap_str\nIndex(['chap_str'], dtype='object')\nParsing OHCO level 1 para_num by delimitter \\n\\n\nParsing OHCO level 2 sent_num by NLTK model\nParsing OHCO level 3 token_num by NLTK model\nTokenizing 15859 THE PIAZZA TALES\nImporting  /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001_2024_01_R/data/gutenberg/austen-melville-set/MELVILLE_HERMAN_THE_PIAZZA_TALES-pg15859.txt\nClipping text\nParsing OHCO level 0 chap_id by milestone ^\\s*[A-Z,;-]+\\.\\s*$\nline_str chap_str\nIndex(['chap_str'], dtype='object')\nParsing OHCO level 1 para_num by delimitter \\n\\n\nParsing OHCO level 2 sent_num by NLTK model\nParsing OHCO level 3 token_num by NLTK model\nTokenizing 21816 THE CONFIDENCE MAN HIS MASQUERADE\nImporting  /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001_2024_01_R/data/gutenberg/austen-melville-set/MELVILLE_HERMAN_THE_CONFIDENCE_MAN_HIS_MASQUERADE-pg21816.txt\nClipping text\nParsing OHCO level 0 chap_id by milestone ^CHAPTER\\s+[IVXLCM]+\\.?$\nline_str chap_str\nIndex(['chap_str'], dtype='object')\nParsing OHCO level 1 para_num by delimitter \\n\\n\nParsing OHCO level 2 sent_num by NLTK model\nParsing OHCO level 3 token_num by NLTK model\nTokenizing 34970 PIERRE OR THE AMBIGUITIES\nImporting  /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001_2024_01_R/data/gutenberg/austen-melville-set/MELVILLE_HERMAN_PIERRE_OR_THE_AMBIGUITIES-pg34970.txt\nClipping text\nParsing OHCO level 0 chap_id by milestone ^\\s*[IVXLCM]+\\.\\s*$\nline_str chap_str\nIndex(['chap_str'], dtype='object')\nParsing OHCO level 1 para_num by delimitter \\n\\n\nParsing OHCO level 2 sent_num by NLTK model\nParsing OHCO level 3 token_num by NLTK model\nDone"
  },
  {
    "objectID": "lessons/M04_NLP/M04_01_Pipeline.html#extract-some-features-for-lib",
    "href": "lessons/M04_NLP/M04_01_Pipeline.html#extract-some-features-for-lib",
    "title": "NLP and the Pipeline",
    "section": "Extract some features for LIB",
    "text": "Extract some features for LIB\n\nLIB['book_len'] = CORPUS.groupby('book_id').term_str.count()\n\n\nLIB.sort_values('book_len')\n\n\n\n\n\n  \n    \n      \n      source_file_path\n      author\n      title\n      chap_regex\n      book_len\n    \n    \n      book_id\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      946\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      AUSTEN, JANE\n      LADY SUSAN\n      ^\\s*[IVXLCM]+\\s*$\n      23116\n    \n    \n      1212\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      AUSTEN, JANE\n      LOVE AND FREINDSHIP SIC\n      ^\\s*LETTER .* to .*$\n      33265\n    \n    \n      15422\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      MELVILLE, HERMAN\n      ISRAEL POTTER HIS FIFTY YEARS OF EXILE\n      ^\\s*CHAPTER\\s+[IVXLCM]+\\.\n      65516\n    \n    \n      15859\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      MELVILLE, HERMAN\n      THE PIAZZA TALES\n      ^\\s*[A-Z,;-]+\\.\\s*$\n      75491\n    \n    \n      121\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      AUSTEN, JANE\n      NORTHANGER ABBEY\n      ^CHAPTER\\s+\\d+$\n      77601\n    \n    \n      105\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      AUSTEN, JANE\n      PERSUASION\n      ^Chapter\\s+\\d+$\n      83624\n    \n    \n      21816\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      MELVILLE, HERMAN\n      THE CONFIDENCE MAN HIS MASQUERADE\n      ^CHAPTER\\s+[IVXLCM]+\\.?$\n      95315\n    \n    \n      13720\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      MELVILLE, HERMAN\n      MARDI AND A VOYAGE THITHER VOL I\n      ^\\s*CHAPTER\\s+[IVXLCM]+\\s*$\n      96878\n    \n    \n      13721\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      MELVILLE, HERMAN\n      MARDI AND A VOYAGE THITHER VOL II\n      ^\\s*CHAPTER\\s+[IVXLCM]+\\s*$\n      102092\n    \n    \n      4045\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      MELVILLE, HERMAN\n      OMOO ADVENTURES IN THE SOUTH SEAS\n      ^\\s*CHAPTER\\s+[IVXLCM]+\\.\\s*$\n      102352\n    \n    \n      1900\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      MELVILLE, HERMAN\n      TYPEE A ROMANCE OF THE SOUTH SEAS\n      ^CHAPTER\n      108021\n    \n    \n      8118\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      MELVILLE, HERMAN\n      REDBURN HIS FIRST VOYAGE BEING THE SAILOR BOY ...\n      ^\\s*[IVXLCM]+\\. .*$\n      119243\n    \n    \n      161\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      AUSTEN, JANE\n      SENSE AND SENSIBILITY\n      ^CHAPTER\\s+\\d+$\n      119873\n    \n    \n      1342\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      AUSTEN, JANE\n      PRIDE AND PREJUDICE\n      ^Chapter\\s+\\d+$\n      122126\n    \n    \n      10712\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      MELVILLE, HERMAN\n      WHITE JACKET OR THE WORLD ON A MAN OF WAR\n      ^CHAPTER\\s+[IVXLCM]+\\.\\s*$\n      143310\n    \n    \n      34970\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      MELVILLE, HERMAN\n      PIERRE OR THE AMBIGUITIES\n      ^\\s*[IVXLCM]+\\.\\s*$\n      155056\n    \n    \n      141\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      AUSTEN, JANE\n      MANSFIELD PARK\n      ^CHAPTER\\s+[IVXLCM]+$\n      160378\n    \n    \n      158\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      AUSTEN, JANE\n      EMMA\n      ^\\s*CHAPTER\\s+[IVXLCM]+\\s*$\n      160926\n    \n    \n      2701\n      /Users/rca2t1/Dropbox/Courses/DS/DS5001/DS5001...\n      MELVILLE, HERMAN\n      MOBY DICK OR THE WHALE\n      ^(?:ETYMOLOGY|EXTRACTS|CHAPTER)\n      215504\n    \n  \n\n\n\n\n\nLIB['n_chaps'] = CORPUS.reset_index()[['book_id','chap_id']]\\\n    .drop_duplicates()\\\n    .groupby('book_id').chap_id.count()"
  },
  {
    "objectID": "lessons/M04_NLP/M04_01_Pipeline.html#handle-anomalies",
    "href": "lessons/M04_NLP/M04_01_Pipeline.html#handle-anomalies",
    "title": "NLP and the Pipeline",
    "section": "Handle Anomalies",
    "text": "Handle Anomalies\nNLTK’s POS tagger is not perfect – note the classification of punctuation as nouns, verbs, etc. We remove these from our corups.\n\nCORPUS[CORPUS.term_str == '']\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      \n      pos_tuple\n      pos\n      token_str\n      term_str\n    \n    \n      book_id\n      chap_id\n      para_num\n      sent_num\n      token_num\n      \n      \n      \n      \n    \n  \n  \n    \n      105\n      3\n      1\n      5\n      12\n      (\", NNP)\n      NNP\n      \"\n      \n    \n    \n      17\n      6\n      22\n      (\", VBZ)\n      VBZ\n      \"\n      \n    \n    \n      6\n      27\n      5\n      0\n      (),, NN)\n      NN\n      ),\n      \n    \n    \n      14\n      8\n      1\n      9\n      (\", VBP)\n      VBP\n      \"\n      \n    \n    \n      20\n      8\n      4\n      14\n      (\", VB)\n      VB\n      \"\n      \n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      34970\n      79\n      12\n      0\n      1\n      (&, CC)\n      CC\n      &\n      \n    \n    \n      80\n      9\n      0\n      4\n      (_., NN)\n      NN\n      _.\n      \n    \n    \n      83\n      8\n      2\n      0\n      (),, NN)\n      NN\n      ),\n      \n    \n    \n      3\n      0\n      ();, NN)\n      NN\n      );\n      \n    \n    \n      111\n      5\n      0\n      3\n      (&, CC)\n      CC\n      &\n      \n    \n  \n\n729 rows × 4 columns\n\n\n\n\nCORPUS[CORPUS.term_str == ''].token_str.value_counts()\n\ntoken_str\n*        359\n\"        225\n|         85\n&          9\n*****      7\n),         5\n.\"         4\n+          4\n'          3\n\"...       2\n.'         2\n?\"         2\n\"*         2\n,\"         2\n!\"         1\n[*         1\n_.         1\n\".         1\n\";         1\n*\"         1\n\",         1\n....       1\n::         1\n'_         1\n_          1\n£          1\n&.         1\n'\"         1\n'*         1\n***        1\n,'         1\n);         1\nName: count, dtype: int64\n\n\n\nCORPUS = CORPUS[CORPUS.term_str != '']\n\n\nVOCAB = CORPUS.term_str.value_counts().to_frame('n').sort_index()\nVOCAB.index.name = 'term_str'\nVOCAB['n_chars'] = VOCAB.index.str.len()\nVOCAB['p'] = VOCAB.n / VOCAB.n.sum()\nVOCAB['i'] = -np.log2(VOCAB.p)"
  },
  {
    "objectID": "lessons/M04_NLP/M04_01_Pipeline.html#get-max-pos",
    "href": "lessons/M04_NLP/M04_01_Pipeline.html#get-max-pos",
    "title": "NLP and the Pipeline",
    "section": "Get Max POS",
    "text": "Get Max POS\nGet the most frequently associated part-of-space category for each word.\n\nVOCAB['max_pos'] = CORPUS[['term_str','pos']].value_counts().unstack(fill_value=0).idxmax(1)"
  },
  {
    "objectID": "lessons/M04_NLP/M04_01_Pipeline.html#compute-pos-ambiguity",
    "href": "lessons/M04_NLP/M04_01_Pipeline.html#compute-pos-ambiguity",
    "title": "NLP and the Pipeline",
    "section": "Compute POS ambiguity",
    "text": "Compute POS ambiguity\nHow many POS categories are associated with each word?\n\nVOCAB['n_pos'] = CORPUS[['term_str','pos']].value_counts().unstack().count(1)\n\n\nVOCAB['cat_pos'] = CORPUS[['term_str','pos']].value_counts().to_frame('n').reset_index()\\\n    .groupby('term_str').pos.apply(lambda x: set(x))\n\n\nVOCAB\n\n\n\n\n\n  \n    \n      \n      n\n      n_chars\n      p\n      i\n      max_pos\n      n_pos\n      cat_pos\n    \n    \n      term_str\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      2\n      1\n      9.713651e-07\n      19.973483\n      CD\n      1\n      {CD}\n    \n    \n      1\n      23\n      1\n      1.117070e-05\n      16.449921\n      CD\n      3\n      {NNP, NN, CD}\n    \n    \n      10\n      6\n      2\n      2.914095e-06\n      18.388520\n      CD\n      1\n      {CD}\n    \n    \n      100\n      2\n      3\n      9.713651e-07\n      19.973483\n      CD\n      1\n      {CD}\n    \n    \n      1000\n      2\n      4\n      9.713651e-07\n      19.973483\n      CD\n      1\n      {CD}\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      æneas\n      1\n      5\n      4.856826e-07\n      20.973483\n      NN\n      1\n      {NN}\n    \n    \n      æniad\n      1\n      5\n      4.856826e-07\n      20.973483\n      NN\n      1\n      {NN}\n    \n    \n      æson\n      2\n      4\n      9.713651e-07\n      19.973483\n      NN\n      1\n      {NN}\n    \n    \n      æsops\n      1\n      5\n      4.856826e-07\n      20.973483\n      NNS\n      1\n      {NNS}\n    \n    \n      ł20000\n      1\n      6\n      4.856826e-07\n      20.973483\n      NN\n      1\n      {NN}\n    \n  \n\n40283 rows × 7 columns\n\n\n\n\n# nltk.help.upenn_tagset()"
  },
  {
    "objectID": "lessons/M04_NLP/M04_01_Pipeline.html#add-stopwords",
    "href": "lessons/M04_NLP/M04_01_Pipeline.html#add-stopwords",
    "title": "NLP and the Pipeline",
    "section": "Add Stopwords",
    "text": "Add Stopwords\nWe use NLTK’s built in stopword list for English. Note that we can add and subtract from this list, or just create our own list and keep it in our data model.\n\nsw = pd.DataFrame(nltk.corpus.stopwords.words('english'), columns=['term_str'])\nsw = sw.reset_index().set_index('term_str')\nsw.columns = ['dummy']\nsw.dummy = 1\n\n\n# sw.sample(10)\n\n\nVOCAB['stop'] = VOCAB.index.map(sw.dummy)\nVOCAB['stop'] = VOCAB['stop'].fillna(0).astype('int')\n\n\nVOCAB[VOCAB.stop == 1].sample(10)\n\n\n\n\n\n  \n    \n      \n      n\n      n_chars\n      p\n      i\n      max_pos\n      n_pos\n      cat_pos\n      stop\n    \n    \n      term_str\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      don\n      245\n      3\n      0.000119\n      13.036845\n      NNP\n      6\n      {RB, JJ, VBZ, NNP, NN, VB}\n      1\n    \n    \n      his\n      19757\n      3\n      0.009596\n      6.703407\n      PRP$\n      16\n      {VBD, RB, JJ, VBP, VBZ, CD, NNP, PDT, JJR, NN,...\n      1\n    \n    \n      now\n      5709\n      3\n      0.002773\n      8.494461\n      RB\n      22\n      {VBP, VBZ, NN, VBD, CD, NNP, PDT, MD, RP, CC, ...\n      1\n    \n    \n      when\n      5030\n      4\n      0.002443\n      8.677140\n      WRB\n      13\n      {VBD, VBP, JJ, RB, VBZ, CD, NNP, IN, WRB, NN, ...\n      1\n    \n    \n      ourselves\n      290\n      9\n      0.000141\n      12.793574\n      PRP\n      13\n      {VBP, JJ, RB, VBZ, NNP, IN, PRP, NN, MD, VB, R...\n      1\n    \n    \n      under\n      1271\n      5\n      0.000617\n      10.661735\n      IN\n      6\n      {VBP, JJ, NNP, IN, NN, RP}\n      1\n    \n    \n      t\n      30\n      1\n      0.000015\n      16.066592\n      NN\n      5\n      {VBD, VBZ, NNP, NN, POS}\n      1\n    \n    \n      of\n      65525\n      2\n      0.031824\n      4.973725\n      IN\n      15\n      {VBD, VB, RB, JJ, VBP, VBZ, NNP, IN, PDT, PRP,...\n      1\n    \n    \n      off\n      1660\n      3\n      0.000806\n      10.276515\n      RP\n      16\n      {VBD, VB, RB, JJ, VBP, VBZ, CD, NNP, IN, PRP, ...\n      1\n    \n    \n      while\n      1970\n      5\n      0.000957\n      10.029503\n      IN\n      11\n      {VBP, JJ, VBZ, NNP, IN, WRB, JJR, NN, VB, CC, ...\n      1"
  },
  {
    "objectID": "lessons/M04_NLP/M04_01_Pipeline.html#interlude-stopword-stats",
    "href": "lessons/M04_NLP/M04_01_Pipeline.html#interlude-stopword-stats",
    "title": "NLP and the Pipeline",
    "section": "Interlude: Stopword Stats",
    "text": "Interlude: Stopword Stats\n\na = VOCAB.groupby('stop').n_chars.mean()\nb = VOCAB.groupby('stop').n_pos.mean().sort_values(ascending=False)\n\n\npd.concat([a,b], axis=1)\n\n\n\n\n\n  \n    \n      \n      n_chars\n      n_pos\n    \n    \n      stop\n      \n      \n    \n  \n  \n    \n      0\n      7.871170\n      2.138943\n    \n    \n      1\n      3.766423\n      12.416058\n    \n  \n\n\n\n\n\nVOCAB.groupby('n_chars').n_pos.mean()\\\n    .sort_values(ascending=False).plot(style='o');\n\n\n\n\nCurious that stopwords would have such variability.\n\nVOCAB[VOCAB.stop == True].sort_values('n_pos', ascending=False)[['n_pos','cat_pos']].head(20)\n\n\n\n\n\n  \n    \n      \n      n_pos\n      cat_pos\n    \n    \n      term_str\n      \n      \n    \n  \n  \n    \n      you\n      25\n      {VBP, VBZ, NN, VBD, CD, NNP, PDT, MD, RP, CC, ...\n    \n    \n      now\n      22\n      {VBP, VBZ, NN, VBD, CD, NNP, PDT, MD, RP, CC, ...\n    \n    \n      me\n      22\n      {VBP, VBZ, RBS, JJS, NN, VBD, CD, NNP, PDT, MD...\n    \n    \n      that\n      21\n      {VBP, VBZ, NN, VBD, NNP, PDT, RP, CC, EX, VBN,...\n    \n    \n      here\n      21\n      {VBP, VBZ, NN, VBD, CD, NNP, PDT, MD, CC, EX, ...\n    \n    \n      there\n      20\n      {VBP, VBZ, NN, VBD, CD, NNP, PDT, MD, RP, EX, ...\n    \n    \n      not\n      20\n      {VBP, VBZ, NN, VBD, CD, NNP, PDT, MD, RP, CC, ...\n    \n    \n      this\n      20\n      {VBP, VBZ, NN, VBD, CD, NNP, PDT, RP, POS, VBN...\n    \n    \n      more\n      20\n      {VBP, VBZ, JJS, NN, VBD, NNP, PDT, MD, CC, VBN...\n    \n    \n      to\n      20\n      {VBP, VBZ, NN, VBD, CD, NNP, PDT, TO, MD, RP, ...\n    \n    \n      him\n      20\n      {VBP, VBZ, NN, VBD, NNP, PDT, RP, CC, VBN, RB,...\n    \n    \n      what\n      20\n      {VBP, VBZ, NN, VBD, CD, NNP, MD, POS, VBN, RB,...\n    \n    \n      which\n      19\n      {VBP, VBZ, JJS, NN, VBD, CD, NNP, PDT, CC, VBN...\n    \n    \n      i\n      19\n      {VBP, VBZ, NN, VBD, CD, NNP, MD, CC, POS, VBN,...\n    \n    \n      then\n      19\n      {VBP, VBZ, NN, VBD, CD, NNP, PDT, RP, CC, EX, ...\n    \n    \n      but\n      19\n      {VBP, VBZ, NN, VBD, CD, NNP, PDT, MD, CC, POS,...\n    \n    \n      all\n      19\n      {VBP, VBZ, NN, VBD, CD, NNP, PDT, RP, CC, POS,...\n    \n    \n      her\n      19\n      {VBP, VBZ, NN, VBD, NNP, PDT, MD, RP, VBN, RB,...\n    \n    \n      she\n      18\n      {NNPS, VBD, RB, JJ, VBZ, VBP, NNP, IN, PDT, PR...\n    \n    \n      he\n      18\n      {VBD, RB, JJ, VBZ, VBP, RBR, NNP, IN, PDT, PRP...\n    \n  \n\n\n\n\nAnyway . . .\n\nX = CORPUS.merge(LIB.reset_index()[['book_id','author']], on='book_id')\\\n    .merge(VOCAB.reset_index()[['term_str', 'stop']], on='term_str')\\\n    .groupby(['author','stop']).agg('sum', numeric_only=True).unstack()\nX.columns = X.columns.droplevel(0)\n\n\n(X.T / X.T.sum()).T.style.background_gradient()\n\n\n\n\n  \n    \n      stop\n      0\n      1\n    \n    \n      author\n       \n       \n    \n  \n  \n    \n      AUSTEN, JANE\n      0.451550\n      0.548450\n    \n    \n      MELVILLE, HERMAN\n      0.508578\n      0.491422"
  },
  {
    "objectID": "lessons/M04_NLP/M04_01_Pipeline.html#add-stems",
    "href": "lessons/M04_NLP/M04_01_Pipeline.html#add-stems",
    "title": "NLP and the Pipeline",
    "section": "Add Stems",
    "text": "Add Stems\n\nfrom nltk.stem.porter import PorterStemmer\nstemmer1 = PorterStemmer()\nVOCAB['stem_porter'] = VOCAB.apply(lambda x: stemmer1.stem(x.name), 1)\n\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer2 = SnowballStemmer(\"english\")\nVOCAB['stem_snowball'] = VOCAB.apply(lambda x: stemmer2.stem(x.name), 1)\n\nfrom nltk.stem.lancaster import LancasterStemmer\nstemmer3 = LancasterStemmer()\nVOCAB['stem_lancaster'] = VOCAB.apply(lambda x: stemmer3.stem(x.name), 1)\n\n\nVOCAB.sample(10)\n\n\n\n\n\n  \n    \n      \n      n\n      n_chars\n      p\n      i\n      max_pos\n      n_pos\n      cat_pos\n      stop\n      stem_porter\n      stem_snowball\n      stem_lancaster\n    \n    \n      term_str\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      crutch\n      8\n      6\n      3.885461e-06\n      17.973483\n      NN\n      1\n      {NN}\n      0\n      crutch\n      crutch\n      crutch\n    \n    \n      distracting\n      9\n      11\n      4.371143e-06\n      17.803558\n      VBG\n      2\n      {VBG, NN}\n      0\n      distract\n      distract\n      distract\n    \n    \n      unthinkingness\n      1\n      14\n      4.856826e-07\n      20.973483\n      NN\n      1\n      {NN}\n      0\n      unthinking\n      unthinking\n      unthink\n    \n    \n      smeared\n      1\n      7\n      4.856826e-07\n      20.973483\n      VBN\n      1\n      {VBN}\n      0\n      smear\n      smear\n      smear\n    \n    \n      curantur\n      1\n      8\n      4.856826e-07\n      20.973483\n      NNP\n      1\n      {NNP}\n      0\n      curantur\n      curantur\n      cur\n    \n    \n      raakeel\n      1\n      7\n      4.856826e-07\n      20.973483\n      NN\n      1\n      {NN}\n      0\n      raakeel\n      raakeel\n      raakeel\n    \n    \n      otoo\n      9\n      4\n      4.371143e-06\n      17.803558\n      NNP\n      2\n      {NNP, NN}\n      0\n      otoo\n      otoo\n      otoo\n    \n    \n      babel\n      6\n      5\n      2.914095e-06\n      18.388520\n      NNP\n      2\n      {NNP, NN}\n      0\n      babel\n      babel\n      babel\n    \n    \n      humanity\n      108\n      8\n      5.245372e-05\n      14.218595\n      NN\n      7\n      {VBP, JJ, NNP, PDT, NN, VB, VBN}\n      0\n      human\n      human\n      hum\n    \n    \n      extreme\n      111\n      7\n      5.391076e-05\n      14.179067\n      JJ\n      4\n      {JJ, NNP, NN, VBP}\n      0\n      extrem\n      extrem\n      extrem\n    \n  \n\n\n\n\n\nVOCAB[VOCAB.stem_porter != VOCAB.stem_snowball]\n\n\n\n\n\n  \n    \n      \n      n\n      n_chars\n      p\n      i\n      max_pos\n      n_pos\n      cat_pos\n      stop\n      stem_porter\n      stem_snowball\n      stem_lancaster\n    \n    \n      term_str\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      abandonedly\n      2\n      11\n      9.713651e-07\n      19.973483\n      RB\n      1\n      {RB}\n      0\n      abandonedli\n      abandon\n      abandon\n    \n    \n      abhorringly\n      1\n      11\n      4.856826e-07\n      20.973483\n      RB\n      1\n      {RB}\n      0\n      abhorringli\n      abhor\n      abhor\n    \n    \n      abjectly\n      2\n      8\n      9.713651e-07\n      19.973483\n      RB\n      1\n      {RB}\n      0\n      abjectli\n      abject\n      abject\n    \n    \n      abjectus\n      1\n      8\n      4.856826e-07\n      20.973483\n      NNP\n      1\n      {NNP}\n      0\n      abjectu\n      abjectus\n      abject\n    \n    \n      aboundingly\n      1\n      11\n      4.856826e-07\n      20.973483\n      RB\n      1\n      {RB}\n      0\n      aboundingli\n      abound\n      abound\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      yes\n      945\n      3\n      4.589700e-04\n      11.089312\n      NN\n      20\n      {VBP, VBZ, NN, VBD, CD, NNP, RP, CC, POS, VBN,...\n      0\n      ye\n      yes\n      ye\n    \n    \n      yous\n      1\n      4\n      4.856826e-07\n      20.973483\n      NN\n      1\n      {NN}\n      0\n      you\n      yous\n      yo\n    \n    \n      yrs\n      4\n      3\n      1.942730e-06\n      18.973483\n      NNP\n      2\n      {NNP, NN}\n      0\n      yr\n      yrs\n      yr\n    \n    \n      zadockprattsville\n      2\n      17\n      9.713651e-07\n      19.973483\n      NN\n      2\n      {NNP, NN}\n      0\n      zadockprattsvil\n      zadockprattsvill\n      zadockprattsvil\n    \n    \n      zealous\n      12\n      7\n      5.828191e-06\n      17.388520\n      JJ\n      1\n      {JJ}\n      0\n      zealou\n      zealous\n      zeal\n    \n  \n\n1259 rows × 11 columns"
  },
  {
    "objectID": "lessons/M04_NLP/M04_02_HMM.html",
    "href": "lessons/M04_NLP/M04_02_HMM.html",
    "title": "Predict POS with an HMM",
    "section": "",
    "text": "Set Up\nWe use the CORPUS table from our previous exercise.\nWe create a table rrom the Penn Treebank Project list of part-of-speech tags.\nImport file of raw test sentences.\nCreate dataframe of sentences. We will add features here later.\nConvert sentences to tokens (and terms).\nThe Vertibi implements the optimization function:\nTranslation to NLP use case:\n\\(Q \\rightarrow\\) A set of \\(N\\) POS tags.\n\\(O \\rightarrow\\) A sequence of \\(T\\) tokens that compose a sentence.\n\\(A \\rightarrow\\) POS to POS table \\(P(q_i|q_{i-1})\\).\n\\(B \\rightarrow\\) POS to word table \\(P(w_i|q_i)\\).\n\\(\\Pi \\rightarrow\\) Probabilites of POS as tag for first token.\nThis version uses Pandas, but assumes that data are stored in F2 compliant dataframes, i.e. there is TOKENS table.\nAll of the resulting data are put where they belong in the model."
  },
  {
    "objectID": "lessons/M04_NLP/M04_02_HMM.html#grab-upenn-codes",
    "href": "lessons/M04_NLP/M04_02_HMM.html#grab-upenn-codes",
    "title": "Predict POS with an HMM",
    "section": "Grab UPenn Codes",
    "text": "Grab UPenn Codes\nWe get a text version of the UPenn Codes, or tag set, by calling nltk.help.upenn_tagset(). The extra information can be used later in visualizations, etc.\n\nPOS = pd.read_csv(f'{data_home}/misc/upenn_tagset.txt', sep='\\t', names=['pos_code','def'])\nPOS = POS[POS.pos_code.str.match(r'^\\w')].set_index('pos_code') # Keep only letter codes\n\n\nPOS\n\n\n\n\n\n  \n    \n      \n      def\n    \n    \n      pos_code\n      \n    \n  \n  \n    \n      CC\n      conjunction, coordinating\n    \n    \n      CD\n      numeral, cardinal\n    \n    \n      DT\n      determiner\n    \n    \n      EX\n      existential there\n    \n    \n      FW\n      foreign word\n    \n    \n      IN\n      preposition or conjunction, subordinating\n    \n    \n      JJ\n      adjective or numeral, ordinal\n    \n    \n      JJR\n      adjective, comparative\n    \n    \n      JJS\n      adjective, superlative\n    \n    \n      LS\n      list item marker\n    \n    \n      MD\n      modal auxiliary\n    \n    \n      NN\n      noun, common, singular or mass\n    \n    \n      NNP\n      noun, proper, singular\n    \n    \n      NNPS\n      noun, proper, plural\n    \n    \n      NNS\n      noun, common, plural\n    \n    \n      PDT\n      pre-determiner\n    \n    \n      POS\n      genitive marker\n    \n    \n      PRP\n      pronoun, personal\n    \n    \n      PRP$\n      pronoun, possessive\n    \n    \n      RB\n      adverb\n    \n    \n      RBR\n      adverb, comparative\n    \n    \n      RBS\n      adverb, superlative\n    \n    \n      RP\n      particle\n    \n    \n      SYM\n      symbol\n    \n    \n      TO\n      \"to\" as preposition or infinitive marker\n    \n    \n      UH\n      interjection\n    \n    \n      VB\n      verb, base form\n    \n    \n      VBD\n      verb, past tense\n    \n    \n      VBG\n      verb, present participle or gerund\n    \n    \n      VBN\n      verb, past participle\n    \n    \n      VBP\n      verb, present tense, not 3rd person singular\n    \n    \n      VBZ\n      verb, present tense, 3rd person singular\n    \n    \n      WDT\n      WH-determiner\n    \n    \n      WP\n      WH-pronoun\n    \n    \n      WP$\n      WH-pronoun, possessive\n    \n    \n      WRB\n      Wh-adverb"
  },
  {
    "objectID": "lessons/M04_NLP/M04_02_HMM.html#add-value-counts",
    "href": "lessons/M04_NLP/M04_02_HMM.html#add-value-counts",
    "title": "Predict POS with an HMM",
    "section": "Add value counts",
    "text": "Add value counts\n\nPOS['n'] = TOKEN.pos.value_counts().to_frame().sort_index()\nPOS['n'] = POS['n'].fillna(0).astype('int')\n\n\nPOS.sort_values('n', ascending=False)\n\n\n\n\n\n  \n    \n      \n      def\n      n\n    \n    \n      pos_code\n      \n      \n    \n  \n  \n    \n      NN\n      noun, common, singular or mass\n      383976\n    \n    \n      IN\n      preposition or conjunction, subordinating\n      264122\n    \n    \n      DT\n      determiner\n      204813\n    \n    \n      JJ\n      adjective or numeral, ordinal\n      155155\n    \n    \n      PRP\n      pronoun, personal\n      117186\n    \n    \n      RB\n      adverb\n      113500\n    \n    \n      VBD\n      verb, past tense\n      104096\n    \n    \n      NNP\n      noun, proper, singular\n      97366\n    \n    \n      VB\n      verb, base form\n      90361\n    \n    \n      CC\n      conjunction, coordinating\n      83450\n    \n    \n      PRP$\n      pronoun, possessive\n      59075\n    \n    \n      NNS\n      noun, common, plural\n      58630\n    \n    \n      TO\n      \"to\" as preposition or infinitive marker\n      55581\n    \n    \n      VBN\n      verb, past participle\n      51108\n    \n    \n      VBG\n      verb, present participle or gerund\n      41914\n    \n    \n      VBP\n      verb, present tense, not 3rd person singular\n      40009\n    \n    \n      MD\n      modal auxiliary\n      31876\n    \n    \n      VBZ\n      verb, present tense, 3rd person singular\n      29391\n    \n    \n      CD\n      numeral, cardinal\n      12610\n    \n    \n      WDT\n      WH-determiner\n      11000\n    \n    \n      WP\n      WH-pronoun\n      9930\n    \n    \n      WRB\n      Wh-adverb\n      9757\n    \n    \n      RP\n      particle\n      7897\n    \n    \n      JJR\n      adjective, comparative\n      5268\n    \n    \n      JJS\n      adjective, superlative\n      4710\n    \n    \n      PDT\n      pre-determiner\n      4686\n    \n    \n      EX\n      existential there\n      3776\n    \n    \n      RBR\n      adverb, comparative\n      3360\n    \n    \n      RBS\n      adverb, superlative\n      2199\n    \n    \n      WP$\n      WH-pronoun, possessive\n      842\n    \n    \n      FW\n      foreign word\n      490\n    \n    \n      NNPS\n      noun, proper, plural\n      415\n    \n    \n      POS\n      genitive marker\n      332\n    \n    \n      UH\n      interjection\n      74\n    \n    \n      LS\n      list item marker\n      3\n    \n    \n      SYM\n      symbol\n      0"
  },
  {
    "objectID": "lessons/M04_NLP/M04_02_HMM.html#create-pos_group-table",
    "href": "lessons/M04_NLP/M04_02_HMM.html#create-pos_group-table",
    "title": "Predict POS with an HMM",
    "section": "Create POS_GROUP table",
    "text": "Create POS_GROUP table\nWe want a simplified list of grammatical categories. We use the first two letters of each code name as our group.\n\n# Add pos_group feature to POS and TOKEN\nPOS['pos_group'] = POS.apply(lambda x: x.name[:2], 1)\nTOKEN['pos_group'] = TOKEN.pos.str[:2]\n\n\n# Generate new table from it\nPOS_GROUP = POS.groupby('pos_group').n.sum().to_frame('n')\nPOS_GROUP = POS_GROUP[POS_GROUP.n > 0]\nPOS_GROUP['def'] = POS.groupby('pos_group').apply(lambda x: '; '.join(x['def']))\nPOS_GROUP['p'] = POS_GROUP.n / POS_GROUP.n.sum()\nPOS_GROUP['i'] = np.log2(1/POS_GROUP.p)\nPOS_GROUP['h'] = POS_GROUP.p * POS_GROUP.i\n\n\nPOS_GROUP.sort_values('i').style.background_gradient(cmap=colors)\n\n\n\n\n  \n    \n       \n      n\n      def\n      p\n      i\n      h\n    \n    \n      pos_group\n       \n       \n       \n       \n       \n    \n  \n  \n    \n      NN\n      540387\n       noun, common, singular or mass;  noun, proper, singular;  noun, proper, plural;  noun, common, plural\n      0.262457\n      1.929850\n      0.506502\n    \n    \n      VB\n      356879\n       verb, base form;  verb, past tense;  verb, present participle or gerund;  verb, past participle;  verb, present tense, not 3rd person singular;  verb, present tense, 3rd person singular\n      0.173330\n      2.528407\n      0.438249\n    \n    \n      IN\n      264122\n       preposition or conjunction, subordinating\n      0.128279\n      2.962638\n      0.380046\n    \n    \n      DT\n      204813\n       determiner\n      0.099474\n      3.329535\n      0.331203\n    \n    \n      PR\n      176261\n       pronoun, personal;  pronoun, possessive\n      0.085607\n      3.546129\n      0.303573\n    \n    \n      JJ\n      165133\n       adjective or numeral, ordinal;  adjective, comparative;  adjective, superlative\n      0.080202\n      3.640214\n      0.291953\n    \n    \n      RB\n      119059\n       adverb;  adverb, comparative;  adverb, superlative\n      0.057825\n      4.112166\n      0.237785\n    \n    \n      CC\n      83450\n       conjunction, coordinating\n      0.040530\n      4.624859\n      0.187446\n    \n    \n      TO\n      55581\n       \"to\" as preposition or infinitive marker\n      0.026995\n      5.211179\n      0.140674\n    \n    \n      MD\n      31876\n       modal auxiliary\n      0.015482\n      6.013300\n      0.093096\n    \n    \n      CD\n      12610\n       numeral, cardinal\n      0.006124\n      7.351202\n      0.045022\n    \n    \n      WD\n      11000\n       WH-determiner\n      0.005343\n      7.548267\n      0.040327\n    \n    \n      WP\n      10772\n       WH-pronoun;  WH-pronoun, possessive\n      0.005232\n      7.578484\n      0.039649\n    \n    \n      WR\n      9757\n       Wh-adverb\n      0.004739\n      7.721261\n      0.036590\n    \n    \n      RP\n      7897\n       particle\n      0.003835\n      8.026394\n      0.030785\n    \n    \n      PD\n      4686\n       pre-determiner\n      0.002276\n      8.779342\n      0.019981\n    \n    \n      EX\n      3776\n       existential there\n      0.001834\n      9.090840\n      0.016672\n    \n    \n      FW\n      490\n       foreign word\n      0.000238\n      12.036845\n      0.002865\n    \n    \n      PO\n      332\n       genitive marker\n      0.000161\n      12.598444\n      0.002031\n    \n    \n      UH\n      74\n       interjection\n      0.000036\n      14.764030\n      0.000531\n    \n    \n      LS\n      3\n       list item marker\n      0.000001\n      19.388520\n      0.000028"
  },
  {
    "objectID": "lessons/M04_NLP/M04_02_HMM.html#save",
    "href": "lessons/M04_NLP/M04_02_HMM.html#save",
    "title": "Predict POS with an HMM",
    "section": "Save",
    "text": "Save\n\nPOS.to_csv(f\"{output_dir}/{data_prefix}-POS.csv\")\nPOS_GROUP.to_csv(f\"{output_dir}/{data_prefix}-POS_GROUP.csv\")"
  },
  {
    "objectID": "lessons/M04_NLP/M04_02_HMM.html#create-table-of-t_n-rightarrow-t_n1-transissions-a",
    "href": "lessons/M04_NLP/M04_02_HMM.html#create-table-of-t_n-rightarrow-t_n1-transissions-a",
    "title": "Predict POS with an HMM",
    "section": "Create Table of \\(t_n \\rightarrow t_{n+1}\\) Transissions (\\(A\\))",
    "text": "Create Table of \\(t_n \\rightarrow t_{n+1}\\) Transissions (\\(A\\))\nAdd sentence boundaries to sequence\nNote that we are breaking our data by adding <s> as a key, although it is not part of the UPenn tagset. We might consider creating a code for sentence beginnings or endings, such as the stop tag in Eistenstein 2019.\n\nA = TOKEN[['pos_group']].join(TOKEN[['pos_group']].shift(-1), lsuffix='_x', rsuffix=\"_y\")\\\n    .value_counts().to_frame('n').sort_index()\n\n\nA['cp'] = A.n / A.groupby('pos_group_x').n.sum()  # P(y|x)\n\n\nA\n\n\n\n\n\n  \n    \n      \n      \n      n\n      cp\n    \n    \n      pos_group_x\n      pos_group_y\n      \n      \n    \n  \n  \n    \n      CC\n      CC\n      117\n      0.001402\n    \n    \n      CD\n      1162\n      0.013925\n    \n    \n      DT\n      9585\n      0.114859\n    \n    \n      EX\n      442\n      0.005297\n    \n    \n      FW\n      1\n      0.000012\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      WP\n      MD\n      1161\n      0.107779\n    \n    \n      NN\n      1521\n      0.141199\n    \n    \n      RB\n      525\n      0.048737\n    \n    \n      TO\n      119\n      0.011047\n    \n    \n      VB\n      5872\n      0.545117\n    \n  \n\n205 rows × 2 columns\n\n\n\n\nAM = A.cp.unstack(fill_value=0)\n\n\nAM.style.background_gradient(axis=None, cmap=colors)\n\n\n\n\n  \n    \n      pos_group_y\n      CC\n      CD\n      DT\n      EX\n      FW\n      IN\n      JJ\n      LS\n      MD\n      NN\n      RB\n      RP\n      TO\n      UH\n      VB\n      WP\n    \n    \n      pos_group_x\n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n  \n  \n    \n      CC\n      0.001402\n      0.013925\n      0.114859\n      0.005297\n      0.000012\n      0.105620\n      0.088424\n      0.000000\n      0.024841\n      0.248209\n      0.103583\n      0.000024\n      0.010030\n      0.000012\n      0.275183\n      0.008580\n    \n    \n      CD\n      0.059794\n      0.016098\n      0.009120\n      0.000397\n      0.000079\n      0.168438\n      0.098652\n      0.000000\n      0.018002\n      0.513561\n      0.017684\n      0.000159\n      0.010706\n      0.000000\n      0.078588\n      0.008723\n    \n    \n      DT\n      0.001094\n      0.009667\n      0.002046\n      0.000112\n      0.000308\n      0.013202\n      0.247465\n      0.000000\n      0.002676\n      0.658044\n      0.028870\n      0.000142\n      0.001396\n      0.000005\n      0.033426\n      0.001548\n    \n    \n      EX\n      0.002119\n      0.001324\n      0.025424\n      0.000000\n      0.000000\n      0.020922\n      0.010593\n      0.000000\n      0.092956\n      0.016419\n      0.029396\n      0.000000\n      0.001589\n      0.000000\n      0.798994\n      0.000265\n    \n    \n      FW\n      0.040816\n      0.018367\n      0.175510\n      0.000000\n      0.057143\n      0.022449\n      0.075510\n      0.000000\n      0.006122\n      0.430612\n      0.059184\n      0.002041\n      0.004082\n      0.000000\n      0.108163\n      0.000000\n    \n    \n      IN\n      0.003771\n      0.012369\n      0.361609\n      0.002344\n      0.000435\n      0.039046\n      0.095100\n      0.000000\n      0.013906\n      0.320852\n      0.024780\n      0.000117\n      0.009863\n      0.000000\n      0.109041\n      0.006766\n    \n    \n      JJ\n      0.050493\n      0.005232\n      0.024156\n      0.000799\n      0.000400\n      0.091696\n      0.070028\n      0.000000\n      0.009120\n      0.630655\n      0.021558\n      0.000418\n      0.033785\n      0.000073\n      0.060127\n      0.001459\n    \n    \n      LS\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.333333\n      0.000000\n      0.000000\n      0.000000\n      0.333333\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.333333\n      0.000000\n    \n    \n      MD\n      0.004674\n      0.000784\n      0.011796\n      0.000063\n      0.000000\n      0.013333\n      0.001286\n      0.000000\n      0.000345\n      0.008094\n      0.256431\n      0.000094\n      0.011890\n      0.000000\n      0.691021\n      0.000188\n    \n    \n      NN\n      0.112549\n      0.003248\n      0.042404\n      0.003133\n      0.000320\n      0.255391\n      0.022146\n      0.000006\n      0.032759\n      0.214466\n      0.049440\n      0.000640\n      0.032720\n      0.000081\n      0.221690\n      0.009007\n    \n    \n      RB\n      0.022930\n      0.005342\n      0.047917\n      0.001764\n      0.000025\n      0.135000\n      0.198019\n      0.000000\n      0.012481\n      0.046229\n      0.117471\n      0.000596\n      0.035319\n      0.000000\n      0.374386\n      0.002520\n    \n    \n      RP\n      0.028365\n      0.008231\n      0.231100\n      0.000253\n      0.000127\n      0.335064\n      0.048626\n      0.000000\n      0.003039\n      0.189439\n      0.048499\n      0.000380\n      0.057110\n      0.000000\n      0.042801\n      0.006965\n    \n    \n      TO\n      0.003634\n      0.004516\n      0.161764\n      0.000162\n      0.000036\n      0.015887\n      0.020798\n      0.000000\n      0.000990\n      0.101779\n      0.005416\n      0.000414\n      0.001727\n      0.000000\n      0.676940\n      0.005937\n    \n    \n      UH\n      0.000000\n      0.000000\n      0.013514\n      0.000000\n      0.000000\n      0.040541\n      0.067568\n      0.000000\n      0.027027\n      0.702703\n      0.040541\n      0.000000\n      0.081081\n      0.000000\n      0.027027\n      0.000000\n    \n    \n      VB\n      0.024745\n      0.006602\n      0.152842\n      0.001760\n      0.000095\n      0.186478\n      0.088134\n      0.000000\n      0.008558\n      0.164949\n      0.123140\n      0.020503\n      0.064991\n      0.000045\n      0.151438\n      0.005722\n    \n    \n      WP\n      0.003620\n      0.002971\n      0.060899\n      0.001021\n      0.000278\n      0.032306\n      0.045024\n      0.000000\n      0.107779\n      0.141199\n      0.048737\n      0.000000\n      0.011047\n      0.000000\n      0.545117\n      0.000000"
  },
  {
    "objectID": "lessons/M04_NLP/M04_02_HMM.html#create-table-of-initial-state-probabilities-pi",
    "href": "lessons/M04_NLP/M04_02_HMM.html#create-table-of-initial-state-probabilities-pi",
    "title": "Predict POS with an HMM",
    "section": "Create Table of Initial State Probabilities \\(\\Pi\\)",
    "text": "Create Table of Initial State Probabilities \\(\\Pi\\)\nWe get counts of pos tags that appear at the beginnings of sentences.\nNote: since we took the trouble to create an OHCO index, we don’t need to add <s> markers to find out these frequencies.\n\nPI = TOKEN.query(\"token_num == 0\").pos_group.value_counts().to_frame('n').sort_index()\nPI.index.name = 'pos_group'\nPI['cp'] = PI.n / PI.n.sum()"
  },
  {
    "objectID": "lessons/M04_NLP/M04_02_HMM.html#make-sure-shares-sample-space-with-a-and-b",
    "href": "lessons/M04_NLP/M04_02_HMM.html#make-sure-shares-sample-space-with-a-and-b",
    "title": "Predict POS with an HMM",
    "section": "Make sure $$ shares sample space with \\(A\\) and \\(B\\)",
    "text": "Make sure $$ shares sample space with \\(A\\) and \\(B\\)\nFind out what is missing from PI.\nThis will be important below.\n\nfor tag in (set(AM.index) - set(PI.index)):\n    PI.loc[tag, ['n', 'cp']] = (0, 0)\n\n\nPI\n\n\n\n\n\n  \n    \n      \n      n\n      cp\n    \n    \n      pos_group\n      \n      \n    \n  \n  \n    \n      CC\n      7864.0\n      0.107079\n    \n    \n      CD\n      743.0\n      0.010117\n    \n    \n      DT\n      9868.0\n      0.134366\n    \n    \n      EX\n      917.0\n      0.012486\n    \n    \n      FW\n      8.0\n      0.000109\n    \n    \n      IN\n      10245.0\n      0.139500\n    \n    \n      JJ\n      4380.0\n      0.059640\n    \n    \n      LS\n      3.0\n      0.000041\n    \n    \n      MD\n      418.0\n      0.005692\n    \n    \n      NN\n      25604.0\n      0.348634\n    \n    \n      RB\n      5347.0\n      0.072807\n    \n    \n      TO\n      745.0\n      0.010144\n    \n    \n      UH\n      50.0\n      0.000681\n    \n    \n      VB\n      6309.0\n      0.085906\n    \n    \n      WP\n      940.0\n      0.012799\n    \n    \n      RP\n      0.0\n      0.000000\n    \n  \n\n\n\n\n\nPI.sort_values('cp', ascending=True).cp.plot.barh();"
  },
  {
    "objectID": "lessons/M04_NLP/M04_02_HMM.html#create-table-of-t_n-rightarrow-o_n-emissions-b",
    "href": "lessons/M04_NLP/M04_02_HMM.html#create-table-of-t_n-rightarrow-o_n-emissions-b",
    "title": "Predict POS with an HMM",
    "section": "Create Table of \\(t_n \\rightarrow o_n\\) Emissions ($ B $)",
    "text": "Create Table of \\(t_n \\rightarrow o_n\\) Emissions ($ B $)\n\nB = TOKEN[['pos_group', 'term_str']].value_counts().to_frame('n').sort_index()\nB.index.names = ['pos_group_y', 'term_str']\nB['cp'] = B.n / B.groupby('pos_group_y').n.sum()\n\n\nB.head()\n\n\n\n\n\n  \n    \n      \n      \n      n\n      cp\n    \n    \n      pos_group_y\n      term_str\n      \n      \n    \n  \n  \n    \n      CC\n      acquired\n      1\n      0.000012\n    \n    \n      ah\n      1\n      0.000012\n    \n    \n      all\n      2\n      0.000024\n    \n    \n      altogether\n      3\n      0.000036\n    \n    \n      and\n      60705\n      0.727442\n    \n  \n\n\n\n\n\nBM = B.cp.unstack(fill_value=0)\n\n\nBM\n\n\n\n\n\n  \n    \n      term_str\n      0\n      1\n      10\n      100\n      1000\n      10000\n      10000000\n      10440\n      10800\n      10th\n      ...\n      zoroaster\n      zozo\n      zuma\n      zur\n      à\n      æneas\n      æniad\n      æson\n      æsops\n      ł20000\n    \n    \n      pos_group_y\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      CC\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      CD\n      0.000159\n      0.001348\n      0.000476\n      0.000159\n      0.000159\n      0.000238\n      0.000079\n      0.000079\n      0.000159\n      0.000079\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      DT\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      EX\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      FW\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      IN\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      JJ\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000006\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      LS\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      MD\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      NN\n      0.000000\n      0.000011\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000002\n      ...\n      0.000004\n      0.000000\n      0.000015\n      0.000004\n      0.000007\n      0.000002\n      0.000002\n      0.000004\n      0.000002\n      0.000002\n    \n    \n      RB\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      RP\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      TO\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      UH\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      VB\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000003\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      WP\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n  \n\n16 rows × 40262 columns"
  },
  {
    "objectID": "lessons/M04_NLP/M04_02_HMM.html#put-into-single-model-hmm",
    "href": "lessons/M04_NLP/M04_02_HMM.html#put-into-single-model-hmm",
    "title": "Predict POS with an HMM",
    "section": "Put into single model HMM",
    "text": "Put into single model HMM\nFor convenience, bind the tables into signle model table.\n\nHMM = pd.concat([AM, PI.cp, BM], keys=['trans','start','emit'], axis=1)\n\n\n# HMM.start"
  },
  {
    "objectID": "lessons/M04_NLP/M04_02_HMM.html#define-function",
    "href": "lessons/M04_NLP/M04_02_HMM.html#define-function",
    "title": "Predict POS with an HMM",
    "section": "Define function",
    "text": "Define function\n\ndef viterbi(obs, HMM):\n    \n    # Import model\n    states = HMM.index.to_list()\n    start_prob = HMM.start.cp\n    trans_prob = HMM.trans\n    emit_prob = HMM.emit\n    \n    # Set up Viterbi lattice\n    V = pd.DataFrame(np.zeros((len(states), len(obs))), \n                     columns=range(len(obs)), \n                     index=states)\n        \n    # Initialize the first observation\n    V[0] = start_prob * emit_prob[obs[0]]\n    \n    # Iterate over the remaining observations\n    for t, o in enumerate(obs[1:], start=1):\n        \n        # Simple method -- works OK\n        # V[t] = trans_probs[V[t-1].idxmax()] * emit_probs[o]\n        \n        for s in states:\n            try:\n                V.loc[s, t] = (V[t-1] * trans_prob[s] * emit_prob.loc[s, o]).max()\n            except KeyError:\n                V.loc[s, t] = (V[t-1] * trans_prob[s]).max() # Fallback of word is OOV\n    \n    return V\n\n\nobs is a list representing the sequence of observed events \\(O\\)\nstates is a list of the hidden states \\(Q\\)\nstart_prob is a pandas Series representing the probability distribution over the states at time 0 \\(\\Pi\\)\ntrans_prob is a pandas DataFrame representing the transition probability matrix \\(A\\)\nemit_prob is a pandas DataFrame representing the emission probability matrix \\(B\\)\n\nThe function returns a data frame of the Viterbi lattice, which can be used to compute most likely sequence of hidden states that generated the observed events."
  },
  {
    "objectID": "lessons/M04_NLP/M04_02_HMM.html#test-with-one-sentence",
    "href": "lessons/M04_NLP/M04_02_HMM.html#test-with-one-sentence",
    "title": "Predict POS with an HMM",
    "section": "Test with one sentence",
    "text": "Test with one sentence\n\ntest_observations = TEST_TOKENS.loc[TEST_SENTENCES.sample().index[0]].term_str.to_list()\n\n\nV = viterbi(test_observations, HMM)\n\n\npd.concat([pd.Series(test_observations), V.idxmax()], keys=['obs', 'tag'], axis=1).T\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n    \n  \n  \n    \n      obs\n      mr\n      weston\n      was\n      a\n      man\n      of\n      unexceptionable\n      character\n      easy\n      fortune\n      suitable\n      age\n      and\n      pleasant\n      manners\n    \n    \n      tag\n      NN\n      NN\n      VB\n      DT\n      NN\n      IN\n      JJ\n      NN\n      JJ\n      NN\n      JJ\n      NN\n      CC\n      JJ\n      NN\n    \n  \n\n\n\n\n\nfor w, k in zip(test_observations, V.idxmax().values):\n    print(f\"{w}/{k}\", end=' ')\n\nmr/NN weston/NN was/VB a/DT man/NN of/IN unexceptionable/JJ character/NN easy/JJ fortune/NN suitable/JJ age/NN and/CC pleasant/JJ manners/NN"
  },
  {
    "objectID": "lessons/M04_NLP/M04_02_HMM.html#diminishing-probabilities",
    "href": "lessons/M04_NLP/M04_02_HMM.html#diminishing-probabilities",
    "title": "Predict POS with an HMM",
    "section": "Diminishing probabilities",
    "text": "Diminishing probabilities\nNote diminishing probabilities in the lattice.\n\nV.style.background_gradient(cmap=colors)\n\n\n\n\n  \n    \n       \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n    \n  \n  \n    \n      CC\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      CD\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      DT\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      EX\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      FW\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      IN\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      JJ\n      0.000018\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      LS\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      MD\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      NN\n      0.002150\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      RB\n      0.000002\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      RP\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      TO\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      UH\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      VB\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      WP\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n  \n\n\n\nTry L1 normalization by column.\n\nVNORM = (V / V.sum()) \n\n\nVNORM.max().plot.bar(rot=0);\n\n\n\n\n\nVNORM.style.background_gradient(cmap=colors)\n\n\n\n\n  \n    \n       \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n    \n  \n  \n    \n      CC\n      0.000000\n      0.000000\n      0.000090\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.988700\n      0.000000\n      0.000000\n    \n    \n      CD\n      0.000000\n      0.000000\n      0.000000\n      0.000048\n      0.000165\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000012\n      0.000000\n      0.000000\n    \n    \n      DT\n      0.000000\n      0.000000\n      0.000000\n      0.993741\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      EX\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      FW\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000405\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.002511\n    \n    \n      IN\n      0.000000\n      0.000000\n      0.001614\n      0.000022\n      0.000022\n      0.996264\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.001986\n      0.000899\n      0.000000\n      0.001069\n    \n    \n      JJ\n      0.008322\n      0.000000\n      0.000251\n      0.002262\n      0.008383\n      0.000132\n      1.000000\n      0.000749\n      0.557584\n      0.014583\n      0.805422\n      0.012132\n      0.000220\n      0.816619\n      0.002611\n    \n    \n      LS\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000022\n      0.000000\n      0.000000\n    \n    \n      MD\n      0.000000\n      0.000000\n      0.000686\n      0.000000\n      0.000018\n      0.000033\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000025\n      0.000000\n      0.000000\n    \n    \n      NN\n      0.990721\n      1.000000\n      0.007791\n      0.002941\n      0.989633\n      0.002906\n      0.000000\n      0.997763\n      0.230737\n      0.974166\n      0.194578\n      0.984918\n      0.008344\n      0.132670\n      0.991695\n    \n    \n      RB\n      0.000845\n      0.000000\n      0.000111\n      0.000883\n      0.000104\n      0.000270\n      0.000000\n      0.000000\n      0.036579\n      0.000000\n      0.000000\n      0.000000\n      0.000511\n      0.015077\n      0.000557\n    \n    \n      RP\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000040\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000013\n      0.000000\n      0.000000\n    \n    \n      TO\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      UH\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      VB\n      0.000111\n      0.000000\n      0.989457\n      0.000104\n      0.001270\n      0.000355\n      0.000000\n      0.001488\n      0.175100\n      0.011251\n      0.000000\n      0.000964\n      0.001253\n      0.035634\n      0.001556\n    \n    \n      WP\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000"
  },
  {
    "objectID": "lessons/M04_NLP/M04_02_HMM.html#define-the-function",
    "href": "lessons/M04_NLP/M04_02_HMM.html#define-the-function",
    "title": "Predict POS with an HMM",
    "section": "Define the function",
    "text": "Define the function\n\ndef viterbi_eta(sent_tokens, verbose=True):\n    \"\"\"\n    - _sent_tokens_ must be a selection from an F1 TOKENS table\n    \"\"\"\n    \n    # Assumes HMM is in scope\n    \n    # Import model\n    TAGS = HMM.index.to_list()\n    PI = HMM.start.cp\n    AM = HMM.trans\n    BM = HMM.emit\n    \n    # Extract basic data\n    ohco = sent_tokens.index.names # Save for return \n    TOKENS = sent_tokens.reset_index() # Reset index so we can access term_str as a column\n\n    # Define dataframes    \n    LATTICE = pd.DataFrame(np.zeros((len(TAGS), len(TOKENS)), dtype='float'), index=TAGS, columns=TOKENS.token_num) \n    LATTICE.index.name = 'pos_group'\n    \n    # Handle first word (NB: fallback net yet implemented)\n    first_word = TOKENS.loc[0].term_str\n    LATTICE[0] = PI * BM[first_word]\n\n    # Predict the rest of the sentence\n    for t in range(1, len(TOKENS)): # Offset of token\n        word = TOKENS.loc[t].term_str\n        for s in TAGS: # tag name\n            if word not in BM.columns:\n                b = 1/len(TAGS) # Fallback probability = equiprobable\n            else:\n                b = BM.loc[s, word]\n            LATTICE.loc[s, t] = (LATTICE[t-1] * AM[s] * b).max()\n\n    # Compute results (no need for backpointer, etc.) and bind to TOKENS\n    # LATTICE = LATTICE / LATTICE.sum() # Normalize diminishing probs\n    TOKENS['tag'] = LATTICE.idxmax()\n    TOKENS['prob'] = LATTICE.max()\n    TOKENS['i'] = np.log2(1/TOKENS.prob)\n    \n    if verbose:\n        pp = 2**TOKENS.i.mean().round()\n        print(\" \".join((TOKENS.term_str + '/' + TOKENS.tag).values), f\"({pp})\")\n    \n    # Add index back and return\n    TOKENS = TOKENS.set_index(ohco)\n    return TOKENS"
  },
  {
    "objectID": "lessons/M04_NLP/M04_02_HMM.html#try-wiith-one-sentence",
    "href": "lessons/M04_NLP/M04_02_HMM.html#try-wiith-one-sentence",
    "title": "Predict POS with an HMM",
    "section": "Try wiith one sentence",
    "text": "Try wiith one sentence\n\nx = TEST_SENTENCES.sample().index[0]\n# x = 41\nX2 = viterbi_eta(TEST_TOKENS.loc[x])\n\nbut/CC it/NN was/VB a/DT black/JJ mornings/NN work/NN for/IN her/NN (2199023255552.0)\n\n\n\nX2\n\n\n\n\n\n  \n    \n      \n      token_str\n      term_str\n      tag\n      prob\n      i\n    \n    \n      token_num\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      but\n      but\n      CC\n      1.912925e-02\n      5.708076\n    \n    \n      1\n      it\n      it\n      NN\n      3.654355e-05\n      14.740024\n    \n    \n      2\n      was\n      was\n      VB\n      5.415216e-07\n      20.816478\n    \n    \n      3\n      a\n      a\n      DT\n      1.764791e-08\n      25.755927\n    \n    \n      4\n      black\n      black\n      JJ\n      9.256373e-12\n      36.652690\n    \n    \n      5\n      morning's\n      mornings\n      NN\n      6.373707e-16\n      50.478717\n    \n    \n      6\n      work\n      work\n      NN\n      1.049796e-19\n      63.046524\n    \n    \n      7\n      for\n      for\n      IN\n      1.616030e-21\n      69.068036\n    \n    \n      8\n      her\n      her\n      NN\n      1.736759e-24\n      78.929877"
  },
  {
    "objectID": "lessons/M04_NLP/M04_02_HMM.html#try-with-all-sentences",
    "href": "lessons/M04_NLP/M04_02_HMM.html#try-with-all-sentences",
    "title": "Predict POS with an HMM",
    "section": "Try with all sentences",
    "text": "Try with all sentences\n\nTEST = TEST_TOKENS.groupby('sent_id', group_keys=False).apply(lambda x: viterbi_eta(x, False))\n\n\nsample_sentence_id = TEST_SENTENCES.sample().index[0]\n\n\nsample_sentence_id\n\n10\n\n\n\nTEST.loc[sample_sentence_id]\n\n\n\n\n\n  \n    \n      \n      token_str\n      term_str\n      tag\n      prob\n      i\n    \n    \n      token_num\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      but\n      but\n      CC\n      1.912925e-02\n      5.708076\n    \n    \n      1\n      the\n      the\n      DT\n      1.168278e-03\n      9.741401\n    \n    \n      2\n      intercourse\n      intercourse\n      NN\n      1.323096e-07\n      22.849579\n    \n    \n      3\n      of\n      of\n      IN\n      8.300612e-09\n      26.844135\n    \n    \n      4\n      the\n      the\n      DT\n      1.595997e-09\n      29.222895\n    \n    \n      5\n      last\n      last\n      JJ\n      4.262058e-12\n      37.771587\n    \n    \n      6\n      seven\n      seven\n      CD\n      1.945256e-16\n      52.190889\n    \n    \n      7\n      years\n      years\n      NN\n      1.362521e-19\n      62.670355"
  },
  {
    "objectID": "lessons/M04_NLP/M04_02_HMM.html#what-pos-tags-are-hardest-to-predict",
    "href": "lessons/M04_NLP/M04_02_HMM.html#what-pos-tags-are-hardest-to-predict",
    "title": "Predict POS with an HMM",
    "section": "What POS tags are hardest to predict?",
    "text": "What POS tags are hardest to predict?\nFirst, create a dataframe for predict POS tags.\n\nTEST_POS = TEST.groupby('tag').i.mean().to_frame('mean_i')\n\n\nTEST_POS_IDX = TEST_POS.index\nTEST_POS = TEST_POS.merge(POS_GROUP, left_on='tag', right_on='pos_group').set_index(TEST_POS_IDX)\ndel(TEST_POS_IDX)\nTEST_POS['label'] = TEST_POS.index + ' /' + TEST_POS['def'].str.split(r'[,;]').str[0]\n\n\nTEST_POS\n\n\n\n\n\n  \n    \n      \n      mean_i\n      n\n      def\n      p\n      i\n      h\n      label\n    \n    \n      tag\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      CC\n      39.029791\n      83450\n      conjunction, coordinating\n      0.040530\n      4.624859\n      0.187446\n      CC / conjunction\n    \n    \n      CD\n      64.896161\n      12610\n      numeral, cardinal\n      0.006124\n      7.351202\n      0.045022\n      CD / numeral\n    \n    \n      DT\n      44.353681\n      204813\n      determiner\n      0.099474\n      3.329535\n      0.331203\n      DT / determiner\n    \n    \n      IN\n      60.731755\n      264122\n      preposition or conjunction, subordinating\n      0.128279\n      2.962638\n      0.380046\n      IN / preposition or conjunction\n    \n    \n      JJ\n      63.466369\n      165133\n      adjective or numeral, ordinal;  adjective, co...\n      0.080202\n      3.640214\n      0.291953\n      JJ / adjective or numeral\n    \n    \n      MD\n      54.594098\n      31876\n      modal auxiliary\n      0.015482\n      6.013300\n      0.093096\n      MD / modal auxiliary\n    \n    \n      NN\n      63.901830\n      540387\n      noun, common, singular or mass;  noun, proper...\n      0.262457\n      1.929850\n      0.506502\n      NN / noun\n    \n    \n      RB\n      55.894583\n      119059\n      adverb;  adverb, comparative;  adverb, superl...\n      0.057825\n      4.112166\n      0.237785\n      RB / adverb\n    \n    \n      TO\n      45.960521\n      55581\n      \"to\" as preposition or infinitive marker\n      0.026995\n      5.211179\n      0.140674\n      TO / \"to\" as preposition or infinitive marker\n    \n    \n      VB\n      52.839969\n      356879\n      verb, base form;  verb, past tense;  verb, pr...\n      0.173330\n      2.528407\n      0.438249\n      VB / verb\n    \n    \n      WP\n      18.723395\n      10772\n      WH-pronoun;  WH-pronoun, possessive\n      0.005232\n      7.578484\n      0.039649\n      WP / WH-pronoun\n    \n  \n\n\n\n\n\nTEST_POS.reset_index().set_index('label').mean_i.sort_values().plot.barh();"
  },
  {
    "objectID": "lessons/M04_NLP/M04_02_HMM.html#add-features-to-sentences",
    "href": "lessons/M04_NLP/M04_02_HMM.html#add-features-to-sentences",
    "title": "Predict POS with an HMM",
    "section": "Add features to sentences",
    "text": "Add features to sentences\n\nTEST['code'] = TEST.apply(lambda x: x.term_str + '/' + x.tag, axis=1) \nTEST_SENTENCES['code'] = TEST.groupby('sent_id').apply(lambda x: ' '.join(x.code))\nTEST_SENTENCES['i_mean'] = TEST.groupby('sent_id').i.mean().round(2)\nTEST_SENTENCES['pp'] = np.exp2(TEST_SENTENCES.i_mean)\n\n\nTEST_SENTENCES.sort_values('pp')\n\n\n\n\n\n  \n    \n      \n      sent_str\n      code\n      i_mean\n      pp\n    \n    \n      sent_id\n      \n      \n      \n      \n    \n  \n  \n    \n      23\n      and a Miss Taylor in the house\n      and/CC a/DT miss/NN taylor/NN in/IN the/DT hou...\n      27.62\n      2.062759e+08\n    \n    \n      10\n      but the intercourse of the last seven years\n      but/CC the/DT intercourse/NN of/IN the/DT last...\n      30.87\n      1.962438e+09\n    \n    \n      15\n      interested in all its concerns\n      interested/JJ in/IN all/DT its/NN concerns/NN\n      32.14\n      4.732646e+09\n    \n    \n      27\n      but he was no companion for her\n      but/CC he/NN was/VB no/DT companion/NN for/IN ...\n      32.69\n      6.929000e+09\n    \n    \n      30\n      and Mr Woodhouse had not married early\n      and/CC mr/NN woodhouse/NN had/VB not/RB marrie...\n      33.02\n      8.709846e+09\n    \n    \n      22\n      only half a mile from them\n      only/RB half/JJ a/DT mile/NN from/IN them/NN\n      34.80\n      2.991189e+10\n    \n    \n      33\n      without activity of mind or body\n      without/IN activity/NN of/IN mind/NN or/CC bod...\n      34.82\n      3.032944e+10\n    \n    \n      24\n      and with all her advantages natural and domestic\n      and/CC with/IN all/DT her/NN advantages/NN nat...\n      36.36\n      8.819636e+10\n    \n    \n      19\n      How was she to bear the change\n      how/NN was/VB she/NN to/TO bear/VB the/DT chan...\n      37.44\n      1.864503e+11\n    \n    \n      14\n      knowing all the ways of the family\n      knowing/VB all/DT the/DT ways/NN of/IN the/DT ...\n      37.52\n      1.970813e+11\n    \n    \n      26\n      She dearly loved her father\n      she/NN dearly/RB loved/VB her/NN father/NN\n      39.89\n      1.018794e+12\n    \n    \n      3\n      but it was a black morning's work for her\n      but/CC it/NN was/VB a/DT black/JJ mornings/NN ...\n      40.58\n      1.643604e+12\n    \n    \n      29\n      The evil of the actual disparity in their ages\n      the/DT evil/NN of/IN the/DT actual/JJ disparit...\n      41.13\n      2.406377e+12\n    \n    \n      9\n      A large debt of gratitude was owing here\n      a/DT large/JJ debt/NN of/IN gratitude/NN was/V...\n      42.05\n      4.553143e+12\n    \n    \n      32\n      for having been a valetudinarian all his life\n      for/IN having/VB been/VB a/DT valetudinarian/J...\n      43.55\n      1.287823e+13\n    \n    \n      31\n      was much increased by his constitution and habits\n      was/VB much/JJ increased/VB by/IN his/NN const...\n      45.61\n      5.370047e+13\n    \n    \n      1\n      The event had every promise of happiness for h...\n      the/DT event/NN had/VB every/DT promise/NN of/...\n      45.65\n      5.521020e+13\n    \n    \n      28\n      He could not meet her in conversation rational...\n      he/NN could/MD not/RB meet/VB her/NN in/IN con...\n      52.38\n      5.860723e+15\n    \n    \n      13\n      She had been a friend and companion such as fe...\n      she/NN had/VB been/VB a/DT friend/NN and/CC co...\n      52.66\n      7.116060e+15\n    \n    \n      34\n      he was a much older man in ways than in years\n      he/NN was/VB a/DT much/JJ older/JJ man/NN in/I...\n      53.01\n      9.069849e+15\n    \n    \n      25\n      she was now in great danger of suffering from ...\n      she/NN was/VB now/RB in/IN great/JJ danger/NN ...\n      54.34\n      2.280184e+16\n    \n    \n      37\n      Her sister though comparatively but little rem...\n      her/NN sister/NN though/IN comparatively/RB bu...\n      54.35\n      2.296044e+16\n    \n    \n      18\n      and who had such an affection for her as could...\n      and/CC who/WP had/VB such/JJ an/DT affection/N...\n      54.67\n      2.866222e+16\n    \n    \n      17\n      one to whom she could speak every thought as i...\n      one/CD to/TO whom/WP she/NN could/MD speak/VB ...\n      55.21\n      4.167408e+16\n    \n    \n      0\n      the quick brown fox jumped over the lazy dogs\n      the/DT quick/JJ brown/NN fox/NN jumped/VB over...\n      55.94\n      6.912226e+16\n    \n    \n      8\n      and how nursed her through the various illness...\n      and/CC how/NN nursed/VB her/NN through/IN the/...\n      56.31\n      8.933035e+16\n    \n    \n      36\n      his talents could not have recommended him at ...\n      his/NN talents/NN could/MD not/RB have/VB reco...\n      56.46\n      9.911823e+16\n    \n    \n      4\n      The want of Miss Taylor would be felt every ho...\n      the/DT want/NN of/IN miss/NN taylor/NN would/M...\n      58.35\n      3.673671e+17\n    \n    \n      40\n      before Christmas brought the next visit from I...\n      before/IN christmas/NN brought/VB the/DT next/...\n      59.56\n      8.498583e+17\n    \n    \n      16\n      and peculiarly interested in herself in every ...\n      and/CC peculiarly/RB interested/JJ in/IN herse...\n      60.13\n      1.261635e+18\n    \n    \n      21\n      but Emma was aware that great must be the diff...\n      but/CC emma/NN was/VB aware/JJ that/IN great/J...\n      63.66\n      1.457369e+19\n    \n    \n      11\n      the equal footing and perfect unreserve which ...\n      the/DT equal/JJ footing/NN and/CC perfect/JJ u...\n      66.53\n      1.065432e+20\n    \n    \n      20\n      It was true that her friend was going only hal...\n      it/NN was/VB true/JJ that/IN her/NN friend/NN ...\n      68.34\n      3.735854e+20\n    \n    \n      39\n      and many a long October and November evening m...\n      and/CC many/JJ a/DT long/JJ october/NN and/CC ...\n      69.25\n      7.019840e+20\n    \n    \n      12\n      on their being left to each other was yet a de...\n      on/IN their/NN being/VB left/VB to/TO each/DT ...\n      69.46\n      8.119766e+20\n    \n    \n      41\n      and their little children to fill the house an...\n      and/CC their/NN little/JJ children/NN to/TO fi...\n      69.56\n      8.702549e+20\n    \n    \n      35\n      and though everywhere beloved for the friendli...\n      and/CC though/IN everywhere/NN beloved/VB for/...\n      72.67\n      7.513630e+21\n    \n    \n      5\n      She recalled her past kindness the kindness th...\n      she/NN recalled/VB her/NN past/NN kindness/NN ...\n      73.80\n      1.644424e+22\n    \n    \n      2\n      Mr Weston was a man of unexceptionable charact...\n      mr/NN weston/NN was/VB a/DT man/NN of/IN unexc...\n      77.37\n      1.952948e+23\n    \n    \n      7\n      how she had devoted all her powers to attach a...\n      how/NN she/NN had/VB devoted/VB all/DT her/NN ...\n      80.76\n      2.047305e+24\n    \n    \n      38\n      being settled in London only sixteen miles off...\n      being/VB settled/VB in/IN london/NN only/RB si...\n      81.49\n      3.395739e+24\n    \n    \n      6\n      how she had taught and how she had played with...\n      how/NN she/NN had/VB taught/VB and/CC how/NN s...\n      85.73\n      6.416549e+25"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "",
    "text": "Overview\nIn this notebook, we explore Luhn’s concept of term significance in light of Zipf’s Law, TFIDF, and vector space models of text.\nWe will be looking for a significance function that will approximate the dotted line below.\nRecall Luhn’s (1958) representation of the problem:\nIn this notebook, we look at ways to approximate the significance curve using the ideas we learned in this module.\nRecall the formula Zipf’s law $f $, and \\(k = fr\\).\nWe explore our data to see if it matches the formula.\nWe can create a BOW with a split-apply-combine pattern that counts instances of words for each bag.\nWe create a document-term count matrix simply bu unstacking the BOW, which converts it from a narrow to a wide representation.\nNote, these operations are slower than using groupby().\nTry to pick the best aggregate statistic."
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#import",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#import",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Import",
    "text": "Import\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport plotly_express as px\n\n\nsns.set()"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#config",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#config",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Config",
    "text": "Config\nChange this to match the location of your data files.\n\nimport configparser\nconfig = configparser.ConfigParser()\nconfig.read(\"../env.ini\")\ndata_home = config['DEFAULT']['data_home'] \noutput_dir = config['DEFAULT']['output_dir']\ndata_prefix = 'austen-melville'"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#hyperparameters",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#hyperparameters",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Hyperparameters",
    "text": "Hyperparameters\nThese are some of the hyperparameters that will affect the computation of TFIDF.\n\ntf_method = 'sum'         # sum, max, log, double_norm, raw, binary\ntf_norm_k = .5            # only used for double_norm\nidf_method = 'standard'   # standard, max, smooth\ngradient_cmap = 'YlGnBu'  # YlGn, GnBu, YlGnBu; For tables; see https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html \n\nOHCO = ['book_id', 'chap_num', 'para_num', 'sent_num', 'token_num']\nSENTS = OHCO[:4]\nPARAS = OHCO[:3]\nCHAPS = OHCO[:2]\nBOOKS = OHCO[:1]\n\n\nbag = CHAPS"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#import-tables",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#import-tables",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Import tables",
    "text": "Import tables\n\nLIB = pd.read_csv(f\"{output_dir}/{data_prefix}-LIB.csv\").set_index(BOOKS)\nTOKEN = pd.read_csv(f'{output_dir}/{data_prefix}-TOKEN.csv').set_index(OHCO).dropna()\nVOCAB = pd.read_csv(f'{output_dir}/{data_prefix}-VOCAB.csv').set_index('term_str').dropna()\nPOS_GROUP = pd.read_csv(f'{output_dir}/{data_prefix}-POS_GROUP.csv').set_index('pos_group')\n\n\nPOS_GROUP\n\n\n\n\n\n  \n    \n      \n      n\n      def\n      p\n      i\n      h\n    \n    \n      pos_group\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      CC\n      83450\n      conjunction, coordinating\n      0.040530\n      4.624859\n      0.187446\n    \n    \n      CD\n      12610\n      numeral, cardinal\n      0.006124\n      7.351202\n      0.045022\n    \n    \n      DT\n      204813\n      determiner\n      0.099474\n      3.329535\n      0.331203\n    \n    \n      EX\n      3776\n      existential there\n      0.001834\n      9.090840\n      0.016672\n    \n    \n      FW\n      490\n      foreign word\n      0.000238\n      12.036845\n      0.002865\n    \n    \n      IN\n      264122\n      preposition or conjunction, subordinating\n      0.128279\n      2.962638\n      0.380046\n    \n    \n      JJ\n      165133\n      adjective or numeral, ordinal;  adjective, co...\n      0.080202\n      3.640214\n      0.291953\n    \n    \n      LS\n      3\n      list item marker\n      0.000001\n      19.388520\n      0.000028\n    \n    \n      MD\n      31876\n      modal auxiliary\n      0.015482\n      6.013300\n      0.093096\n    \n    \n      NN\n      540387\n      noun, common, singular or mass;  noun, proper...\n      0.262457\n      1.929850\n      0.506502\n    \n    \n      PD\n      4686\n      pre-determiner\n      0.002276\n      8.779342\n      0.019981\n    \n    \n      PO\n      332\n      genitive marker\n      0.000161\n      12.598444\n      0.002031\n    \n    \n      PR\n      176261\n      pronoun, personal;  pronoun, possessive\n      0.085607\n      3.546129\n      0.303573\n    \n    \n      RB\n      119059\n      adverb;  adverb, comparative;  adverb, superl...\n      0.057825\n      4.112166\n      0.237785\n    \n    \n      RP\n      7897\n      particle\n      0.003835\n      8.026394\n      0.030785\n    \n    \n      TO\n      55581\n      \"to\" as preposition or infinitive marker\n      0.026995\n      5.211179\n      0.140674\n    \n    \n      UH\n      74\n      interjection\n      0.000036\n      14.764030\n      0.000531\n    \n    \n      VB\n      356879\n      verb, base form;  verb, past tense;  verb, pr...\n      0.173330\n      2.528407\n      0.438249\n    \n    \n      WD\n      11000\n      WH-determiner\n      0.005343\n      7.548267\n      0.040327\n    \n    \n      WP\n      10772\n      WH-pronoun;  WH-pronoun, possessive\n      0.005232\n      7.578484\n      0.039649\n    \n    \n      WR\n      9757\n      Wh-adverb\n      0.004739\n      7.721261\n      0.036590\n    \n  \n\n\n\n\n\nLIB\n\n\n\n\n\n  \n    \n      \n      source_file_path\n      author\n      title\n      chap_regex\n      book_len\n      n_chaps\n    \n    \n      book_id\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      105\n      ../data/gutenberg/austen-melville-set/AUSTEN_J...\n      AUSTEN, JANE\n      PERSUASION\n      ^Chapter\\s+\\d+$\n      83624\n      24\n    \n    \n      121\n      ../data/gutenberg/austen-melville-set/AUSTEN_J...\n      AUSTEN, JANE\n      NORTHANGER ABBEY\n      ^CHAPTER\\s+\\d+$\n      77601\n      31\n    \n    \n      141\n      ../data/gutenberg/austen-melville-set/AUSTEN_J...\n      AUSTEN, JANE\n      MANSFIELD PARK\n      ^CHAPTER\\s+[IVXLCM]+$\n      160378\n      48\n    \n    \n      158\n      ../data/gutenberg/austen-melville-set/AUSTEN_J...\n      AUSTEN, JANE\n      EMMA\n      ^\\s*CHAPTER\\s+[IVXLCM]+\\s*$\n      160926\n      55\n    \n    \n      161\n      ../data/gutenberg/austen-melville-set/AUSTEN_J...\n      AUSTEN, JANE\n      SENSE AND SENSIBILITY\n      ^CHAPTER\\s+\\d+$\n      119873\n      50\n    \n    \n      946\n      ../data/gutenberg/austen-melville-set/AUSTEN_J...\n      AUSTEN, JANE\n      LADY SUSAN\n      ^\\s*[IVXLCM]+\\s*$\n      23116\n      41\n    \n    \n      1212\n      ../data/gutenberg/austen-melville-set/AUSTEN_J...\n      AUSTEN, JANE\n      LOVE AND FREINDSHIP SIC\n      ^\\s*LETTER .* to .*$\n      33265\n      24\n    \n    \n      1342\n      ../data/gutenberg/austen-melville-set/AUSTEN_J...\n      AUSTEN, JANE\n      PRIDE AND PREJUDICE\n      ^Chapter\\s+\\d+$\n      122126\n      61\n    \n    \n      1900\n      ../data/gutenberg/austen-melville-set/MELVILLE...\n      MELVILLE, HERMAN\n      TYPEE A ROMANCE OF THE SOUTH SEAS\n      ^CHAPTER\n      108021\n      34\n    \n    \n      2701\n      ../data/gutenberg/austen-melville-set/MELVILLE...\n      MELVILLE, HERMAN\n      MOBY DICK OR THE WHALE\n      ^(?:ETYMOLOGY|EXTRACTS|CHAPTER)\n      215504\n      138\n    \n    \n      4045\n      ../data/gutenberg/austen-melville-set/MELVILLE...\n      MELVILLE, HERMAN\n      OMOO ADVENTURES IN THE SOUTH SEAS\n      ^\\s*CHAPTER\\s+[IVXLCM]+\\.\\s*$\n      102352\n      82\n    \n    \n      8118\n      ../data/gutenberg/austen-melville-set/MELVILLE...\n      MELVILLE, HERMAN\n      REDBURN HIS FIRST VOYAGE BEING THE SAILOR BOY ...\n      ^\\s*[IVXLCM]+\\. .*$\n      119243\n      78\n    \n    \n      10712\n      ../data/gutenberg/austen-melville-set/MELVILLE...\n      MELVILLE, HERMAN\n      WHITE JACKET OR THE WORLD ON A MAN OF WAR\n      ^CHAPTER\\s+[IVXLCM]+\\.\\s*$\n      143310\n      92\n    \n    \n      13720\n      ../data/gutenberg/austen-melville-set/MELVILLE...\n      MELVILLE, HERMAN\n      MARDI AND A VOYAGE THITHER VOL I\n      ^\\s*CHAPTER\\s+[IVXLCM]+\\s*$\n      96878\n      104\n    \n    \n      13721\n      ../data/gutenberg/austen-melville-set/MELVILLE...\n      MELVILLE, HERMAN\n      MARDI AND A VOYAGE THITHER VOL II\n      ^\\s*CHAPTER\\s+[IVXLCM]+\\s*$\n      102092\n      91\n    \n    \n      15422\n      ../data/gutenberg/austen-melville-set/MELVILLE...\n      MELVILLE, HERMAN\n      ISRAEL POTTER HIS FIFTY YEARS OF EXILE\n      ^\\s*CHAPTER\\s+[IVXLCM]+\\.\n      65516\n      27\n    \n    \n      15859\n      ../data/gutenberg/austen-melville-set/MELVILLE...\n      MELVILLE, HERMAN\n      THE PIAZZA TALES\n      ^\\s*[A-Z,;-]+\\.\\s*$\n      75491\n      1\n    \n    \n      21816\n      ../data/gutenberg/austen-melville-set/MELVILLE...\n      MELVILLE, HERMAN\n      THE CONFIDENCE MAN HIS MASQUERADE\n      ^CHAPTER\\s+[IVXLCM]+\\.?$\n      95315\n      90\n    \n    \n      34970\n      ../data/gutenberg/austen-melville-set/MELVILLE...\n      MELVILLE, HERMAN\n      PIERRE OR THE AMBIGUITIES\n      ^\\s*[IVXLCM]+\\.\\s*$\n      155056\n      114\n    \n  \n\n\n\n\n\nVOCAB\n\n\n\n\n\n  \n    \n      \n      n\n      n_chars\n      p\n      i\n      max_pos\n      n_pos\n      cat_pos\n      stop\n      stem_porter\n      stem_snowball\n      stem_lancaster\n    \n    \n      term_str\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      2\n      1\n      9.713651e-07\n      19.973483\n      CD\n      1\n      {'CD'}\n      0\n      0\n      0\n      0\n    \n    \n      1\n      23\n      1\n      1.117070e-05\n      16.449921\n      CD\n      3\n      {'NN', 'CD', 'NNP'}\n      0\n      1\n      1\n      1\n    \n    \n      10\n      6\n      2\n      2.914095e-06\n      18.388520\n      CD\n      1\n      {'CD'}\n      0\n      10\n      10\n      10\n    \n    \n      100\n      2\n      3\n      9.713651e-07\n      19.973483\n      CD\n      1\n      {'CD'}\n      0\n      100\n      100\n      100\n    \n    \n      1000\n      2\n      4\n      9.713651e-07\n      19.973483\n      CD\n      1\n      {'CD'}\n      0\n      1000\n      1000\n      1000\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      æneas\n      1\n      5\n      4.856826e-07\n      20.973483\n      NN\n      1\n      {'NN'}\n      0\n      ænea\n      ænea\n      ænea\n    \n    \n      æniad\n      1\n      5\n      4.856826e-07\n      20.973483\n      NN\n      1\n      {'NN'}\n      0\n      æniad\n      æniad\n      æniad\n    \n    \n      æson\n      2\n      4\n      9.713651e-07\n      19.973483\n      NN\n      1\n      {'NN'}\n      0\n      æson\n      æson\n      æson\n    \n    \n      æsops\n      1\n      5\n      4.856826e-07\n      20.973483\n      NNS\n      1\n      {'NNS'}\n      0\n      æsop\n      æsop\n      æsop\n    \n    \n      ł20000\n      1\n      6\n      4.856826e-07\n      20.973483\n      NN\n      1\n      {'NN'}\n      0\n      ł20000\n      ł20000\n      ł20000\n    \n  \n\n40278 rows × 11 columns\n\n\n\n\nTOKEN\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      \n      pos_tuple\n      pos\n      token_str\n      term_str\n    \n    \n      book_id\n      chap_num\n      para_num\n      sent_num\n      token_num\n      \n      \n      \n      \n    \n  \n  \n    \n      158\n      1\n      1\n      0\n      0\n      ('Emma', 'NNP')\n      NNP\n      Emma\n      emma\n    \n    \n      1\n      ('Woodhouse,', 'NNP')\n      NNP\n      Woodhouse,\n      woodhouse\n    \n    \n      2\n      ('handsome,', 'NN')\n      NN\n      handsome,\n      handsome\n    \n    \n      3\n      ('clever,', 'NN')\n      NN\n      clever,\n      clever\n    \n    \n      4\n      ('and', 'CC')\n      CC\n      and\n      and\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      10712\n      92\n      23\n      0\n      7\n      ('a', 'DT')\n      DT\n      a\n      a\n    \n    \n      8\n      ('voyage', 'NN')\n      NN\n      voyage\n      voyage\n    \n    \n      9\n      (\"that's\", 'NN')\n      NN\n      that's\n      thats\n    \n    \n      10\n      ('homeward', 'SYM')\n      SYM\n      homeward\n      homeward\n    \n    \n      12\n      ('bound!', 'NN')\n      NN\n      bound!\n      bound\n    \n  \n\n2070242 rows × 4 columns"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#keep-only-pos-groups",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#keep-only-pos-groups",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Keep only POS groups",
    "text": "Keep only POS groups\n\nVOCAB['max_pos'] = VOCAB.max_pos.str[:2]\n\nLet’s take a quick peek at the distribution of terms (types) and tokens over POS categories.\n\nPOS_GROUP['n_terms'] = VOCAB.max_pos.value_counts() \nPOS_GROUP['n_tokens'] = VOCAB.groupby('max_pos').n.sum()\n\n\n# POS_GROUP.dropna().n_terms.sort_values().plot.bar(figsize=(15,5), rot=45, title=\"N Terms per POS Group\");\n\n\n# POS_GROUP.dropna().n_tokens.sort_values().plot.bar(figsize=(15,5), rot=45, title=\"N Tokens per POS Group\");\n\n\ndef plot_pos():\n    X = POS_GROUP[['n_terms','n_tokens', 'def']].dropna().sort_values('n_terms')\n    labels = X['def']\n    Z = ((X - X.mean(numeric_only=True)) / X.std(numeric_only=True)).iloc[:,1:]\n    return px.bar(Z, height=600, hover_name=labels, title=\"Comparison of Z-scores for POS token and term counts\")\n\n\nplot_pos()\n\n\n                                                \n\n\nThis provides some insight into the difference between open and closed categories."
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#add-term-rank-to-vocab",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#add-term-rank-to-vocab",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Add Term Rank to VOCAB",
    "text": "Add Term Rank to VOCAB\nFirst, we compute the rank of each word, which is just its numeric index when inversely sorted by frequency.\n\nVOCAB\n\n\n\n\n\n  \n    \n      \n      n\n      n_chars\n      p\n      i\n      max_pos\n      n_pos\n      cat_pos\n      stop\n      stem_porter\n      stem_snowball\n      stem_lancaster\n    \n    \n      term_str\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      2\n      1\n      9.713651e-07\n      19.973483\n      CD\n      1\n      {'CD'}\n      0\n      0\n      0\n      0\n    \n    \n      1\n      23\n      1\n      1.117070e-05\n      16.449921\n      CD\n      3\n      {'NN', 'CD', 'NNP'}\n      0\n      1\n      1\n      1\n    \n    \n      10\n      6\n      2\n      2.914095e-06\n      18.388520\n      CD\n      1\n      {'CD'}\n      0\n      10\n      10\n      10\n    \n    \n      100\n      2\n      3\n      9.713651e-07\n      19.973483\n      CD\n      1\n      {'CD'}\n      0\n      100\n      100\n      100\n    \n    \n      1000\n      2\n      4\n      9.713651e-07\n      19.973483\n      CD\n      1\n      {'CD'}\n      0\n      1000\n      1000\n      1000\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      æneas\n      1\n      5\n      4.856826e-07\n      20.973483\n      NN\n      1\n      {'NN'}\n      0\n      ænea\n      ænea\n      ænea\n    \n    \n      æniad\n      1\n      5\n      4.856826e-07\n      20.973483\n      NN\n      1\n      {'NN'}\n      0\n      æniad\n      æniad\n      æniad\n    \n    \n      æson\n      2\n      4\n      9.713651e-07\n      19.973483\n      NN\n      1\n      {'NN'}\n      0\n      æson\n      æson\n      æson\n    \n    \n      æsops\n      1\n      5\n      4.856826e-07\n      20.973483\n      NN\n      1\n      {'NNS'}\n      0\n      æsop\n      æsop\n      æsop\n    \n    \n      ł20000\n      1\n      6\n      4.856826e-07\n      20.973483\n      NN\n      1\n      {'NN'}\n      0\n      ł20000\n      ł20000\n      ł20000\n    \n  \n\n40278 rows × 11 columns\n\n\n\n\nif 'term_rank' not in VOCAB.columns:\n    VOCAB = VOCAB.sort_values('n', ascending=False).reset_index()\n    VOCAB.index.name = 'term_rank' \n    VOCAB = VOCAB.reset_index()\n    VOCAB = VOCAB.set_index('term_str')\n    VOCAB['term_rank'] = VOCAB['term_rank'] + 1\n\n\nVOCAB.head()\n\n\n\n\n\n  \n    \n      \n      term_rank\n      n\n      n_chars\n      p\n      i\n      max_pos\n      n_pos\n      cat_pos\n      stop\n      stem_porter\n      stem_snowball\n      stem_lancaster\n    \n    \n      term_str\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      the\n      1\n      109921\n      3\n      0.053387\n      4.227375\n      DT\n      17\n      {'NN', 'IN', 'POS', 'VBD', 'VB', 'VBP', 'PRP',...\n      1\n      the\n      the\n      the\n    \n    \n      of\n      2\n      65525\n      2\n      0.031824\n      4.973725\n      IN\n      15\n      {'NN', 'IN', 'VBD', 'VB', 'VBP', 'PRP', 'VBN',...\n      1\n      of\n      of\n      of\n    \n    \n      and\n      3\n      62954\n      3\n      0.030576\n      5.031473\n      CC\n      17\n      {'NN', 'IN', 'POS', 'VBZ', 'VBD', 'VB', 'VBP',...\n      1\n      and\n      and\n      and\n    \n    \n      to\n      4\n      56271\n      2\n      0.027330\n      5.193379\n      TO\n      20\n      {'IN', 'POS', 'VB', 'PRP', 'NNS', 'NNP', 'NN',...\n      1\n      to\n      to\n      to\n    \n    \n      a\n      5\n      44174\n      1\n      0.021455\n      5.542573\n      DT\n      12\n      {'NN', 'IN', 'POS', 'VB', 'VBP', 'CD', 'NNP', ...\n      1\n      a\n      a\n      a"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#create-alternate-rank",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#create-alternate-rank",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Create Alternate Rank",
    "text": "Create Alternate Rank\nRaw term rank has the problem of arbitrarily assigning rank numbers to terms that have the same count – essentially terms with the same count are assigned a value based on their alphanumeric sort order. So we come up with a different rank that is a function of term count only.\n\nnew_rank = VOCAB.n.value_counts()\\\n    .sort_index(ascending=False).reset_index().reset_index()\\\n    .rename(columns={'index':'term_rank2', 'count':'nn'})\\\n    .set_index('n')\n\n\nVOCAB['term_rank2'] = VOCAB.n.map(new_rank.term_rank2) + 1\n\n\nVOCAB.head()\n\n\n\n\n\n  \n    \n      \n      term_rank\n      n\n      n_chars\n      p\n      i\n      max_pos\n      n_pos\n      cat_pos\n      stop\n      stem_porter\n      stem_snowball\n      stem_lancaster\n      term_rank2\n    \n    \n      term_str\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      the\n      1\n      109921\n      3\n      0.053387\n      4.227375\n      DT\n      17\n      {'NN', 'IN', 'POS', 'VBD', 'VB', 'VBP', 'PRP',...\n      1\n      the\n      the\n      the\n      1\n    \n    \n      of\n      2\n      65525\n      2\n      0.031824\n      4.973725\n      IN\n      15\n      {'NN', 'IN', 'VBD', 'VB', 'VBP', 'PRP', 'VBN',...\n      1\n      of\n      of\n      of\n      2\n    \n    \n      and\n      3\n      62954\n      3\n      0.030576\n      5.031473\n      CC\n      17\n      {'NN', 'IN', 'POS', 'VBZ', 'VBD', 'VB', 'VBP',...\n      1\n      and\n      and\n      and\n      3\n    \n    \n      to\n      4\n      56271\n      2\n      0.027330\n      5.193379\n      TO\n      20\n      {'IN', 'POS', 'VB', 'PRP', 'NNS', 'NNP', 'NN',...\n      1\n      to\n      to\n      to\n      4\n    \n    \n      a\n      5\n      44174\n      1\n      0.021455\n      5.542573\n      DT\n      12\n      {'NN', 'IN', 'POS', 'VB', 'VBP', 'CD', 'NNP', ...\n      1\n      a\n      a\n      a\n      5\n    \n  \n\n\n\n\nNote how the two ranks are related: the alternate rank groups more terms as frequency gets smaller.\n\nVOCAB.plot.scatter('term_rank2', 'term_rank');"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#compute-zipfs-k",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#compute-zipfs-k",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Compute Zipf’s K",
    "text": "Compute Zipf’s K\nNow we see if K is linear as it’s supposed to be.\n\nVOCAB['zipf_k'] = VOCAB.n * VOCAB.term_rank\nVOCAB['zipf_k2'] = VOCAB.p * VOCAB.term_rank2"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#visualize",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#visualize",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Visualize",
    "text": "Visualize\n\nRank and N\n\nVOCAB.plot.scatter('term_rank', 'n', figsize=(10,5), title=\"r vs n\");\n\n\n\n\n\nVOCAB.plot.scatter('term_rank2', 'n', figsize=(10,5), title=\"r2 vs n\");\n\n\n\n\n\nVOCAB.plot.scatter('term_rank', 'n', figsize=(10,5), logx=True, logy=True, title=\"Log r vs Log n\");\n\n\n\n\n\nVOCAB.plot.scatter('term_rank2', 'n', figsize=(10,5), logx=True, logy=True, title=\"Log r2 vs Log N\");\n\n\n\n\n\nVOCAB.plot.scatter('term_rank2', 'i', figsize=(10,5), logx=False, logy=False, title=\"r2 vs i\");\n\n\n\n\n\nVOCAB.plot.scatter('i', 'term_rank2', figsize=(10,5), logx=False, logy=False, title=\"r2 vs i\");\n\n\n\n\nIs \\(i\\) a normalized variant of \\(r\\)?\n\n\nRank and Zipf \\(k\\)\n\\(k\\) is supposed to be constant.\n\n# VOCAB.plot.scatter('term_rank', 'zipf_k', figsize=(10,5));\n\n\n# VOCAB.plot.scatter('term_rank2', 'zipf_k2',  figsize=(10,5));\n\nLook at low \\(k\\) terms.\n\nprint(VOCAB[VOCAB.zipf_k2 <= VOCAB.zipf_k2.quantile(.1)].sort_values('zipf_k', ascending=True).head(10).index.to_list())\n\n['wily', 'coordinates', 'teeter', 'contriver', 'clysters', 'coping', 'copernicus', 'windrows', 'tinkerings', 'teetered']\n\n\nLook at high \\(k\\) terms.\n\nprint(VOCAB[VOCAB.zipf_k2 >= VOCAB.zipf_k2.quantile(.9)].sort_values('zipf_k', ascending=False).head(10).index.to_list())\n\n['is', 'him', 'from', 'at', 'by', 'my', 'this', 'but', 'all', 'be']\n\n\n\n\nDemo Rank Index\nWe look at a sample drawn from rank increments to see global patterns in the corpus.\n\nrank_index = [1, 2, 3, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000]\n\n\ndemo = VOCAB.loc[VOCAB.term_rank.isin(rank_index)][['max_pos', 'p', 'term_rank', 'term_rank2', 'i', 'zipf_k', 'zipf_k2']]\n\n\ndemo.style.background_gradient(cmap=gradient_cmap, high=.5)\n\n\n\n\n  \n    \n       \n      max_pos\n      p\n      term_rank\n      term_rank2\n      i\n      zipf_k\n      zipf_k2\n    \n    \n      term_str\n       \n       \n       \n       \n       \n       \n       \n    \n  \n  \n    \n      the\n      DT\n      0.053387\n      1\n      1\n      4.227375\n      109921\n      0.053387\n    \n    \n      of\n      IN\n      0.031824\n      2\n      2\n      4.973725\n      131050\n      0.063649\n    \n    \n      and\n      CC\n      0.030576\n      3\n      3\n      5.031473\n      188862\n      0.091727\n    \n    \n      it\n      PR\n      0.011316\n      10\n      10\n      6.465503\n      232990\n      0.113159\n    \n    \n      you\n      PR\n      0.006968\n      20\n      20\n      7.165021\n      286940\n      0.139362\n    \n    \n      so\n      RB\n      0.004730\n      30\n      30\n      7.724073\n      292140\n      0.141887\n    \n    \n      their\n      PR\n      0.003438\n      40\n      40\n      8.184357\n      283120\n      0.137506\n    \n    \n      are\n      VB\n      0.002770\n      50\n      50\n      8.495725\n      285200\n      0.138517\n    \n    \n      will\n      MD\n      0.002192\n      60\n      60\n      8.833292\n      270840\n      0.131542\n    \n    \n      then\n      RB\n      0.001874\n      70\n      70\n      9.059846\n      270060\n      0.131163\n    \n    \n      its\n      PR\n      0.001530\n      80\n      80\n      9.351889\n      252080\n      0.122431\n    \n    \n      after\n      IN\n      0.001423\n      90\n      90\n      9.457290\n      263610\n      0.128031\n    \n    \n      mrs\n      NN\n      0.001291\n      100\n      100\n      9.597358\n      265800\n      0.129094\n    \n    \n      night\n      NN\n      0.000539\n      200\n      197\n      10.857139\n      222000\n      0.106204\n    \n    \n      kind\n      NN\n      0.000337\n      300\n      279\n      11.534691\n      208200\n      0.094041\n    \n    \n      god\n      NN\n      0.000240\n      400\n      356\n      12.025116\n      197600\n      0.085414\n    \n    \n      fire\n      NN\n      0.000191\n      500\n      420\n      12.355097\n      196500\n      0.080167\n    \n    \n      heaven\n      NN\n      0.000157\n      600\n      475\n      12.633633\n      194400\n      0.074747\n    \n    \n      countenance\n      NN\n      0.000135\n      700\n      514\n      12.854542\n      194600\n      0.069400\n    \n    \n      seldom\n      RB\n      0.000118\n      800\n      546\n      13.054620\n      193600\n      0.064174\n    \n    \n      superior\n      JJ\n      0.000104\n      900\n      573\n      13.225290\n      193500\n      0.059834\n    \n    \n      hearts\n      NN\n      0.000095\n      1000\n      592\n      13.366153\n      195000\n      0.056067\n    \n    \n      seeking\n      VB\n      0.000046\n      2000\n      692\n      14.403627\n      190000\n      0.031929\n    \n    \n      distinction\n      NN\n      0.000029\n      3000\n      728\n      15.090840\n      177000\n      0.020861\n    \n    \n      millthorpe\n      NN\n      0.000020\n      4000\n      745\n      15.581166\n      168000\n      0.015197\n    \n    \n      maker\n      NN\n      0.000016\n      5000\n      755\n      15.973483\n      160000\n      0.011734\n    \n    \n      resemble\n      VB\n      0.000012\n      6000\n      762\n      16.329627\n      150000\n      0.009252\n    \n    \n      infamous\n      JJ\n      0.000010\n      7000\n      767\n      16.651555\n      140000\n      0.007450\n    \n    \n      homes\n      NN\n      0.000008\n      8000\n      771\n      16.973483\n      128000\n      0.005991"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#rank-as-information",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#rank-as-information",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Rank as Information",
    "text": "Rank as Information\nIf we replace frequency \\(n\\) with relative frequency as an estimate of probability \\(p\\), it is easy to see that rank and information are closely related.\nInformation \\(i\\) is just the log normalized value of rank, expressed as inverse probability, or surprise \\(s\\).\nWe see also that \\(i\\) provides a smoother distribution.\n\nVOCAB['term_rank2_log2'] = np.log2(VOCAB.term_rank2)\nVOCAB['term_rank_log2'] = np.log2(VOCAB.term_rank)\n\n\nVOCAB.plot.scatter('term_rank2_log2', 'n', title=\"Log of Rank vs Frequency\");\n\n\n\n\n\nVOCAB.plot.scatter('i','p', title=\"Information vs Relative Frequency\");"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#observations",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#observations",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Observations",
    "text": "Observations\n\nZipf’s law sheds light on a basic structure of language – the division between words that do grammatical work and those that do semantic work.\nZipf’s constant is not borne out by the data.\nZipf’s rank order does not group words with the same count; so, we’ve created an alternate rank based on word count groupings.\nInformation \\(i\\) may be used as a replacement for rank when exploring corpus level significance trends."
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#pause-and-explore-doc-table",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#pause-and-explore-doc-table",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Pause and explore DOC table",
    "text": "Pause and explore DOC table\nDOC is a table of bags.\n\nDOC = DTCM.sum(1).to_frame('n_tokens')\nDOC['n_types'] = DTCM.astype('bool').sum(1)\n\n\nDOC.sort_values('n_tokens').plot(logy=True, figsize=(10, 5), rot=45);\n\n\n\n\n\n(DOC.n_types / DOC.n_tokens).sort_values().plot(title=\"Type / Token Ration by Bag\");"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#compute-tf",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#compute-tf",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Compute TF",
    "text": "Compute TF\n\n# We could do all this directly in the BOW table\n# BOW.groupby(['book_id','chap_num']).apply(lambda x: x.n / x.n.sum())\n\n\nprint('TF method:', tf_method)\nif tf_method == 'sum':\n    TF = DTCM.T / DTCM.T.sum()\nelif tf_method == 'max':\n    TF = DTCM.T / DTCM.T.max()\nelif tf_method == 'log':\n    TF = np.log2(1 + DTCM.T)\nelif tf_method == 'raw':\n    TF = DTCM.T\nelif tf_method == 'double_norm':\n    TF = DTCM.T / DTCM.T.max()\nelif tf_method == 'binary':\n    TF = DTCM.T.astype('bool').astype('int')\nTF = TF.T\n\nTF method: sum\n\n\n\nTF.head()\n\n\n\n\n\n  \n    \n      \n      term_str\n      0\n      1\n      10\n      100\n      1000\n      10000\n      1000000\n      10000000\n      10440\n      10800\n      ...\n      zoroaster\n      zozo\n      zuma\n      zur\n      à\n      æneas\n      æniad\n      æson\n      æsops\n      ł20000\n    \n    \n      book_id\n      chap_num\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      105\n      1\n      0.0\n      0.000762\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      2\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      3\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      4\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      5\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n5 rows × 40478 columns"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#compute-df",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#compute-df",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Compute DF",
    "text": "Compute DF\n\nDF = DTCM.astype('bool').sum()\n\n\nDF\n\nterm_str\n0          1\n1         11\n10         5\n100        2\n1000       2\n          ..\næneas      1\næniad      1\næson       1\næsops      1\nł20000     1\nLength: 40478, dtype: int64"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#compute-idf",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#compute-idf",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Compute IDF",
    "text": "Compute IDF\n\nN = DTCM.shape[0]\n\n\nprint('IDF method:', idf_method)\nif idf_method == 'standard':\n    IDF = np.log2(N / DF)\nelif idf_method == 'max':\n    IDF = np.log2(DF.max() / DF) \nelif idf_method == 'smooth':\n    IDF = np.log2((1 + N) / (1 + DF)) + 1\n\nIDF method: standard\n\n\n\nIDF\n\nterm_str\n0         10.131857\n1          6.672425\n10         7.809929\n100        9.131857\n1000       9.131857\n            ...    \næneas     10.131857\næniad     10.131857\næson      10.131857\næsops     10.131857\nł20000    10.131857\nLength: 40478, dtype: float64"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#compute-tfidf-1",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#compute-tfidf-1",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Compute TFIDF",
    "text": "Compute TFIDF\n\nTFIDF = TF * IDF\n\n\nTFIDF.head()\n\n\n\n\n\n  \n    \n      \n      term_str\n      0\n      1\n      10\n      100\n      1000\n      10000\n      1000000\n      10000000\n      10440\n      10800\n      ...\n      zoroaster\n      zozo\n      zuma\n      zur\n      à\n      æneas\n      æniad\n      æson\n      æsops\n      ł20000\n    \n    \n      book_id\n      chap_num\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      105\n      1\n      0.0\n      0.005084\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      2\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      3\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      4\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      5\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n5 rows × 40478 columns"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#move-things-to-their-places",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#move-things-to-their-places",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Move things to their places",
    "text": "Move things to their places\n\nVOCAB['df'] = DF\nVOCAB['idf'] = IDF\n\n\nBOW['tf'] = TF.stack()\nBOW['tfidf'] = TFIDF.stack()"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#apply-aggregates-to-vocab",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#apply-aggregates-to-vocab",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Apply aggregates to VOCAB",
    "text": "Apply aggregates to VOCAB\n\nVOCAB['tfidf_mean'] = TFIDF.mean() \nVOCAB['tfidf_sum'] = TFIDF.sum()\nVOCAB['tfidf_median'] = TFIDF.median()\nVOCAB['tfidf_max'] = TFIDF.max()\n\n\nVOCAB.tfidf_mean\n\nterm_str\nthe            0.000000\nof             0.000000\nand            0.000079\nto             0.000034\na              0.000055\n                 ...   \nglib           0.000007\nglentinebay    0.000006\nglenford       0.000008\nglendin        0.000003\nł20000         0.000002\nName: tfidf_mean, Length: 40278, dtype: float64"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#add-dfidf-and-dh",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#add-dfidf-and-dh",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Add DFIDF and DH",
    "text": "Add DFIDF and DH\n\nVOCAB['dfidf'] = VOCAB.df * VOCAB.idf\n\n\nVOCAB['dp'] = VOCAB.df / len(DOC)\n\n\nVOCAB['dh'] = VOCAB.dp * np.log2(1/VOCAB.dp)"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#observe-results",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#observe-results",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Observe results",
    "text": "Observe results\n\nVOCAB.sample(50).sort_values('i', ascending=True)[['max_pos', 'i','tfidf_mean','tfidf_median','tfidf_max', 'dfidf', 'dh']].style.background_gradient(cmap=gradient_cmap)\n\n\n\n\n  \n    \n       \n      max_pos\n      i\n      tfidf_mean\n      tfidf_median\n      tfidf_max\n      dfidf\n      dh\n    \n    \n      term_str\n       \n       \n       \n       \n       \n       \n       \n    \n  \n  \n    \n      doctor\n      NN\n      12.434324\n      0.000704\n      0.000000\n      0.027225\n      368.554697\n      0.328480\n    \n    \n      authority\n      NN\n      14.019287\n      0.000208\n      0.000000\n      0.024909\n      338.390128\n      0.301595\n    \n    \n      boots\n      NN\n      14.481630\n      0.000190\n      0.000000\n      0.017584\n      221.340209\n      0.197273\n    \n    \n      confined\n      VB\n      14.651555\n      0.000142\n      0.000000\n      0.012253\n      282.729799\n      0.251987\n    \n    \n      sin\n      NN\n      14.907394\n      0.000173\n      0.000000\n      0.024918\n      185.589319\n      0.165409\n    \n    \n      gallery\n      NN\n      15.301058\n      0.000129\n      0.000000\n      0.022938\n      137.200019\n      0.122282\n    \n    \n      compliance\n      NN\n      16.115502\n      0.000088\n      0.000000\n      0.015244\n      137.200019\n      0.122282\n    \n    \n      endeavored\n      VB\n      16.329627\n      0.000088\n      0.000000\n      0.009748\n      137.200019\n      0.122282\n    \n    \n      extracted\n      VB\n      16.514051\n      0.000052\n      0.000000\n      0.010554\n      98.109711\n      0.087442\n    \n    \n      amendment\n      NN\n      16.725555\n      0.000039\n      0.000000\n      0.007105\n      102.754700\n      0.091582\n    \n    \n      dwarf\n      NN\n      16.803558\n      0.000090\n      0.000000\n      0.017477\n      83.608424\n      0.074517\n    \n    \n      madman\n      NN\n      16.803558\n      0.000061\n      0.000000\n      0.015759\n      93.374495\n      0.083221\n    \n    \n      yawning\n      VB\n      16.973483\n      0.000035\n      0.000000\n      0.007389\n      93.374495\n      0.083221\n    \n    \n      professing\n      VB\n      17.273043\n      0.000030\n      0.000000\n      0.005815\n      83.608424\n      0.074517\n    \n    \n      discomposed\n      VB\n      17.651555\n      0.000018\n      0.000000\n      0.003994\n      68.099289\n      0.060695\n    \n    \n      matron\n      NN\n      17.651555\n      0.000057\n      0.000000\n      0.013406\n      57.054856\n      0.050851\n    \n    \n      transactions\n      NN\n      17.651555\n      0.000014\n      0.000000\n      0.002386\n      68.099289\n      0.060695\n    \n    \n      tons\n      NN\n      17.803558\n      0.000047\n      0.000000\n      0.023755\n      51.271514\n      0.045697\n    \n    \n      unthought\n      JJ\n      18.166128\n      0.000015\n      0.000000\n      0.003463\n      51.271514\n      0.045697\n    \n    \n      beamed\n      VB\n      18.166128\n      0.000025\n      0.000000\n      0.008586\n      45.281367\n      0.040358\n    \n    \n      chronicled\n      VB\n      18.388520\n      0.000032\n      0.000000\n      0.010812\n      45.281367\n      0.040358\n    \n    \n      acclivity\n      NN\n      18.388520\n      0.000026\n      0.000000\n      0.008161\n      39.049644\n      0.034804\n    \n    \n      hamlets\n      NN\n      18.388520\n      0.000031\n      0.000000\n      0.010757\n      39.049644\n      0.034804\n    \n    \n      gaming\n      NN\n      18.651555\n      0.000017\n      0.000000\n      0.007313\n      39.049644\n      0.034804\n    \n    \n      spencer\n      NN\n      18.651555\n      0.000010\n      0.000000\n      0.002783\n      39.049644\n      0.034804\n    \n    \n      michigan\n      NN\n      18.973483\n      0.000013\n      0.000000\n      0.005067\n      32.527428\n      0.028991\n    \n    \n      interspersed\n      VB\n      19.388520\n      0.000008\n      0.000000\n      0.004727\n      25.640683\n      0.022853\n    \n    \n      disembarked\n      VB\n      19.388520\n      0.000009\n      0.000000\n      0.005699\n      32.527428\n      0.028991\n    \n    \n      duxbury\n      NN\n      19.973483\n      nan\n      nan\n      nan\n      nan\n      nan\n    \n    \n      evasions\n      NN\n      19.973483\n      0.000004\n      0.000000\n      0.003112\n      18.263714\n      0.016278\n    \n    \n      rivalries\n      NN\n      19.973483\n      0.000005\n      0.000000\n      0.002825\n      18.263714\n      0.016278\n    \n    \n      softener\n      NN\n      19.973483\n      0.000010\n      0.000000\n      0.007134\n      18.263714\n      0.016278\n    \n    \n      sickens\n      VB\n      19.973483\n      0.000009\n      0.000000\n      0.006666\n      18.263714\n      0.016278\n    \n    \n      fireboard\n      NN\n      19.973483\n      0.000007\n      0.000000\n      0.005846\n      18.263714\n      0.016278\n    \n    \n      scorbutic\n      NN\n      19.973483\n      0.000008\n      0.000000\n      0.005153\n      18.263714\n      0.016278\n    \n    \n      spinozaist\n      NN\n      19.973483\n      0.000020\n      0.000000\n      0.022490\n      10.131857\n      0.009030\n    \n    \n      flavoured\n      VB\n      19.973483\n      0.000011\n      0.000000\n      0.009532\n      18.263714\n      0.016278\n    \n    \n      inadvertently\n      RB\n      19.973483\n      0.000004\n      0.000000\n      0.004125\n      18.263714\n      0.016278\n    \n    \n      toady\n      NN\n      20.973483\n      0.000006\n      0.000000\n      0.006588\n      10.131857\n      0.009030\n    \n    \n      unsoftened\n      VB\n      20.973483\n      0.000005\n      0.000000\n      0.005613\n      10.131857\n      0.009030\n    \n    \n      affix\n      VB\n      20.973483\n      0.000002\n      0.000000\n      0.002121\n      10.131857\n      0.009030\n    \n    \n      ascetic\n      JJ\n      20.973483\n      0.000003\n      0.000000\n      0.003106\n      10.131857\n      0.009030\n    \n    \n      portioning\n      VB\n      20.973483\n      0.000003\n      0.000000\n      0.003092\n      10.131857\n      0.009030\n    \n    \n      wynnas\n      NN\n      20.973483\n      0.000013\n      0.000000\n      0.014270\n      10.131857\n      0.009030\n    \n    \n      remembrancer\n      NN\n      20.973483\n      0.000003\n      0.000000\n      0.003267\n      10.131857\n      0.009030\n    \n    \n      unhurt\n      RP\n      20.973483\n      0.000003\n      0.000000\n      0.003158\n      10.131857\n      0.009030\n    \n    \n      battalionings\n      NN\n      20.973483\n      0.000007\n      0.000000\n      0.007687\n      10.131857\n      0.009030\n    \n    \n      leadin\n      NN\n      20.973483\n      0.000008\n      0.000000\n      0.009261\n      10.131857\n      0.009030\n    \n    \n      nappishness\n      JJ\n      20.973483\n      0.000012\n      0.000000\n      0.013879\n      10.131857\n      0.009030\n    \n    \n      exceptionable\n      JJ\n      20.973483\n      0.000009\n      0.000000\n      0.010173\n      10.131857\n      0.009030"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#information-and-tfidf-mean",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#information-and-tfidf-mean",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Information and TFIDF Mean",
    "text": "Information and TFIDF Mean\n\npx.scatter(VOCAB.reset_index(), x='i', y='tfidf_mean', hover_name='term_str', hover_data=['n'], color='max_pos', \n           log_x=False, log_y=False)"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#information-and-tfidf-median",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#information-and-tfidf-median",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Information and TFIDF Median",
    "text": "Information and TFIDF Median\n\npx.scatter(VOCAB.reset_index(), x='i', y='tfidf_median', hover_name='term_str', hover_data=['n'], color='max_pos', \n           log_x=False, log_y=False)"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#information-and-tfidf-max",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#information-and-tfidf-max",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Information and TFIDF Max",
    "text": "Information and TFIDF Max\n\npx.scatter(VOCAB.reset_index(), x='i', y='tfidf_max', hover_name='term_str', hover_data=['n'], color='max_pos', \n           log_x=False, log_y=False)"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#information-and-tfidf-sum",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#information-and-tfidf-sum",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Information and TFIDF Sum",
    "text": "Information and TFIDF Sum\n\npx.scatter(VOCAB.reset_index(), x='i', y='tfidf_sum', hover_name='term_str', hover_data=['n'], color='max_pos')"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#information-and-dfidf-dh",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#information-and-dfidf-dh",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Information and DFIDF / DH",
    "text": "Information and DFIDF / DH\n\npx.scatter(VOCAB.reset_index(), x='i', y='dh', hover_name='term_str', hover_data=['n'], color='max_pos', height=800)"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#demo-table",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#demo-table",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Demo Table",
    "text": "Demo Table\n\ndemo2 = VOCAB.loc[VOCAB.term_rank.isin(rank_index), ['max_pos', 'i', 'tfidf_mean', 'tfidf_median', 'tfidf_sum', 'tfidf_max', 'dfidf', 'dh']]\n\n\ndemo2.style.background_gradient(cmap=gradient_cmap)\n\n\n\n\n  \n    \n       \n      max_pos\n      i\n      tfidf_mean\n      tfidf_median\n      tfidf_sum\n      tfidf_max\n      dfidf\n      dh\n    \n    \n      term_str\n       \n       \n       \n       \n       \n       \n       \n       \n    \n  \n  \n    \n      the\n      DT\n      4.227375\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      of\n      IN\n      4.973725\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      and\n      CC\n      5.031473\n      0.000079\n      0.000078\n      0.088788\n      0.000186\n      2.882817\n      0.002569\n    \n    \n      it\n      PR\n      6.465503\n      0.000208\n      0.000199\n      0.233208\n      0.000801\n      21.495121\n      0.019158\n    \n    \n      you\n      PR\n      7.165021\n      0.002240\n      0.001196\n      2.513699\n      0.022266\n      310.618692\n      0.276844\n    \n    \n      so\n      RB\n      7.724073\n      0.000293\n      0.000275\n      0.329229\n      0.001022\n      67.746505\n      0.060380\n    \n    \n      their\n      PR\n      8.184357\n      0.000777\n      0.000575\n      0.871750\n      0.006124\n      212.307769\n      0.189223\n    \n    \n      are\n      VB\n      8.495725\n      0.000683\n      0.000548\n      0.766291\n      0.004542\n      228.069595\n      0.203271\n    \n    \n      will\n      MD\n      8.833292\n      0.000935\n      0.000659\n      1.049116\n      0.009406\n      356.913125\n      0.318104\n    \n    \n      then\n      RB\n      9.059846\n      0.000547\n      0.000436\n      0.613695\n      0.005745\n      252.987300\n      0.225479\n    \n    \n      its\n      PR\n      9.351889\n      0.000629\n      0.000450\n      0.705280\n      0.007255\n      323.566824\n      0.288384\n    \n    \n      after\n      IN\n      9.457290\n      0.000517\n      0.000423\n      0.579540\n      0.003470\n      326.766502\n      0.291236\n    \n    \n      mrs\n      NN\n      9.597358\n      0.001757\n      0.000000\n      1.971870\n      0.021432\n      585.356554\n      0.521708\n    \n    \n      night\n      NN\n      10.857139\n      0.000745\n      0.000000\n      0.836399\n      0.014576\n      585.657321\n      0.521976\n    \n    \n      kind\n      NN\n      11.534691\n      0.000445\n      0.000000\n      0.499156\n      0.010708\n      594.863226\n      0.530181\n    \n    \n      god\n      NN\n      12.025116\n      0.000595\n      0.000000\n      0.667682\n      0.025793\n      547.797606\n      0.488233\n    \n    \n      fire\n      NN\n      12.355097\n      0.000512\n      0.000000\n      0.574262\n      0.024237\n      516.198308\n      0.460070\n    \n    \n      heaven\n      NN\n      12.633633\n      0.000433\n      0.000000\n      0.486369\n      0.012071\n      522.439228\n      0.465632\n    \n    \n      countenance\n      NN\n      12.854542\n      0.000283\n      0.000000\n      0.317535\n      0.008813\n      489.003736\n      0.435832\n    \n    \n      seldom\n      RB\n      13.054620\n      0.000312\n      0.000000\n      0.350161\n      0.010058\n      483.387988\n      0.430827\n    \n    \n      superior\n      JJ\n      13.225290\n      0.000280\n      0.000000\n      0.314010\n      0.011082\n      454.986013\n      0.405513\n    \n    \n      hearts\n      NN\n      13.366153\n      0.000337\n      0.000000\n      0.378017\n      0.018917\n      446.836042\n      0.398250\n    \n    \n      seeking\n      VB\n      14.403627\n      0.000187\n      0.000000\n      0.209335\n      0.020134\n      329.789671\n      0.293930\n    \n    \n      distinction\n      NN\n      15.090840\n      0.000123\n      0.000000\n      0.137765\n      0.010888\n      230.433697\n      0.205378\n    \n    \n      millthorpe\n      NN\n      15.581166\n      0.000139\n      0.000000\n      0.156272\n      0.041048\n      51.271514\n      0.045697\n    \n    \n      maker\n      NN\n      15.973483\n      0.000077\n      0.000000\n      0.086578\n      0.010017\n      111.794659\n      0.099639\n    \n    \n      resemble\n      VB\n      16.329627\n      0.000076\n      0.000000\n      0.085576\n      0.009522\n      128.990785\n      0.114965\n    \n    \n      infamous\n      JJ\n      16.651555\n      0.000044\n      0.000000\n      0.048915\n      0.009082\n      102.754700\n      0.091582\n    \n    \n      homes\n      NN\n      16.973483\n      0.000070\n      0.000000\n      0.078221\n      0.017292\n      93.374495\n      0.083221"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#select-significant-terms-based-on-dfidf",
    "href": "lessons/M05_VectorSpaceModels/M05_01_BOW_TFIDF.html#select-significant-terms-based-on-dfidf",
    "title": "Vector Spaces, BOW, and TFIDF",
    "section": "Select Significant Terms based on DFIDF",
    "text": "Select Significant Terms based on DFIDF\n\nlen(VOCAB)\n\n40278\n\n\n\nVOCAB.dfidf.hist();\n\n\n\n\n\nSIGS = VOCAB.sort_values('dfidf', ascending=False).head(4000)\n\n\nSIGS[['max_pos', 'n', 'i', 'dh']].head(100).style.background_gradient(cmap=gradient_cmap)\n\n\n\n\n  \n    \n       \n      max_pos\n      n\n      i\n      dh\n    \n    \n      term_str\n       \n       \n       \n       \n    \n  \n  \n    \n      hands\n      NN\n      771\n      11.382896\n      0.530737\n    \n    \n      sure\n      JJ\n      1047\n      10.941437\n      0.530737\n    \n    \n      short\n      JJ\n      699\n      11.524334\n      0.530735\n    \n    \n      new\n      JJ\n      763\n      11.397944\n      0.530726\n    \n    \n      word\n      NN\n      725\n      11.471646\n      0.530726\n    \n    \n      passed\n      VB\n      696\n      11.530539\n      0.530722\n    \n    \n      fine\n      JJ\n      719\n      11.483635\n      0.530716\n    \n    \n      name\n      NN\n      724\n      11.473637\n      0.530710\n    \n    \n      sight\n      NN\n      687\n      11.549317\n      0.530702\n    \n    \n      given\n      VB\n      639\n      11.653811\n      0.530702\n    \n    \n      people\n      NN\n      822\n      11.290488\n      0.530695\n    \n    \n      high\n      JJ\n      662\n      11.602796\n      0.530678\n    \n    \n      friend\n      NN\n      1076\n      10.902021\n      0.530666\n    \n    \n      sometimes\n      RB\n      697\n      11.528468\n      0.530666\n    \n    \n      rest\n      NN\n      696\n      11.530539\n      0.530633\n    \n    \n      knew\n      VB\n      713\n      11.495725\n      0.530617\n    \n    \n      manner\n      NN\n      748\n      11.426589\n      0.530606\n    \n    \n      true\n      JJ\n      726\n      11.469657\n      0.530543\n    \n    \n      face\n      NN\n      757\n      11.409333\n      0.530520\n    \n    \n      ill\n      NN\n      727\n      11.467671\n      0.530520\n    \n    \n      find\n      VB\n      686\n      11.551418\n      0.530507\n    \n    \n      turned\n      VB\n      643\n      11.644808\n      0.530440\n    \n    \n      open\n      JJ\n      589\n      11.771359\n      0.530440\n    \n    \n      within\n      IN\n      696\n      11.530539\n      0.530426\n    \n    \n      felt\n      VB\n      1000\n      11.007699\n      0.530426\n    \n    \n      known\n      VB\n      720\n      11.481630\n      0.530426\n    \n    \n      looking\n      VB\n      880\n      11.192123\n      0.530380\n    \n    \n      general\n      JJ\n      831\n      11.274778\n      0.530332\n    \n    \n      does\n      VB\n      848\n      11.245563\n      0.530294\n    \n    \n      point\n      NN\n      605\n      11.732692\n      0.530294\n    \n    \n      making\n      VB\n      588\n      11.773811\n      0.530294\n    \n    \n      love\n      NN\n      976\n      11.042746\n      0.530239\n    \n    \n      therefore\n      NN\n      660\n      11.607161\n      0.530239\n    \n    \n      coming\n      VB\n      644\n      11.642566\n      0.530239\n    \n    \n      room\n      NN\n      1282\n      10.649302\n      0.530239\n    \n    \n      whether\n      IN\n      760\n      11.403627\n      0.530227\n    \n    \n      kind\n      NN\n      694\n      11.534691\n      0.530181\n    \n    \n      gone\n      VB\n      688\n      11.547218\n      0.530120\n    \n    \n      told\n      VB\n      730\n      11.461730\n      0.530120\n    \n    \n      hope\n      VB\n      865\n      11.216927\n      0.530055\n    \n    \n      speak\n      VB\n      748\n      11.426589\n      0.530055\n    \n    \n      get\n      VB\n      763\n      11.397944\n      0.529987\n    \n    \n      certain\n      JJ\n      707\n      11.507917\n      0.529981\n    \n    \n      near\n      IN\n      640\n      11.651555\n      0.529916\n    \n    \n      why\n      WR\n      896\n      11.166128\n      0.529840\n    \n    \n      hear\n      VB\n      709\n      11.503841\n      0.529763\n    \n    \n      home\n      NN\n      954\n      11.075638\n      0.529687\n    \n    \n      years\n      NN\n      763\n      11.397944\n      0.529682\n    \n    \n      feel\n      VB\n      732\n      11.457783\n      0.529682\n    \n    \n      took\n      VB\n      607\n      11.727930\n      0.529597\n    \n    \n      because\n      IN\n      666\n      11.594105\n      0.529418\n    \n    \n      cried\n      VB\n      1232\n      10.706696\n      0.529255\n    \n    \n      yes\n      NN\n      945\n      11.089312\n      0.529226\n    \n    \n      herself\n      PR\n      1537\n      10.387582\n      0.529020\n    \n    \n      also\n      RB\n      581\n      11.791089\n      0.528912\n    \n    \n      brought\n      VB\n      636\n      11.660600\n      0.528912\n    \n    \n      along\n      IN\n      701\n      11.520212\n      0.528912\n    \n    \n      set\n      VB\n      717\n      11.487654\n      0.528856\n    \n    \n      mr\n      NN\n      3388\n      9.247265\n      0.528801\n    \n    \n      believe\n      VB\n      721\n      11.479628\n      0.528801\n    \n    \n      times\n      NN\n      737\n      11.447962\n      0.528750\n    \n    \n      eye\n      NN\n      644\n      11.642566\n      0.528686\n    \n    \n      gave\n      VB\n      635\n      11.662870\n      0.528568\n    \n    \n      course\n      NN\n      604\n      11.735078\n      0.528568\n    \n    \n      oh\n      NN\n      1311\n      10.617031\n      0.528528\n    \n    \n      began\n      VB\n      663\n      11.600618\n      0.528446\n    \n    \n      dear\n      JJ\n      1208\n      10.735078\n      0.528321\n    \n    \n      white\n      JJ\n      932\n      11.109297\n      0.528321\n    \n    \n      others\n      NN\n      782\n      11.362458\n      0.528173\n    \n    \n      taken\n      VB\n      545\n      11.883371\n      0.528060\n    \n    \n      moment\n      NN\n      1067\n      10.914139\n      0.527793\n    \n    \n      cannot\n      VB\n      885\n      11.183949\n      0.527498\n    \n    \n      water\n      NN\n      864\n      11.218595\n      0.527348\n    \n    \n      hour\n      NN\n      650\n      11.629187\n      0.527196\n    \n    \n      strange\n      JJ\n      688\n      11.547218\n      0.527196\n    \n    \n      together\n      RB\n      870\n      11.208611\n      0.527103\n    \n    \n      themselves\n      PR\n      838\n      11.262677\n      0.526957\n    \n    \n      best\n      JJ\n      831\n      11.274778\n      0.526957\n    \n    \n      else\n      RB\n      598\n      11.749481\n      0.526879\n    \n    \n      reason\n      NN\n      591\n      11.766469\n      0.526879\n    \n    \n      matter\n      NN\n      557\n      11.851949\n      0.526716\n    \n    \n      quite\n      RB\n      1286\n      10.644808\n      0.526656\n    \n    \n      put\n      VB\n      807\n      11.317058\n      0.526656\n    \n    \n      possible\n      JJ\n      579\n      11.796063\n      0.526549\n    \n    \n      looked\n      VB\n      939\n      11.098502\n      0.526501\n    \n    \n      large\n      JJ\n      614\n      11.711388\n      0.526379\n    \n    \n      four\n      CD\n      637\n      11.658333\n      0.526204\n    \n    \n      days\n      NN\n      845\n      11.250675\n      0.526184\n    \n    \n      small\n      JJ\n      847\n      11.247265\n      0.526184\n    \n    \n      either\n      DT\n      549\n      11.872821\n      0.526027\n    \n    \n      hard\n      JJ\n      561\n      11.841626\n      0.526027\n    \n    \n      rather\n      RB\n      892\n      11.172583\n      0.526021\n    \n    \n      next\n      JJ\n      794\n      11.340488\n      0.526021\n    \n    \n      sea\n      NN\n      1715\n      10.229490\n      0.525687\n    \n    \n      light\n      NN\n      628\n      11.678862\n      0.525661\n    \n    \n      end\n      NN\n      853\n      11.237081\n      0.525342\n    \n    \n      hardly\n      RB\n      598\n      11.749481\n      0.525280\n    \n    \n      house\n      NN\n      1423\n      10.498763\n      0.525166\n    \n    \n      person\n      NN\n      584\n      11.783658\n      0.524885\n    \n    \n      keep\n      VB\n      517\n      11.959463\n      0.524682"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_02_TimeTokenMatrices.html#set-up",
    "href": "lessons/M05_VectorSpaceModels/M05_02_TimeTokenMatrices.html#set-up",
    "title": "Time-Token Matrices",
    "section": "Set Up",
    "text": "Set Up\n\nImport\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nsns.set()\n\n\n\nConfig\n\nimport configparser\nconfig = configparser.ConfigParser()\nconfig.read(\"../env.ini\")\ndata_home = config['DEFAULT']['data_home'] \noutput_dir = config['DEFAULT']['output_dir']\ndata_prefix = 'austen-melville'\n\n\nOHCO = \"book_id chap_num para_num sent_num token_num\".split()\nbook_str = 'Persuasion'\n# data_dir1 = '../../lessons/data/output'\n# data_dir2 = '../../lessons/data/output'\ndata_prefix = 'austen-melville'"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_02_TimeTokenMatrices.html#prepare-the-data",
    "href": "lessons/M05_VectorSpaceModels/M05_02_TimeTokenMatrices.html#prepare-the-data",
    "title": "Time-Token Matrices",
    "section": "Prepare the Data",
    "text": "Prepare the Data\n\nImport tables\n\nTOKEN = pd.read_csv('{}/{}-TOKEN2.csv'.format(output_dir, data_prefix)).set_index(OHCO)\n# VOCAB = pd.read_csv('{}/{}-VOCAB.csv'.format(data_dir1, data_prefix)).set_index('term_str')\nLIB = pd.read_csv('{}/{}-LIB.csv'.format(output_dir, data_prefix)).set_index('book_id')\n\n\nTOKEN.head()\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      \n      pos_tuple\n      pos\n      token_str\n      term_str\n    \n    \n      book_id\n      chap_num\n      para_num\n      sent_num\n      token_num\n      \n      \n      \n      \n    \n  \n  \n    \n      158\n      1\n      1\n      0\n      0\n      ('Emma', 'NNP')\n      NNP\n      Emma\n      emma\n    \n    \n      1\n      ('Woodhouse,', 'NNP')\n      NNP\n      Woodhouse,\n      woodhouse\n    \n    \n      2\n      ('handsome,', 'NN')\n      NN\n      handsome,\n      handsome\n    \n    \n      3\n      ('clever,', 'NN')\n      NN\n      clever,\n      clever\n    \n    \n      4\n      ('and', 'CC')\n      CC\n      and\n      and\n    \n  \n\n\n\n\n\n\nSelect a book\n\nmy_book_id = LIB[LIB.title.str.match(book_str.upper())].index[0]\n\n\nmy_book_id\n\n105\n\n\n\nBOOK = TOKEN.loc[my_book_id].copy()\n\n\nBOOK.head()\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      pos_tuple\n      pos\n      token_str\n      term_str\n    \n    \n      chap_num\n      para_num\n      sent_num\n      token_num\n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      1\n      0\n      0\n      ('Sir', 'NNP')\n      NNP\n      Sir\n      sir\n    \n    \n      1\n      ('Walter', 'NNP')\n      NNP\n      Walter\n      walter\n    \n    \n      2\n      ('Elliot,', 'NNP')\n      NNP\n      Elliot,\n      elliot\n    \n    \n      3\n      ('of', 'IN')\n      IN\n      of\n      of\n    \n    \n      4\n      ('Kellynch', 'NNP')\n      NNP\n      Kellynch\n      kellynch"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_02_TimeTokenMatrices.html#create-token-time-matrix",
    "href": "lessons/M05_VectorSpaceModels/M05_02_TimeTokenMatrices.html#create-token-time-matrix",
    "title": "Time-Token Matrices",
    "section": "Create Token-Time Matrix",
    "text": "Create Token-Time Matrix\nWe use df.get_dummies().\n\nTTM = pd.get_dummies(BOOK['term_str'], columns=['term_str'], prefix='', prefix_sep='', drop_first=True)\\\n    .reset_index(drop=True).iloc[:,1:]\nTTM.index.name = 'time_id'\n\n\nTTM = TTM.astype('int')\n\n\nTTM.sum().sort_values(ascending=False)\n\nthe           3329\nto            2808\nand           2800\nof            2570\na             1594\n              ... \nincurious        1\nincurred         1\nindebted         1\nindecision       1\nzealously        1\nLength: 5797, dtype: int64"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_02_TimeTokenMatrices.html#visualize-dispersion-plots-of-words",
    "href": "lessons/M05_VectorSpaceModels/M05_02_TimeTokenMatrices.html#visualize-dispersion-plots-of-words",
    "title": "Time-Token Matrices",
    "section": "Visualize Dispersion Plots of Words",
    "text": "Visualize Dispersion Plots of Words\n\ncfg = {'figsize': (20,1)}\n\n\nTTM['anne'].plot(**cfg);\n\n\n\n\n\nTTM['wentworth'].plot(**cfg);\n\n\n\n\n\nTTM['walter'].plot(**cfg);"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_02_TimeTokenMatrices.html#do-better-with-seaborn-strip-plots",
    "href": "lessons/M05_VectorSpaceModels/M05_02_TimeTokenMatrices.html#do-better-with-seaborn-strip-plots",
    "title": "Time-Token Matrices",
    "section": "Do Better with Seaborn Strip Plots",
    "text": "Do Better with Seaborn Strip Plots\n\nB = BOOK['term_str'].reset_index(drop=True).to_frame().reset_index().rename(columns={'index':'offset'})\n\n\nB.head()\n\n\n\n\n\n  \n    \n      \n      offset\n      term_str\n    \n  \n  \n    \n      0\n      0\n      sir\n    \n    \n      1\n      1\n      walter\n    \n    \n      2\n      2\n      elliot\n    \n    \n      3\n      3\n      of\n    \n    \n      4\n      4\n      kellynch\n    \n  \n\n\n\n\n\ndef plot_words(words, book):    \n    \n    X = book[book.term_str.isin(words)]\n    \n    plt.figure(figsize=(22, len(words)))\n    \n    ax = sns.stripplot(y='term_str', x='offset', data=X, orient='h', marker=\".\", color='navy', size=15, jitter=0)\n    ax.set_title('Dispersion Plots', size=30, pad=20)\n    ax.set_xlabel('Narrative Time', size=20)\n    ax.set_ylabel('Term', size=20)\n    \n    plt.xticks(rotation=0, fontsize=20)\n    plt.yticks(rotation=0, fontsize=20)\n    plt.tight_layout()\n    plt.show()\n\n\nnames = 'walter elizabeth russell anne wentworth mary lyme bath'.split()\n\n\nplot_words(names, B)\n\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_02_TimeTokenMatrices.html#use-kde",
    "href": "lessons/M05_VectorSpaceModels/M05_02_TimeTokenMatrices.html#use-kde",
    "title": "Time-Token Matrices",
    "section": "Use KDE",
    "text": "Use KDE\nKDE works by positing that each instance of a token represents a small gaussian distribution before and after the word. When words appear in bunches, their curves add up, producing an aggregate curve over narrative time.\n\nkde_kernel = 'gaussian'\nkde_bandwidth = 2000\nkde_samples = 1000\n\n\nimport numpy as np\nimport scipy as sp\nfrom sklearn.neighbors import KernelDensity as KDE\nfrom nltk.corpus import stopwords\n\n\nCreate arrays of offsets for each term\n\nX = B.reset_index().groupby(['term_str']).offset.apply(lambda x: x.tolist()).to_frame()\n\n\nX['x'] = X.apply(lambda x: np.array(x.offset)[:, np.newaxis], 1)\n\n\nX.head()\n\n\n\n\n\n  \n    \n      \n      offset\n      x\n    \n    \n      term_str\n      \n      \n    \n  \n  \n    \n      1\n      [121, 153]\n      [[121], [153]]\n    \n    \n      15\n      [125]\n      [[125]]\n    \n    \n      16\n      [209]\n      [[209]]\n    \n    \n      1760\n      [122]\n      [[122]]\n    \n    \n      1784\n      [126]\n      [[126]]\n    \n  \n\n\n\n\n\n\nGet KDE for each term\n\nscale_max = B.offset.max() # THIS IS CRUCIAL\nx_axis = np.linspace(0, scale_max, kde_samples)[:, np.newaxis]\nX['kde'] = X.apply(lambda row: KDE(kernel=kde_kernel, bandwidth=kde_bandwidth).fit(row.x), 1)\nX['scores'] = X.apply(lambda row: row.kde.score_samples(x_axis), axis=1)\n# B['scaled'] = B.apply(lambda row: np.exp(row.scores) * (scale_max / kde_samples), axis=1)\n\n\n\nVisualize KDE plots\n\nPLOTS = X.apply(lambda row: pd.Series(np.exp(row.scores) * (scale_max / kde_samples)), axis=1)\n\n\nFIG = dict(figsize=(15, 5))\n\n\nPLOTS.loc['wentworth'].plot(**FIG);\nPLOTS.loc['anne'].plot(**FIG);\n\n\n\n\n\nPLOTS.loc['anne'].plot(**FIG);\nPLOTS.loc['walter'].plot(**FIG);\n\n\n\n\n\nPLOTS.loc['walter'].plot(**FIG);\nPLOTS.loc['wentworth'].plot(**FIG);\n\n\n\n\n\nPLOTS.loc['walter'].plot(**FIG);\nPLOTS.loc['elizabeth'].plot(**FIG);"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_02_TimeTokenMatrices.html#question",
    "href": "lessons/M05_VectorSpaceModels/M05_02_TimeTokenMatrices.html#question",
    "title": "Time-Token Matrices",
    "section": "Question",
    "text": "Question\nWe can treat each word as a vector of narrative time. What can we learn by correlated words in this space?\nFor a clue, see this essay by David McClure."
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_03_TFIDF_Exploration.html",
    "href": "lessons/M05_VectorSpaceModels/M05_03_TFIDF_Exploration.html",
    "title": "Variant TFIDFs and Document Significance",
    "section": "",
    "text": "Exposition\nGlobal significance in this context means aggregate significance for the whole corpus.\nWe get the sums of each variant TF.\nWe compare sums to means.\nCombine and compare\nWe can see that there is no difference between mean and sum for sorting terms by global significance.\nWe can also see that boolean counting produces a different distribution than the others, which are similar to each other.\nThis is an experimental concept that focuses on the amount of significance contained in a document, such as a book chapter, based on the sum or mean of TFIDF of terms in the document.\n>>> We computes TFIDF with the book as the context.\nFor example, for Persuasion (pg105), we would use this BOW:\nApply to all books.\nDefine function to visualize chapter significance over narrative time.\nAlso define function to see how length and significance are related."
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_03_TFIDF_Exploration.html#extract-the-docs-table",
    "href": "lessons/M05_VectorSpaceModels/M05_03_TFIDF_Exploration.html#extract-the-docs-table",
    "title": "Variant TFIDFs and Document Significance",
    "section": "Extract the DOCS table",
    "text": "Extract the DOCS table\nThis is a table of bag-level observations. We’ll use this later when exploring document level significance.\n\nDOCS = BOW.groupby(BAG).n.sum().to_frame('n')\n\n\nDOCS\n\n\n\n\n\n  \n    \n      \n      \n      n\n    \n    \n      book_id\n      chap_num\n      \n    \n  \n  \n    \n      105\n      1\n      2625\n    \n    \n      2\n      1974\n    \n    \n      3\n      2837\n    \n    \n      4\n      1805\n    \n    \n      5\n      3322\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      53861\n      8\n      2286\n    \n    \n      11\n      3387\n    \n    \n      13\n      2180\n    \n    \n      14\n      2790\n    \n    \n      15\n      2513\n    \n  \n\n1122 rows × 1 columns"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_03_TFIDF_Exploration.html#traditional",
    "href": "lessons/M05_VectorSpaceModels/M05_03_TFIDF_Exploration.html#traditional",
    "title": "Variant TFIDFs and Document Significance",
    "section": "Traditional",
    "text": "Traditional\n\nTF = BOW.n.unstack(fill_value=0) # Document-Term Count Matrix\nDF = TF.astype('bool').sum() \nN = len(DOCS)\nIDF = np.log2(N/DF)      \nTFIDF = TF * IDF\nTFIDF_agg = TFIDF.sum()\n\n\nTFIDF_agg.sort_values(ascending=False).head(20)\n\nterm_str\nshe          10147.034980\nher           9340.136767\nmr            5368.081714\nyou           5117.779036\npierre        5089.597669\nmrs           4599.970085\ni             4338.610084\nmiss          3790.908454\nfanny         3554.708113\nemma          3383.286987\nwhale         3358.816595\nsir           3162.338202\ncaptain       2973.754321\nme            2962.274984\nelinor        2832.715249\nyour          2789.668389\nmy            2750.546355\nisrael        2737.141626\nelizabeth     2612.442505\nam            2541.677752\ndtype: float64"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_03_TFIDF_Exploration.html#variants",
    "href": "lessons/M05_VectorSpaceModels/M05_03_TFIDF_Exploration.html#variants",
    "title": "Variant TFIDFs and Document Significance",
    "section": "Variants",
    "text": "Variants\n\ntf_variants = {\n    'raw': lambda tf: tf,\n    'rel': lambda tf: (tf.T / tf.T.sum()).T,\n    'max': lambda tf, alpha=.4: alpha + (1 - alpha) * (tf.T / tf.T.max()).T,\n    'log': lambda tf: np.log2(1 + tf),\n    'bool': lambda tf: tf.astype('bool').astype('int'),\n    # 'sub': lambda tf: 1 + np.log2(tf)\n}\n\n\ntfidf_variants = {k: tf_variants[k](TF) * IDF for k, v in tf_variants.items()}"
  },
  {
    "objectID": "lessons/M05_VectorSpaceModels/M05_03_TFIDF_Exploration.html#dfidf-as-dh",
    "href": "lessons/M05_VectorSpaceModels/M05_03_TFIDF_Exploration.html#dfidf-as-dh",
    "title": "Variant TFIDFs and Document Significance",
    "section": "DFIDF as DH",
    "text": "DFIDF as DH\nLet’s compute the global entropy of terms in the corpus and use that as another measure of global significance.\nIt turns out that this measure is the same as TFIDF summing from a boolean count matrix.\n\nDFIDF = (DF * IDF).to_frame('val')\nDP = DF / N\nDI = np.log2(1/DP)\nDH = (DP * DI).to_frame('val')\n\n\npd.concat([\n        DFIDF.sort_values('val', ascending=False).head(20).reset_index(),\n        DH.sort_values('val', ascending=False).head(20).reset_index(), \n        tfidf_sums['bool'].sort_values('sum_val', ascending=False).head(20).reset_index()\n    ], keys=['dfidf', 'dh', 'bool'], axis=1)\\\n    .style.background_gradient('YlGnBu')\n\n\n\n\n  \n    \n       \n      dfidf\n      dh\n      bool\n    \n    \n       \n      term_str\n      val\n      term_str\n      val\n      term_str\n      sum_val\n    \n  \n  \n    \n      0\n      hands\n      595.486851\n      hands\n      0.530737\n      hands\n      595.486851\n    \n    \n      1\n      sure\n      595.486851\n      sure\n      0.530737\n      sure\n      595.486851\n    \n    \n      2\n      short\n      595.485181\n      short\n      0.530735\n      short\n      595.485181\n    \n    \n      3\n      new\n      595.474513\n      new\n      0.530726\n      new\n      595.474513\n    \n    \n      4\n      word\n      595.474513\n      word\n      0.530726\n      word\n      595.474513\n    \n    \n      5\n      passed\n      595.469573\n      passed\n      0.530722\n      passed\n      595.469573\n    \n    \n      6\n      fine\n      595.463070\n      fine\n      0.530716\n      fine\n      595.463070\n    \n    \n      7\n      name\n      595.456562\n      name\n      0.530710\n      name\n      595.456562\n    \n    \n      8\n      given\n      595.448100\n      given\n      0.530702\n      given\n      595.448100\n    \n    \n      9\n      sight\n      595.448100\n      sight\n      0.530702\n      sight\n      595.448100\n    \n    \n      10\n      people\n      595.440092\n      people\n      0.530695\n      people\n      595.440092\n    \n    \n      11\n      high\n      595.420171\n      high\n      0.530678\n      high\n      595.420171\n    \n    \n      12\n      friend\n      595.407544\n      friend\n      0.530666\n      friend\n      595.407544\n    \n    \n      13\n      sometimes\n      595.407544\n      sometimes\n      0.530666\n      sometimes\n      595.407544\n    \n    \n      14\n      rest\n      595.370006\n      rest\n      0.530633\n      rest\n      595.370006\n    \n    \n      15\n      knew\n      595.352773\n      knew\n      0.530617\n      knew\n      595.352773\n    \n    \n      16\n      manner\n      595.339780\n      manner\n      0.530606\n      manner\n      595.339780\n    \n    \n      17\n      true\n      595.269079\n      true\n      0.530543\n      true\n      595.269079\n    \n    \n      18\n      ill\n      595.243812\n      ill\n      0.530520\n      ill\n      595.243812\n    \n    \n      19\n      face\n      595.243812\n      face\n      0.530520\n      face\n      595.243812"
  },
  {
    "objectID": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html",
    "href": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html",
    "title": "Similarity and Distance Measures",
    "section": "",
    "text": "Set Up\nUse Pandas’ df.corr('kendall') method to generate pairs.\nCorrelation is normalized covariance, which is the dot product two vectors.\nKendall’s correlation is non-parametric – it does not require the two variables have a bell curve distribution."
  },
  {
    "objectID": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#config",
    "href": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#config",
    "title": "Similarity and Distance Measures",
    "section": "Config",
    "text": "Config\n\nimport configparser\nconfig = configparser.ConfigParser()\nconfig.read(\"../env.ini\")\ndata_hone = config['DEFAULT']['data_home']\noutput_dir = config['DEFAULT']['output_dir']\n\n\ndata_prefix = 'austen-melville'\n\n\nOHCO = ['book_id', 'chap_num']\n\n\ncolors = \"YlGnBu\""
  },
  {
    "objectID": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#import",
    "href": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#import",
    "title": "Similarity and Distance Measures",
    "section": "Import",
    "text": "Import\n\nimport pandas as pd\nimport numpy as np\n\n\nimport seaborn as sns; sns.set()\n\n\nfrom numpy.linalg import norm\nfrom scipy.spatial.distance import pdist\n\n\nimport scipy.cluster.hierarchy as sch\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#import-tables",
    "href": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#import-tables",
    "title": "Similarity and Distance Measures",
    "section": "Import tables",
    "text": "Import tables\n\nBOW = pd.read_csv(f\"{output_dir}/{data_prefix}-BOW.csv\").set_index(OHCO+['term_str'])\nLIB = pd.read_csv(f'{output_dir}/{data_prefix}-LIB.csv').set_index('book_id')\n\nAdd back a row to LIB that was removed earlier.\n\nLIB.loc[53861, 'title'] = 'THE APPLE TREE TABLE AND OTHER SKETCHES'\nLIB.loc[53861, 'author'] = 'MELVILLE, HERMAN'\nLIB.loc[53861, 'book_len'] = BOW.loc[53861].n.sum()\n\n\n# LIB\n\nAdd label for visualizations\n\nLIB['label'] = LIB.author.str.split(', ').str[0] + ': ' +  LIB.title.str[:20] + ' (' + LIB.index.astype('str') + ')'\nLIB['label'] = LIB.label.str.title()\n\n\nLIB.label\n\nbook_id\n105                    Austen: Persuasion (105)\n121              Austen: Northanger Abbey (121)\n141                Austen: Mansfield Park (141)\n158                          Austen: Emma (158)\n161          Austen: Sense And Sensibilit (161)\n946                    Austen: Lady Susan (946)\n1212        Austen: Love And Freindship  (1212)\n1342         Austen: Pride And Prejudice (1342)\n1900      Melville: Typee A Romance Of T (1900)\n2701      Melville: Moby Dick Or The Wha (2701)\n4045      Melville: Omoo Adventures In T (4045)\n8118      Melville: Redburn His First Vo (8118)\n10712    Melville: White Jacket Or The  (10712)\n13720    Melville: Mardi And A Voyage T (13720)\n13721    Melville: Mardi And A Voyage T (13721)\n15422    Melville: Israel Potter His Fi (15422)\n15859        Melville: The Piazza Tales (15859)\n21816    Melville: The Confidence Man H (21816)\n34970    Melville: Pierre Or The Ambigu (34970)\n53861    Melville: The Apple Tree Table (53861)\nName: label, dtype: object"
  },
  {
    "objectID": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#get-tfidf-matrix",
    "href": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#get-tfidf-matrix",
    "title": "Similarity and Distance Measures",
    "section": "Get TFIDF Matrix",
    "text": "Get TFIDF Matrix\nEven though we already computed TFIDF in our imported BOW, we recreate it here.\n\n# TFIDF_CHAP = BOW.tfidf.unstack(fill_value=0)\n\n\n# TFIDF_CHAP\n\n\nget_tfidf = lambda X, agg_func='sum': (X.T / X.T.agg(agg_func)).T * (np.log2(len(X)/X.astype('bool').sum()))\n\n\nTFIDF_CHAP = get_tfidf(BOW.n.unstack(fill_value=0))\n\n\nTFIDF_CHAP\n\n\n\n\n\n  \n    \n      \n      term_str\n      0\n      1\n      10\n      100\n      1000\n      10000\n      1000000\n      10000000\n      10440\n      10800\n      ...\n      zoroaster\n      zozo\n      zuma\n      zur\n      à\n      æneas\n      æniad\n      æson\n      æsops\n      ł20000\n    \n    \n      book_id\n      chap_num\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      105\n      1\n      0.0\n      0.005084\n      0.0\n      0.0\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      2\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      3\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      4\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      5\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      53861\n      8\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      11\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      13\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.004648\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      14\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      15\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n1122 rows × 40478 columns"
  },
  {
    "objectID": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#collapse-bags",
    "href": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#collapse-bags",
    "title": "Similarity and Distance Measures",
    "section": "Collapse Bags",
    "text": "Collapse Bags\nWe want to work with larger bags in this notebook, in order to better visualize our resulting clusters.\nNote that this is not the same as beginning with book as the bag! We are getting aggregate TFIDF for chapters.\n\nTFIDF = TFIDF_CHAP.groupby(OHCO[:1]).sum()\n\n\nTFIDF\n\n\n\n\n\n  \n    \n      term_str\n      0\n      1\n      10\n      100\n      1000\n      10000\n      1000000\n      10000000\n      10440\n      10800\n      ...\n      zoroaster\n      zozo\n      zuma\n      zur\n      à\n      æneas\n      æniad\n      æson\n      æsops\n      ł20000\n    \n    \n      book_id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      105\n      0.00000\n      0.005084\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.00000\n      0.000000\n      0.00000\n      0.000000\n      0.000000\n    \n    \n      121\n      0.00000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.00000\n      0.000000\n      0.00000\n      0.000000\n      0.000000\n    \n    \n      141\n      0.00000\n      0.000000\n      0.007691\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.00000\n      0.000000\n      0.00000\n      0.000000\n      0.002243\n    \n    \n      158\n      0.00000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.011089\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.00000\n      0.000000\n      0.00000\n      0.000000\n      0.000000\n    \n    \n      161\n      0.00000\n      0.002349\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.00000\n      0.000000\n      0.00000\n      0.000000\n      0.000000\n    \n    \n      946\n      0.00000\n      0.000000\n      0.011205\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.00000\n      0.000000\n      0.00000\n      0.000000\n      0.000000\n    \n    \n      1212\n      0.00259\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.00000\n      0.000000\n      0.00000\n      0.000000\n      0.000000\n    \n    \n      1342\n      0.00000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.00000\n      0.000000\n      0.00000\n      0.000000\n      0.000000\n    \n    \n      1900\n      0.00000\n      0.000000\n      0.002637\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.00000\n      0.000000\n      0.00000\n      0.000000\n      0.000000\n    \n    \n      2701\n      0.00000\n      0.001286\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.003214\n      0.011436\n      ...\n      0.004030\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.00000\n      0.000000\n      0.00000\n      0.000000\n      0.000000\n    \n    \n      4045\n      0.00000\n      0.000000\n      0.000000\n      0.000000\n      0.005668\n      0.005668\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.030154\n      0.000000\n      0.00000\n      0.000000\n      0.00000\n      0.000000\n      0.000000\n    \n    \n      8118\n      0.00000\n      0.001952\n      0.002285\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.00000\n      0.000000\n      0.00000\n      0.000000\n      0.000000\n    \n    \n      10712\n      0.00000\n      0.038576\n      0.000000\n      0.005212\n      0.005633\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.00000\n      0.000000\n      0.00000\n      0.000000\n      0.000000\n    \n    \n      13720\n      0.00000\n      0.000000\n      0.000000\n      0.009653\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.095696\n      0.000000\n      0.000000\n      0.00000\n      0.000000\n      0.00000\n      0.000000\n      0.000000\n    \n    \n      13721\n      0.00000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.008648\n      0.007417\n      0.000000\n      0.000000\n      0.000000\n      0.00000\n      0.000000\n      0.00000\n      0.000000\n      0.000000\n    \n    \n      15422\n      0.00000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.004160\n      0.00000\n      0.000000\n      0.00000\n      0.000000\n      0.000000\n    \n    \n      15859\n      0.00000\n      0.000508\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000386\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.00000\n      0.000000\n      0.00000\n      0.000000\n      0.000000\n    \n    \n      21816\n      0.00000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.011302\n      0.00347\n      0.000000\n      0.00694\n      0.004211\n      0.000000\n    \n    \n      34970\n      0.00000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.00000\n      0.007289\n      0.00000\n      0.000000\n      0.000000\n    \n    \n      53861\n      0.00000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.004648\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.00000\n      0.000000\n      0.00000\n      0.000000\n      0.000000\n    \n  \n\n20 rows × 40478 columns"
  },
  {
    "objectID": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#create-normalized-tables",
    "href": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#create-normalized-tables",
    "title": "Similarity and Distance Measures",
    "section": "Create Normalized Tables",
    "text": "Create Normalized Tables\nWe normalize TFIDF values in various ways. Normalizations types are required for certain distance measures.\n\nL0 = TFIDF.astype('bool').astype('int') # Binary (Pseudo L)\nL1 = TFIDF.apply(lambda x: x / x.sum(), 1) # Probabilistic\nL2 = TFIDF.apply(lambda x: x / norm(x), 1) # Pythagorean / Euclidean"
  },
  {
    "objectID": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#create-doc-pair-table",
    "href": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#create-doc-pair-table",
    "title": "Similarity and Distance Measures",
    "section": "Create Doc Pair Table",
    "text": "Create Doc Pair Table\nCreate a table to store our results.\nNote that pist() is a “distance matrix computation from a collection of raw observation vectors stored in a rectangular array”.\n\nPAIRS = pd.DataFrame(index=pd.MultiIndex.from_product([LIB.index.tolist(), LIB.index.tolist()])).reset_index()\nPAIRS = PAIRS[PAIRS.level_0 < PAIRS.level_1].set_index(['level_0','level_1'])\nPAIRS.index.names = ['doc_a', 'doc_b']\n\n\nPAIRS.shape\n\n(190, 0)\n\n\n\nPAIRS.head()\n\n\n\n\n\n  \n    \n      \n      \n    \n    \n      doc_a\n      doc_b\n    \n  \n  \n    \n      105\n      121\n    \n    \n      141\n    \n    \n      158\n    \n    \n      161\n    \n    \n      946"
  },
  {
    "objectID": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#compute-distances",
    "href": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#compute-distances",
    "title": "Similarity and Distance Measures",
    "section": "Compute Distances",
    "text": "Compute Distances\n\nPAIRS['cityblock'] = pdist(TFIDF, 'cityblock')\nPAIRS['euclidean'] = pdist(TFIDF, 'euclidean')\nPAIRS['cosine'] = pdist(TFIDF, 'cosine')\nPAIRS['jaccard'] = pdist(L0, 'jaccard')\nPAIRS['dice'] = pdist(L0, 'dice')\nPAIRS['js'] = pdist(L1, 'jensenshannon')  \n# PAIRS['euclidean2'] = pdist(L2, 'euclidean') # Should be the same as cosine (colinear)\n\n\nPAIRS.head(20).style.background_gradient(colors)\n\n\n\n\n  \n    \n       \n       \n      cityblock\n      euclidean\n      cosine\n      jaccard\n      dice\n      js\n    \n    \n      doc_a\n      doc_b\n       \n       \n       \n       \n       \n       \n    \n  \n  \n    \n      105\n      121\n      40.909822\n      1.573914\n      0.688280\n      0.568853\n      0.397480\n      0.509943\n    \n    \n      141\n      54.815622\n      2.078217\n      0.668838\n      0.546709\n      0.376187\n      0.474997\n    \n    \n      158\n      64.161108\n      2.520715\n      0.688528\n      0.551126\n      0.380382\n      0.484127\n    \n    \n      161\n      60.158757\n      2.412967\n      0.709612\n      0.542778\n      0.372475\n      0.491733\n    \n    \n      946\n      63.533353\n      2.606124\n      0.777684\n      0.670040\n      0.503804\n      0.584151\n    \n    \n      1212\n      48.718396\n      1.546478\n      0.760849\n      0.662869\n      0.495739\n      0.604515\n    \n    \n      1342\n      74.383627\n      2.656548\n      0.647429\n      0.544591\n      0.374184\n      0.488157\n    \n    \n      1900\n      51.624963\n      1.389650\n      0.850855\n      0.692883\n      0.530085\n      0.629089\n    \n    \n      2701\n      274.970259\n      4.921130\n      0.865946\n      0.781581\n      0.641471\n      0.656644\n    \n    \n      4045\n      149.670196\n      2.542090\n      0.812592\n      0.715233\n      0.556702\n      0.629589\n    \n    \n      8118\n      107.311932\n      1.944733\n      0.799339\n      0.727273\n      0.571429\n      0.628384\n    \n    \n      10712\n      175.806472\n      3.027022\n      0.831706\n      0.741320\n      0.588966\n      0.642494\n    \n    \n      13720\n      216.292756\n      3.447123\n      0.852731\n      0.752329\n      0.602986\n      0.665921\n    \n    \n      13721\n      208.470684\n      4.102092\n      0.902403\n      0.772496\n      0.629323\n      0.686688\n    \n    \n      15422\n      66.675840\n      1.789416\n      0.890629\n      0.722441\n      0.565485\n      0.652326\n    \n    \n      15859\n      35.428614\n      1.272290\n      0.896508\n      0.757494\n      0.609650\n      0.688291\n    \n    \n      21816\n      86.413753\n      1.896219\n      0.828939\n      0.712235\n      0.553079\n      0.626686\n    \n    \n      34970\n      216.427798\n      5.415305\n      0.862221\n      0.743697\n      0.591972\n      0.630191\n    \n    \n      53861\n      42.909882\n      1.306865\n      0.868884\n      0.720715\n      0.563373\n      0.650398\n    \n    \n      121\n      141\n      54.892301\n      2.027582\n      0.606981\n      0.559241\n      0.388157\n      0.477118"
  },
  {
    "objectID": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#compare-distributions",
    "href": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#compare-distributions",
    "title": "Similarity and Distance Measures",
    "section": "Compare Distributions",
    "text": "Compare Distributions\n\nSAMPLE = PAIRS.sample(1000) if PAIRS.shape[0] > 1000 else PAIRS\n\n\nsns.pairplot(SAMPLE);\n\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n\n\n\n\n\n\nPAIRS.sort_values('cosine').head(20).style.background_gradient(colors)\n\n\n\n\n  \n    \n       \n       \n      cityblock\n      euclidean\n      cosine\n      jaccard\n      dice\n      js\n    \n    \n      doc_a\n      doc_b\n       \n       \n       \n       \n       \n       \n    \n  \n  \n    \n      13720\n      13721\n      228.712580\n      3.568536\n      0.451381\n      0.611127\n      0.440017\n      0.525269\n    \n    \n      8118\n      10712\n      161.525500\n      2.568463\n      0.492479\n      0.600236\n      0.428813\n      0.520802\n    \n    \n      4045\n      8118\n      146.948632\n      2.200207\n      0.507743\n      0.616342\n      0.445444\n      0.535058\n    \n    \n      1900\n      4045\n      139.280425\n      2.206924\n      0.542522\n      0.581559\n      0.409999\n      0.527813\n    \n    \n      4045\n      10712\n      190.300010\n      2.896442\n      0.558332\n      0.614213\n      0.443223\n      0.544420\n    \n    \n      158\n      1342\n      68.966111\n      2.902299\n      0.569073\n      0.525485\n      0.356378\n      0.459043\n    \n    \n      121\n      1342\n      70.969509\n      2.564696\n      0.575981\n      0.536637\n      0.366715\n      0.475328\n    \n    \n      4045\n      13720\n      221.667824\n      3.257794\n      0.584066\n      0.628488\n      0.458245\n      0.560435\n    \n    \n      2701\n      8118\n      250.393407\n      4.504029\n      0.590995\n      0.632841\n      0.462887\n      0.535405\n    \n    \n      141\n      1342\n      67.651317\n      2.749906\n      0.591104\n      0.525060\n      0.355988\n      0.454695\n    \n    \n      158\n      58.257443\n      2.631266\n      0.605536\n      0.504309\n      0.337175\n      0.444887\n    \n    \n      121\n      141\n      54.892301\n      2.027582\n      0.606981\n      0.559241\n      0.388157\n      0.477118\n    \n    \n      8118\n      13720\n      210.878684\n      3.157527\n      0.611373\n      0.635488\n      0.465725\n      0.568121\n    \n    \n      121\n      158\n      63.516401\n      2.448988\n      0.621776\n      0.566453\n      0.395140\n      0.487305\n    \n    \n      141\n      161\n      59.239427\n      2.567986\n      0.624661\n      0.524103\n      0.355108\n      0.458174\n    \n    \n      2701\n      10712\n      271.509569\n      4.706280\n      0.626132\n      0.613995\n      0.442996\n      0.539846\n    \n    \n      161\n      1342\n      65.910417\n      2.975595\n      0.629725\n      0.494283\n      0.328271\n      0.450815\n    \n    \n      2701\n      13720\n      291.083668\n      4.860478\n      0.635975\n      0.630116\n      0.459977\n      0.549841\n    \n    \n      4045\n      270.230194\n      4.645122\n      0.639610\n      0.648967\n      0.480348\n      0.556385\n    \n    \n      105\n      1342\n      74.383627\n      2.656548\n      0.647429\n      0.544591\n      0.374184\n      0.488157"
  },
  {
    "objectID": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#hiearchical",
    "href": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#hiearchical",
    "title": "Similarity and Distance Measures",
    "section": "Hiearchical",
    "text": "Hiearchical\n\ndef hca(sims, linkage_method='complete', color_thresh=.3, figsize=(10, 10)):\n    tree = sch.linkage(sims, method=linkage_method)\n    labels = LIB.label.values\n    plt.figure()\n    fig, axes = plt.subplots(figsize=figsize)\n    dendrogram = sch.dendrogram(tree, \n                                labels=labels, \n                                orientation=\"left\", \n                                count_sort=True,\n                                distance_sort=True,\n                                above_threshold_color='.75',\n                                color_threshold=color_thresh\n                               )\n    plt.tick_params(axis='both', which='major', labelsize=14)\n\n\nhca(PAIRS.cosine, linkage_method='ward', color_thresh=1)\n\n<Figure size 640x480 with 0 Axes>\n\n\n\n\n\n\nhca(PAIRS.cosine, linkage_method='complete', color_thresh=.9);\n\n<Figure size 640x480 with 0 Axes>\n\n\n\n\n\n\nhca(PAIRS.jaccard, color_thresh=.8);\n\n<Figure size 640x480 with 0 Axes>\n\n\n\n\n\n\nhca(PAIRS.euclidean, linkage_method='ward', color_thresh=80);\n\n<Figure size 640x480 with 0 Axes>\n\n\n\n\n\n\nhca(PAIRS.cityblock, color_thresh=1300);\n\n<Figure size 640x480 with 0 Axes>\n\n\n\n\n\n\nhca(PAIRS.js, color_thresh=.7);\n\n<Figure size 640x480 with 0 Axes>"
  },
  {
    "objectID": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#questions",
    "href": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#questions",
    "title": "Similarity and Distance Measures",
    "section": "Questions",
    "text": "Questions\nWhat distance measures produce the best results?"
  },
  {
    "objectID": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#k-means",
    "href": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#k-means",
    "title": "Similarity and Distance Measures",
    "section": "K-Means",
    "text": "K-Means\nK-Means only uses Euclidean distance. Why?\n\nThe K-Means procedure does not explicitly use pairwise distances between data points.\nInstead, it repeatedly assigns points to the closest centroid thereby using Euclidean distance from data points to a centroid.\nHowever, K-Means is implicitly based on pairwise Euclidean distances between data points, because the sum of squared deviations from centroid is equal to the sum of pairwise squared Euclidean distances divided by the number of points.\nThe term “centroid” is itself from Euclidean geometry. It is multivariate mean in Euclidean space. Euclidean space is about euclidean distances. Non-Euclidean distances will generally not span Euclidean space. That’s why K-Means is for Euclidean distances only.\n\nSee the Cross Validated post on this.\n\nfrom sklearn.cluster import KMeans\n\n\ndef get_k_clusters(k=10):\n    LIB[f'y_raw_{k}'] = KMeans(k).fit_predict(TFIDF)\n    LIB[f'y_L0_{k}'] = KMeans(k).fit_predict(L0)\n    LIB[f'y_L1_{k}'] = KMeans(k).fit_predict(L1)\n    LIB[f'y_L2_{k}'] = KMeans(k).fit_predict(L2)\n    y_cols = [col for col in LIB.columns if 'y_' in col and f'_{k}' in col]\n    return LIB.reset_index().set_index('label')[y_cols].sort_values('label').style.background_gradient(colors)\n\n\nget_k_clusters(2)\n\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\n  \n    \n       \n      y_raw_2\n      y_L0_2\n      y_L1_2\n      y_L2_2\n    \n    \n      label\n       \n       \n       \n       \n    \n  \n  \n    \n      Austen: Emma (158)\n      0\n      1\n      1\n      1\n    \n    \n      Austen: Lady Susan (946)\n      0\n      1\n      0\n      1\n    \n    \n      Austen: Love And Freindship  (1212)\n      0\n      1\n      1\n      1\n    \n    \n      Austen: Mansfield Park (141)\n      0\n      1\n      1\n      1\n    \n    \n      Austen: Northanger Abbey (121)\n      0\n      1\n      1\n      1\n    \n    \n      Austen: Persuasion (105)\n      0\n      1\n      1\n      1\n    \n    \n      Austen: Pride And Prejudice (1342)\n      0\n      1\n      1\n      1\n    \n    \n      Austen: Sense And Sensibilit (161)\n      0\n      1\n      1\n      1\n    \n    \n      Melville: Israel Potter His Fi (15422)\n      0\n      1\n      1\n      0\n    \n    \n      Melville: Mardi And A Voyage T (13720)\n      1\n      0\n      1\n      0\n    \n    \n      Melville: Mardi And A Voyage T (13721)\n      1\n      0\n      1\n      0\n    \n    \n      Melville: Moby Dick Or The Wha (2701)\n      1\n      0\n      1\n      0\n    \n    \n      Melville: Omoo Adventures In T (4045)\n      0\n      0\n      1\n      0\n    \n    \n      Melville: Pierre Or The Ambigu (34970)\n      0\n      1\n      1\n      0\n    \n    \n      Melville: Redburn His First Vo (8118)\n      0\n      0\n      1\n      0\n    \n    \n      Melville: The Apple Tree Table (53861)\n      0\n      1\n      1\n      0\n    \n    \n      Melville: The Confidence Man H (21816)\n      0\n      1\n      1\n      0\n    \n    \n      Melville: The Piazza Tales (15859)\n      0\n      1\n      1\n      0\n    \n    \n      Melville: Typee A Romance Of T (1900)\n      0\n      0\n      1\n      0\n    \n    \n      Melville: White Jacket Or The  (10712)\n      0\n      0\n      1\n      0\n    \n  \n\n\n\n\nget_k_clusters(4)\n\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/rca2t1/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\n  \n    \n       \n      y_raw_4\n      y_L0_4\n      y_L1_4\n      y_L2_4\n    \n    \n      label\n       \n       \n       \n       \n    \n  \n  \n    \n      Austen: Emma (158)\n      0\n      2\n      1\n      0\n    \n    \n      Austen: Lady Susan (946)\n      0\n      2\n      3\n      0\n    \n    \n      Austen: Love And Freindship  (1212)\n      0\n      2\n      1\n      0\n    \n    \n      Austen: Mansfield Park (141)\n      0\n      2\n      1\n      0\n    \n    \n      Austen: Northanger Abbey (121)\n      0\n      2\n      1\n      0\n    \n    \n      Austen: Persuasion (105)\n      0\n      2\n      2\n      0\n    \n    \n      Austen: Pride And Prejudice (1342)\n      0\n      2\n      1\n      0\n    \n    \n      Austen: Sense And Sensibilit (161)\n      0\n      2\n      0\n      0\n    \n    \n      Melville: Israel Potter His Fi (15422)\n      0\n      0\n      1\n      2\n    \n    \n      Melville: Mardi And A Voyage T (13720)\n      3\n      1\n      1\n      3\n    \n    \n      Melville: Mardi And A Voyage T (13721)\n      3\n      1\n      1\n      3\n    \n    \n      Melville: Moby Dick Or The Wha (2701)\n      2\n      0\n      1\n      1\n    \n    \n      Melville: Omoo Adventures In T (4045)\n      0\n      1\n      1\n      1\n    \n    \n      Melville: Pierre Or The Ambigu (34970)\n      1\n      3\n      1\n      3\n    \n    \n      Melville: Redburn His First Vo (8118)\n      0\n      0\n      1\n      1\n    \n    \n      Melville: The Apple Tree Table (53861)\n      0\n      1\n      1\n      1\n    \n    \n      Melville: The Confidence Man H (21816)\n      0\n      2\n      1\n      1\n    \n    \n      Melville: The Piazza Tales (15859)\n      0\n      1\n      1\n      1\n    \n    \n      Melville: Typee A Romance Of T (1900)\n      0\n      2\n      1\n      1\n    \n    \n      Melville: White Jacket Or The  (10712)\n      0\n      0\n      1\n      1"
  },
  {
    "objectID": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#compare-correlations-by-author",
    "href": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#compare-correlations-by-author",
    "title": "Similarity and Distance Measures",
    "section": "Compare Correlations by Author",
    "text": "Compare Correlations by Author\n\n(CORR_MATRIX * 10).astype('int').style.background_gradient(cmap=colors, axis=None)\n\n\n\n\n  \n    \n      book_id\n      105\n      121\n      141\n      158\n      161\n      946\n      1212\n      1342\n      1900\n      2701\n      4045\n      8118\n      10712\n      13720\n      13721\n      15422\n      15859\n      21816\n      34970\n      53861\n    \n    \n      book_id\n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n  \n  \n    \n      105\n      10\n      5\n      5\n      5\n      5\n      4\n      4\n      5\n      3\n      2\n      3\n      3\n      2\n      2\n      2\n      3\n      2\n      3\n      3\n      3\n    \n    \n      121\n      5\n      10\n      5\n      5\n      5\n      4\n      4\n      5\n      3\n      2\n      3\n      3\n      2\n      2\n      2\n      3\n      2\n      3\n      3\n      3\n    \n    \n      141\n      5\n      5\n      10\n      5\n      5\n      4\n      4\n      5\n      3\n      2\n      3\n      3\n      2\n      2\n      2\n      3\n      2\n      3\n      2\n      3\n    \n    \n      158\n      5\n      5\n      5\n      10\n      5\n      4\n      4\n      5\n      3\n      2\n      3\n      3\n      2\n      2\n      2\n      2\n      2\n      3\n      2\n      3\n    \n    \n      161\n      5\n      5\n      5\n      5\n      10\n      4\n      4\n      6\n      3\n      2\n      3\n      3\n      2\n      2\n      2\n      3\n      2\n      3\n      3\n      3\n    \n    \n      946\n      4\n      4\n      4\n      4\n      4\n      10\n      4\n      5\n      3\n      2\n      2\n      2\n      2\n      2\n      1\n      2\n      2\n      2\n      2\n      2\n    \n    \n      1212\n      4\n      4\n      4\n      4\n      4\n      4\n      10\n      4\n      2\n      2\n      2\n      2\n      2\n      2\n      1\n      2\n      2\n      2\n      2\n      2\n    \n    \n      1342\n      5\n      5\n      5\n      5\n      6\n      5\n      4\n      10\n      3\n      2\n      3\n      3\n      2\n      2\n      2\n      2\n      2\n      3\n      2\n      3\n    \n    \n      1900\n      3\n      3\n      3\n      3\n      3\n      3\n      2\n      3\n      10\n      3\n      4\n      3\n      3\n      3\n      2\n      3\n      3\n      3\n      3\n      3\n    \n    \n      2701\n      2\n      2\n      2\n      2\n      2\n      2\n      2\n      2\n      3\n      10\n      3\n      3\n      3\n      3\n      2\n      3\n      2\n      2\n      2\n      3\n    \n    \n      4045\n      3\n      3\n      3\n      3\n      3\n      2\n      2\n      3\n      4\n      3\n      10\n      3\n      3\n      3\n      2\n      3\n      3\n      3\n      2\n      3\n    \n    \n      8118\n      3\n      3\n      3\n      3\n      3\n      2\n      2\n      3\n      3\n      3\n      3\n      10\n      3\n      3\n      2\n      3\n      2\n      3\n      2\n      3\n    \n    \n      10712\n      2\n      2\n      2\n      2\n      2\n      2\n      2\n      2\n      3\n      3\n      3\n      3\n      10\n      3\n      2\n      3\n      2\n      2\n      2\n      3\n    \n    \n      13720\n      2\n      2\n      2\n      2\n      2\n      2\n      2\n      2\n      3\n      3\n      3\n      3\n      3\n      10\n      3\n      3\n      3\n      2\n      2\n      3\n    \n    \n      13721\n      2\n      2\n      2\n      2\n      2\n      1\n      1\n      2\n      2\n      2\n      2\n      2\n      2\n      3\n      10\n      2\n      2\n      2\n      2\n      2\n    \n    \n      15422\n      3\n      3\n      3\n      2\n      3\n      2\n      2\n      2\n      3\n      3\n      3\n      3\n      3\n      3\n      2\n      10\n      3\n      3\n      2\n      3\n    \n    \n      15859\n      2\n      2\n      2\n      2\n      2\n      2\n      2\n      2\n      3\n      2\n      3\n      2\n      2\n      3\n      2\n      3\n      10\n      2\n      2\n      2\n    \n    \n      21816\n      3\n      3\n      3\n      3\n      3\n      2\n      2\n      3\n      3\n      2\n      3\n      3\n      2\n      2\n      2\n      3\n      2\n      10\n      2\n      3\n    \n    \n      34970\n      3\n      3\n      2\n      2\n      3\n      2\n      2\n      2\n      3\n      2\n      2\n      2\n      2\n      2\n      2\n      2\n      2\n      2\n      10\n      3\n    \n    \n      53861\n      3\n      3\n      3\n      3\n      3\n      2\n      2\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n      2\n      3\n      2\n      3\n      3\n      10\n    \n  \n\n\n\n\nSORT = CORR_MATRIX.mean().sort_values(ascending=False).index\n\n\n(CORR_MATRIX.loc[SORT, SORT] * 10).astype('int').style.background_gradient(cmap=colors, axis=None)\n\n\n\n\n  \n    \n      book_id\n      161\n      1342\n      121\n      105\n      141\n      158\n      1900\n      946\n      4045\n      1212\n      8118\n      15422\n      53861\n      21816\n      10712\n      13720\n      34970\n      15859\n      2701\n      13721\n    \n    \n      book_id\n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n  \n  \n    \n      161\n      10\n      6\n      5\n      5\n      5\n      5\n      3\n      4\n      3\n      4\n      3\n      3\n      3\n      3\n      2\n      2\n      3\n      2\n      2\n      2\n    \n    \n      1342\n      6\n      10\n      5\n      5\n      5\n      5\n      3\n      5\n      3\n      4\n      3\n      2\n      3\n      3\n      2\n      2\n      2\n      2\n      2\n      2\n    \n    \n      121\n      5\n      5\n      10\n      5\n      5\n      5\n      3\n      4\n      3\n      4\n      3\n      3\n      3\n      3\n      2\n      2\n      3\n      2\n      2\n      2\n    \n    \n      105\n      5\n      5\n      5\n      10\n      5\n      5\n      3\n      4\n      3\n      4\n      3\n      3\n      3\n      3\n      2\n      2\n      3\n      2\n      2\n      2\n    \n    \n      141\n      5\n      5\n      5\n      5\n      10\n      5\n      3\n      4\n      3\n      4\n      3\n      3\n      3\n      3\n      2\n      2\n      2\n      2\n      2\n      2\n    \n    \n      158\n      5\n      5\n      5\n      5\n      5\n      10\n      3\n      4\n      3\n      4\n      3\n      2\n      3\n      3\n      2\n      2\n      2\n      2\n      2\n      2\n    \n    \n      1900\n      3\n      3\n      3\n      3\n      3\n      3\n      10\n      3\n      4\n      2\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n      2\n    \n    \n      946\n      4\n      5\n      4\n      4\n      4\n      4\n      3\n      10\n      2\n      4\n      2\n      2\n      2\n      2\n      2\n      2\n      2\n      2\n      2\n      1\n    \n    \n      4045\n      3\n      3\n      3\n      3\n      3\n      3\n      4\n      2\n      10\n      2\n      3\n      3\n      3\n      3\n      3\n      3\n      2\n      3\n      3\n      2\n    \n    \n      1212\n      4\n      4\n      4\n      4\n      4\n      4\n      2\n      4\n      2\n      10\n      2\n      2\n      2\n      2\n      2\n      2\n      2\n      2\n      2\n      1\n    \n    \n      8118\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n      2\n      3\n      2\n      10\n      3\n      3\n      3\n      3\n      3\n      2\n      2\n      3\n      2\n    \n    \n      15422\n      3\n      2\n      3\n      3\n      3\n      2\n      3\n      2\n      3\n      2\n      3\n      10\n      3\n      3\n      3\n      3\n      2\n      3\n      3\n      2\n    \n    \n      53861\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n      2\n      3\n      2\n      3\n      3\n      10\n      3\n      3\n      3\n      3\n      2\n      3\n      2\n    \n    \n      21816\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n      2\n      3\n      2\n      3\n      3\n      3\n      10\n      2\n      2\n      2\n      2\n      2\n      2\n    \n    \n      10712\n      2\n      2\n      2\n      2\n      2\n      2\n      3\n      2\n      3\n      2\n      3\n      3\n      3\n      2\n      10\n      3\n      2\n      2\n      3\n      2\n    \n    \n      13720\n      2\n      2\n      2\n      2\n      2\n      2\n      3\n      2\n      3\n      2\n      3\n      3\n      3\n      2\n      3\n      10\n      2\n      3\n      3\n      3\n    \n    \n      34970\n      3\n      2\n      3\n      3\n      2\n      2\n      3\n      2\n      2\n      2\n      2\n      2\n      3\n      2\n      2\n      2\n      10\n      2\n      2\n      2\n    \n    \n      15859\n      2\n      2\n      2\n      2\n      2\n      2\n      3\n      2\n      3\n      2\n      2\n      3\n      2\n      2\n      2\n      3\n      2\n      10\n      2\n      2\n    \n    \n      2701\n      2\n      2\n      2\n      2\n      2\n      2\n      3\n      2\n      3\n      2\n      3\n      3\n      3\n      2\n      3\n      3\n      2\n      2\n      10\n      2\n    \n    \n      13721\n      2\n      2\n      2\n      2\n      2\n      2\n      2\n      1\n      2\n      1\n      2\n      2\n      2\n      2\n      2\n      3\n      2\n      2\n      2\n      10\n    \n  \n\n\n\n\nLIB['kendall_sum'] = CORR_MATRIX.sum()\n\n\nax = LIB.plot.scatter('book_len', 'kendall_sum', figsize=(15,15), title=\"Book Length by Kendall Sum\")\nLIB.apply(lambda row: ax.text(row.book_len, row.kendall_sum, f\"  {row.label.title()}\"), axis=1);\n\n\n\n\n\nAUS_IDX = LIB[LIB.author.str.contains(\"AUS\")].index.to_list()\nMEL_IDX = LIB[LIB.author.str.contains(\"MEL\")].index.to_list()\n\n\nCORR_MATRIX.loc[AUS_IDX, AUS_IDX].stack().mean()\n\n0.5775258984968228\n\n\n\nCORR_MATRIX.loc[MEL_IDX, MEL_IDX].stack().mean()\n\n0.3688712364823002"
  },
  {
    "objectID": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#hierarchical",
    "href": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#hierarchical",
    "title": "Similarity and Distance Measures",
    "section": "Hierarchical",
    "text": "Hierarchical\n\nCORR = CORR_MATRIX.stack().to_frame(corr_type).sort_index()\nCORR.index.names = ['doc_a', 'doc_b']\nCORR = CORR.query('doc_a < doc_b')\n\n\nhca(CORR[corr_type], color_thresh=.6);\n\n<Figure size 640x480 with 0 Axes>"
  },
  {
    "objectID": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#why-so-bad-as-clustering",
    "href": "lessons/M06_CusteringSimilarity/M06_01_SimilarityMeasures.html#why-so-bad-as-clustering",
    "title": "Similarity and Distance Measures",
    "section": "Why so bad as clustering?",
    "text": "Why so bad as clustering?\nCorrelation is not a distance measure in vector space.\n\nIt measures the degree of association between two variables.\nIt measure the strength and direction of the relationship between two variables\n\nSo, although both correlation and distance measures concern the relationship between variables, they are fundamentally different concepts and are used for different purposes."
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html",
    "title": "PCA from Scratch",
    "section": "",
    "text": "Set Up"
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#import",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#import",
    "title": "PCA from Scratch",
    "section": "Import",
    "text": "Import\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom scipy.linalg import norm\n\n\nimport plotly_express as px\nimport seaborn as sns\n\n\nsns.set(style='ticks')"
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#config",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#config",
    "title": "PCA from Scratch",
    "section": "Config",
    "text": "Config\n\nimport configparser\nconfig = configparser.ConfigParser()\nconfig.read(\"../env.ini\")\ndata_home = config['DEFAULT']['data_home']\noutput_dir = config['DEFAULT']['output_dir']\n\n\ndata_prefix = 'austen-melville'\n\n\nOHCO = ['book_id', 'chap_num']\n\n\n# Added after class\nnorm_docs = True # This has the effect of exaggerating variance when False\ncenter_term_vectors = True # This has the effect of demoting authorship when False\n\nWe pick a color map for our gradient visualizations. For more info on color maps, see the Matplotlib docs on the subject.\n\n# colors = \"YlGnBu\" \ncolors = \"Spectral\""
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#import-tables",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#import-tables",
    "title": "PCA from Scratch",
    "section": "Import tables",
    "text": "Import tables\n\nLIB = pd.read_csv(f'{output_dir}/{data_prefix}-LIB_FIXED.csv').set_index('book_id')\nVOCAB = pd.read_csv(f'{output_dir}/{data_prefix}-VOCAB2.csv').set_index('term_str')\nBOW = pd.read_csv(f'{output_dir}/{data_prefix}-BOW.csv').set_index(OHCO+['term_str'])\n\n\nVOCAB[['n','p','i']].head(20)\n\n\n\n\n\n  \n    \n      \n      n\n      p\n      i\n    \n    \n      term_str\n      \n      \n      \n    \n  \n  \n    \n      the\n      109921\n      0.053387\n      4.227375\n    \n    \n      of\n      65525\n      0.031824\n      4.973725\n    \n    \n      and\n      62954\n      0.030576\n      5.031473\n    \n    \n      to\n      56271\n      0.027330\n      5.193379\n    \n    \n      a\n      44174\n      0.021455\n      5.542573\n    \n    \n      in\n      36439\n      0.017698\n      5.820287\n    \n    \n      i\n      27280\n      0.013249\n      6.237927\n    \n    \n      was\n      24231\n      0.011769\n      6.408917\n    \n    \n      that\n      23670\n      0.011496\n      6.442711\n    \n    \n      it\n      23299\n      0.011316\n      6.465503\n    \n    \n      his\n      19757\n      0.009596\n      6.703407\n    \n    \n      he\n      18313\n      0.008894\n      6.812902\n    \n    \n      as\n      17735\n      0.008614\n      6.859171\n    \n    \n      with\n      17267\n      0.008386\n      6.897753\n    \n    \n      her\n      16927\n      0.008221\n      6.926444\n    \n    \n      not\n      16745\n      0.008133\n      6.942040\n    \n    \n      for\n      16655\n      0.008089\n      6.949815\n    \n    \n      but\n      16443\n      0.007986\n      6.968297\n    \n    \n      be\n      15379\n      0.007469\n      7.064809\n    \n    \n      you\n      14347\n      0.006968\n      7.165021"
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#generate-reduced-tfidf-table",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#generate-reduced-tfidf-table",
    "title": "PCA from Scratch",
    "section": "Generate reduced TFIDF table",
    "text": "Generate reduced TFIDF table\nWe reduce the feature space of the the TFIDF table by selecting the top 1000 significant nouns, verbs, and adjectives.\nNote that the TFIDF table is an extension of an implied DOC table, where each doc is a chapter in this case.\n\nTFIDF = BOW['tfidf'].unstack(fill_value=0) \nVSHORT = VOCAB[VOCAB.max_pos.isin(['NN', 'VB', 'JJ'])].sort_values('dfidf', ascending=False).head(1000)\nTFIDF = TFIDF[VSHORT.index]\n\n\nTFIDF\n\n\n\n\n\n  \n    \n      \n      term_str\n      hands\n      sure\n      short\n      new\n      word\n      passed\n      fine\n      name\n      given\n      sight\n      ...\n      mad\n      legs\n      letters\n      intelligence\n      importance\n      friendly\n      language\n      bringing\n      beheld\n      news\n    \n    \n      book_id\n      chap_num\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      105\n      1\n      0.000551\n      0.000000\n      0.000000\n      0.001107\n      0.000553\n      0.000545\n      0.000555\n      0.001088\n      0.002224\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.001281\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n    \n    \n      2\n      0.000732\n      0.000000\n      0.001457\n      0.000000\n      0.000736\n      0.000000\n      0.000000\n      0.000000\n      0.000739\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.003408\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n    \n    \n      3\n      0.000509\n      0.001019\n      0.000507\n      0.000000\n      0.000512\n      0.000000\n      0.000000\n      0.003523\n      0.000514\n      0.000514\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.001186\n      0.0\n      0.0\n    \n    \n      4\n      0.000000\n      0.000000\n      0.002391\n      0.000000\n      0.000805\n      0.000000\n      0.000807\n      0.000791\n      0.001617\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.001856\n      0.001864\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n    \n    \n      5\n      0.000435\n      0.001305\n      0.000000\n      0.000874\n      0.000874\n      0.000000\n      0.000438\n      0.000000\n      0.000439\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.002025\n      0.001013\n      0.0\n      0.000000\n      0.0\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      53861\n      8\n      0.000632\n      0.000000\n      0.001258\n      0.002541\n      0.000635\n      0.000000\n      0.000000\n      0.001249\n      0.000000\n      0.001277\n      ...\n      0.002931\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n    \n    \n      11\n      0.000427\n      0.000853\n      0.000000\n      0.000429\n      0.000429\n      0.000845\n      0.001719\n      0.000422\n      0.000000\n      0.000431\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n    \n    \n      13\n      0.000663\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000668\n      0.000000\n      0.000000\n      0.000669\n      ...\n      0.000000\n      0.001537\n      0.001537\n      0.000000\n      0.000000\n      0.001543\n      0.0\n      0.000000\n      0.0\n      0.0\n    \n    \n      14\n      0.000000\n      0.000518\n      0.000000\n      0.000000\n      0.002082\n      0.000000\n      0.000000\n      0.000000\n      0.000523\n      0.000523\n      ...\n      0.000000\n      0.002402\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n    \n    \n      15\n      0.000575\n      0.000575\n      0.001717\n      0.001734\n      0.002312\n      0.000000\n      0.000000\n      0.001705\n      0.000581\n      0.000581\n      ...\n      0.000000\n      0.004000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n    \n  \n\n1122 rows × 1000 columns"
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#add-some-labels-to-lib",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#add-some-labels-to-lib",
    "title": "PCA from Scratch",
    "section": "Add some labels to LIB",
    "text": "Add some labels to LIB\n\ngenre_csv = \"\"\"\n105, domestic, romance\n121, gothic, satire\n141, domestic, satire\n158, domestic, romance\n161, domestic, romance\n946, domestic, romance\n1212, domestic, satire\n1342, domestic, romance\n1900, travel, adventure\n2701, sea, adventure\n4045, travel, adventure\n8118, sea, adventure\n10712, sea, critical\n13720, sea, adventure\n13721, sea, adventure\n15422, historical, adventure\n15859, mixed, mixed\n21816, travel, satire\n34970, gothic, psychological\n53861, mixed, mixed\n\"\"\".split('\\n')[1:-1]\ngenre = pd.DataFrame([line.split(', ') for line in genre_csv], columns=['book_id','genre', 'mode'])\ngenre.book_id = genre.book_id.astype('int')\ngenre = genre.set_index('book_id')\n\n\nLIB = pd.concat([LIB, genre], axis=1)\n\n\nLIB_COLS = ['author','title', 'label', 'genre', 'mode']\n\n\nLIB[LIB_COLS].head()\n\n\n\n\n\n  \n    \n      \n      author\n      title\n      label\n      genre\n      mode\n    \n    \n      book_id\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      105\n      AUSTEN, JANE\n      PERSUASION\n      Austen: Persuasion (105)\n      domestic\n      romance\n    \n    \n      121\n      AUSTEN, JANE\n      NORTHANGER ABBEY\n      Austen: Northanger Abbey (121)\n      gothic\n      satire\n    \n    \n      141\n      AUSTEN, JANE\n      MANSFIELD PARK\n      Austen: Mansfield Park (141)\n      domestic\n      satire\n    \n    \n      158\n      AUSTEN, JANE\n      EMMA\n      Austen: Emma (158)\n      domestic\n      romance\n    \n    \n      161\n      AUSTEN, JANE\n      SENSE AND SENSIBILITY\n      Austen: Sense And Sensibilit (161)\n      domestic\n      romance"
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#normalize-doc-vector-lengths",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#normalize-doc-vector-lengths",
    "title": "PCA from Scratch",
    "section": "Normalize doc vector lengths",
    "text": "Normalize doc vector lengths\nWe use L2 normalization, which scales documents by their pythagorean (Euclidean) length.\n\n# TFIDF_L2 = (TFIDF.T / np.sqrt(np.square(TFIDF).sum(axis=1))).T\nTFIDF_L2 = (TFIDF.T / norm(TFIDF, 2, axis=1)).T"
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#normalize-term-vector-variance",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#normalize-term-vector-variance",
    "title": "PCA from Scratch",
    "section": "Normalize term vector variance",
    "text": "Normalize term vector variance\nWe do not normalize variance, which we would normally do, such as with data containing divergent units of measure.\nThis is because to do so would exaggerate the importance of rare words (see Ng, 2008: 6m40s–8m00s)."
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#center-the-term-vectors",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#center-the-term-vectors",
    "title": "PCA from Scratch",
    "section": "Center the term vectors",
    "text": "Center the term vectors\nWe can take the column-wise means (the means for the term vectors), but this alters the cosine angles between terms.\n\nTFIDF_L2 = TFIDF_L2 - TFIDF_L2.mean()"
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#compute-covariance-matrix",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#compute-covariance-matrix",
    "title": "PCA from Scratch",
    "section": "Compute Covariance Matrix",
    "text": "Compute Covariance Matrix\n\\(n = |X| = |Y|\\)\n\\(Cov(X,Y) = \\dfrac{\\sum_{i=1}^{n} (x_i - \\mu_X) (y_i - \\mu_Y)}{n - 1} = \\dfrac{XY}{n-1}\\)\nWe could we use the built in Pandas method here, but compute it ourselves.\n\n# COV = TFIDF_L2.cov() # This also centers the vectors\nCOV = TFIDF_L2.T.dot(TFIDF_L2) / (TFIDF_L2.shape[0] - 1)\n\n\nCOV.head()\n\n\n\n\n\n  \n    \n      term_str\n      hands\n      sure\n      short\n      new\n      word\n      passed\n      fine\n      name\n      given\n      sight\n      ...\n      mad\n      legs\n      letters\n      intelligence\n      importance\n      friendly\n      language\n      bringing\n      beheld\n      news\n    \n    \n      term_str\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      hands\n      0.000805\n      -0.000100\n      -0.000017\n      -0.000014\n      0.000003\n      0.000022\n      0.000030\n      0.000023\n      -2.732542e-07\n      0.000065\n      ...\n      0.000047\n      0.000051\n      0.000011\n      0.000011\n      -0.000011\n      0.000021\n      0.000014\n      -2.854973e-05\n      0.000059\n      -0.000022\n    \n    \n      sure\n      -0.000100\n      0.000961\n      0.000081\n      -0.000010\n      0.000066\n      0.000036\n      0.000009\n      0.000049\n      7.265544e-05\n      -0.000031\n      ...\n      -0.000054\n      -0.000059\n      0.000095\n      0.000014\n      0.000105\n      0.000018\n      0.000034\n      2.794903e-05\n      -0.000056\n      0.000167\n    \n    \n      short\n      -0.000017\n      0.000081\n      0.000392\n      -0.000014\n      0.000050\n      0.000041\n      0.000024\n      0.000018\n      6.882225e-05\n      -0.000011\n      ...\n      -0.000030\n      0.000046\n      -0.000006\n      0.000012\n      0.000040\n      0.000013\n      0.000017\n      2.484538e-05\n      -0.000052\n      0.000006\n    \n    \n      new\n      -0.000014\n      -0.000010\n      -0.000014\n      0.000984\n      -0.000003\n      0.000027\n      0.000097\n      0.000039\n      -1.666918e-05\n      0.000047\n      ...\n      -0.000007\n      0.000040\n      -0.000010\n      0.000004\n      0.000009\n      0.000016\n      0.000046\n      -8.511709e-07\n      0.000017\n      -0.000039\n    \n    \n      word\n      0.000003\n      0.000066\n      0.000050\n      -0.000003\n      0.000481\n      0.000014\n      0.000003\n      0.000069\n      4.732417e-05\n      -0.000008\n      ...\n      -0.000032\n      0.000016\n      0.000083\n      0.000034\n      0.000026\n      0.000017\n      -0.000001\n      -3.192503e-06\n      -0.000023\n      0.000005\n    \n  \n\n5 rows × 1000 columns"
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#decompose-the-matrix",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#decompose-the-matrix",
    "title": "PCA from Scratch",
    "section": "Decompose the Matrix",
    "text": "Decompose the Matrix\nThere a at least three options to choose from. We go with SciPy’s Hermitian Eigendecomposition\nmethod eigh(), since our covarience matrix is symmetric.\n\nfrom scipy.linalg import eigh\n\n\neig_vals, eig_vecs = eigh(COV)"
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#convert-eigen-data-to-dataframes",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#convert-eigen-data-to-dataframes",
    "title": "PCA from Scratch",
    "section": "Convert eigen data to dataframes",
    "text": "Convert eigen data to dataframes\n\nEIG_VEC = pd.DataFrame(eig_vecs, index=COV.index, columns=COV.index)\nEIG_VAL = pd.DataFrame(eig_vals, index=COV.index, columns=['eig_val'])\nEIG_VAL.index.name = 'term_str'\n\n\nEIG_VEC.iloc[:10, :10].style.background_gradient(cmap=colors)\n\n\n\n\n  \n    \n      term_str\n      hands\n      sure\n      short\n      new\n      word\n      passed\n      fine\n      name\n      given\n      sight\n    \n    \n      term_str\n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n  \n  \n    \n      hands\n      -0.037357\n      -0.086434\n      -0.005275\n      -0.028545\n      -0.018669\n      0.050672\n      0.033771\n      -0.037018\n      -0.025691\n      0.012285\n    \n    \n      sure\n      0.030053\n      0.004899\n      -0.020445\n      0.035813\n      -0.005917\n      -0.024594\n      0.005718\n      0.054559\n      0.008752\n      -0.013220\n    \n    \n      short\n      -0.017095\n      -0.009361\n      -0.001474\n      -0.035726\n      0.000287\n      0.031975\n      -0.054997\n      0.072142\n      0.092958\n      -0.038365\n    \n    \n      new\n      -0.032639\n      -0.002786\n      0.016319\n      0.004140\n      0.016414\n      0.002177\n      0.036716\n      -0.008850\n      0.004647\n      0.005246\n    \n    \n      word\n      -0.034299\n      0.016029\n      0.037630\n      0.011503\n      0.000075\n      0.012121\n      -0.060358\n      -0.088147\n      -0.015519\n      -0.056855\n    \n    \n      passed\n      -0.013132\n      0.031293\n      0.007022\n      0.028543\n      -0.002376\n      0.014404\n      -0.042055\n      0.037728\n      -0.022936\n      0.047324\n    \n    \n      fine\n      0.013986\n      0.016279\n      0.024518\n      -0.002167\n      -0.030947\n      0.013857\n      0.022277\n      0.004101\n      0.027578\n      0.014207\n    \n    \n      name\n      -0.051162\n      0.029167\n      -0.047833\n      0.000951\n      -0.023632\n      -0.017940\n      -0.020133\n      -0.039737\n      -0.029629\n      0.033401\n    \n    \n      given\n      0.018993\n      0.101092\n      -0.001056\n      0.006974\n      0.022330\n      -0.023598\n      0.010857\n      -0.031301\n      -0.014293\n      0.030360\n    \n    \n      sight\n      -0.002532\n      0.004883\n      0.026964\n      -0.050948\n      0.011343\n      -0.043752\n      0.021245\n      -0.083468\n      0.012253\n      -0.013611"
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#explore-term-pairs",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#explore-term-pairs",
    "title": "PCA from Scratch",
    "section": "Explore Term Pairs",
    "text": "Explore Term Pairs\n\nEIG_VEC_PAIRS = EIG_VEC.stack().sort_values(ascending=False).to_frame('covariance')\nEIG_VEC_PAIRS.index.names = ['term1', 'term2']\n\n\n# EIG_VEC_PAIRS['label'] = EIG_VEC_PAIRS.apply(lambda x: '-'.join(x.name), axis=1)\n\n\nEIG_VEC_PAIRS.head(20)\n\n\n\n\n\n  \n    \n      \n      \n      covariance\n    \n    \n      term1\n      term2\n      \n    \n  \n  \n    \n      pierre\n      beheld\n      0.828247\n    \n    \n      whale\n      bringing\n      0.767416\n    \n    \n      fish\n      addressed\n      0.611777\n    \n    \n      sir\n      uncle\n      0.556264\n    \n    \n      ye\n      doctor\n      0.541244\n    \n    \n      fish\n      last\n      0.527156\n    \n    \n      captain\n      alive\n      0.473727\n    \n    \n      thou\n      friendly\n      0.470904\n    \n    \n      wine\n      cry\n      0.448556\n    \n    \n      mr\n      legs\n      0.372105\n    \n    \n      letters\n      0.368976\n    \n    \n      lord\n      importance\n      0.361999\n    \n    \n      doctor\n      anxiety\n      0.352016\n    \n    \n      captain\n      anxiety\n      0.348810\n    \n    \n      thee\n      friendly\n      0.343885\n    \n    \n      ye\n      anxiety\n      0.338947\n    \n    \n      mr\n      beach\n      0.334831\n    \n    \n      miss\n      increased\n      0.318014\n    \n    \n      mrs\n      worthy\n      0.311291\n    \n    \n      whales\n      bringing\n      0.310884\n    \n  \n\n\n\n\n\n# px.scatter(EIG_VEC_PAIRS.sample(10000).sort_values('covariance'), y='label', x='covariance', height=1000)\n\n\nEIG_VEC_PAIRS.sample(10000).sort_values('covariance', ascending=False).plot(rot=45, style='.', figsize=(10,5));"
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#select-principal-components",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#select-principal-components",
    "title": "PCA from Scratch",
    "section": "Select Principal Components",
    "text": "Select Principal Components\nNext, we associate each eigenvalue with its corresponding column in the eigenvalue matrix.\nThis is why we transpose the EIG_VEC dataframe."
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#combine-eigenvalues-and-eignvectors",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#combine-eigenvalues-and-eignvectors",
    "title": "PCA from Scratch",
    "section": "Combine eigenvalues and eignvectors",
    "text": "Combine eigenvalues and eignvectors\n\nEIG_PAIRS = EIG_VAL.join(EIG_VEC.T)\n\n\nEIG_PAIRS.sort_values('eig_val', ascending=False).head(10)\n\n\n\n\n\n  \n    \n      \n      eig_val\n      hands\n      sure\n      short\n      new\n      word\n      passed\n      fine\n      name\n      given\n      ...\n      mad\n      legs\n      letters\n      intelligence\n      importance\n      friendly\n      language\n      bringing\n      beheld\n      news\n    \n    \n      term_str\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      news\n      0.058514\n      0.025464\n      -0.067036\n      -0.019332\n      0.007172\n      -0.014322\n      -0.012414\n      0.001326\n      -0.003741\n      -0.025355\n      ...\n      0.019839\n      0.017280\n      -0.023452\n      -0.014126\n      -0.022254\n      -0.007510\n      -0.007685\n      -0.006531\n      0.015825\n      -0.022272\n    \n    \n      beheld\n      0.041807\n      -0.020866\n      -0.003918\n      -0.012903\n      -0.011347\n      -0.005595\n      -0.004632\n      -0.011046\n      -0.008214\n      -0.005514\n      ...\n      -0.006155\n      -0.017599\n      0.008154\n      0.000701\n      -0.003032\n      -0.005168\n      -0.005517\n      -0.006073\n      -0.007949\n      -0.004304\n    \n    \n      bringing\n      0.021089\n      -0.011235\n      0.007565\n      -0.008887\n      -0.018115\n      -0.002006\n      -0.012843\n      -0.020190\n      -0.016949\n      -0.005980\n      ...\n      -0.005983\n      -0.018892\n      0.003813\n      0.001474\n      0.001985\n      -0.007477\n      -0.008055\n      -0.007372\n      0.002504\n      0.000376\n    \n    \n      language\n      0.018494\n      -0.016695\n      -0.005318\n      -0.016437\n      0.007463\n      -0.010396\n      -0.000396\n      -0.010676\n      -0.012665\n      -0.014995\n      ...\n      0.013770\n      0.004438\n      -0.008724\n      -0.003128\n      -0.012323\n      0.001028\n      -0.000311\n      -0.003498\n      0.017253\n      -0.008014\n    \n    \n      friendly\n      0.014653\n      0.022959\n      0.020740\n      -0.003929\n      -0.023905\n      0.014753\n      0.001149\n      -0.019546\n      0.000154\n      0.003641\n      ...\n      0.028587\n      0.001534\n      0.001759\n      -0.011353\n      0.004259\n      -0.014967\n      -0.012810\n      -0.002653\n      -0.018950\n      0.010852\n    \n    \n      importance\n      0.010784\n      -0.002610\n      0.003244\n      -0.023678\n      -0.014752\n      -0.010093\n      -0.037056\n      -0.001699\n      -0.006622\n      0.001409\n      ...\n      0.037514\n      -0.010118\n      0.008527\n      -0.012733\n      0.002179\n      -0.012624\n      -0.010170\n      -0.010502\n      -0.026189\n      0.003022\n    \n    \n      intelligence\n      0.010105\n      -0.011341\n      -0.017860\n      -0.021591\n      -0.009454\n      -0.027353\n      0.014510\n      -0.010530\n      -0.016920\n      0.005669\n      ...\n      -0.001917\n      -0.027058\n      -0.001864\n      0.015417\n      0.011523\n      -0.009159\n      0.001370\n      0.012411\n      0.028743\n      -0.001243\n    \n    \n      letters\n      0.009265\n      0.018864\n      0.025726\n      -0.005846\n      0.017307\n      0.023400\n      0.018551\n      0.031843\n      -0.013151\n      -0.025529\n      ...\n      0.015587\n      0.022367\n      -0.000566\n      -0.014205\n      -0.013140\n      0.006111\n      -0.026389\n      -0.003718\n      0.009759\n      0.006359\n    \n    \n      legs\n      0.007981\n      -0.002146\n      -0.040698\n      -0.027483\n      -0.032146\n      -0.029445\n      -0.021648\n      -0.030951\n      -0.023475\n      -0.009824\n      ...\n      -0.004969\n      -0.011260\n      -0.021345\n      -0.011769\n      -0.016637\n      -0.009403\n      -0.015491\n      -0.011390\n      0.003570\n      -0.013889\n    \n    \n      mad\n      0.007616\n      0.004930\n      0.024524\n      -0.008227\n      -0.007031\n      -0.015488\n      0.018952\n      -0.047057\n      -0.021748\n      0.000693\n      ...\n      0.033498\n      -0.011225\n      0.010583\n      0.003850\n      0.004006\n      -0.009216\n      -0.004459\n      0.002056\n      0.033520\n      0.007615\n    \n  \n\n10 rows × 1001 columns\n\n\n\nNext, we sort in descending order and pick the top K (=10)."
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#compute-and-show-explained-variance",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#compute-and-show-explained-variance",
    "title": "PCA from Scratch",
    "section": "Compute and Show Explained Variance",
    "text": "Compute and Show Explained Variance\nWe might have usd this value to sort our components.\n\nEIG_PAIRS['exp_var'] = np.round((EIG_PAIRS.eig_val / EIG_PAIRS.eig_val.sum()) * 100, 2)\n\n\nEIG_PAIRS.exp_var.sort_values(ascending=False).head().plot.bar(rot=45);"
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#pick-top-k-10-components",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#pick-top-k-10-components",
    "title": "PCA from Scratch",
    "section": "Pick Top K (10) Components",
    "text": "Pick Top K (10) Components\nWe pick these based on explained variance.\n\nCOMPS = EIG_PAIRS.sort_values('exp_var', ascending=False).head(10).reset_index(drop=True)\nCOMPS.index.name = 'comp_id'\nCOMPS.index = [\"PC{}\".format(i) for i in COMPS.index.tolist()]\nCOMPS.index.name = 'pc_id'\n\n\nCOMPS\n\n\n\n\n\n  \n    \n      \n      eig_val\n      hands\n      sure\n      short\n      new\n      word\n      passed\n      fine\n      name\n      given\n      ...\n      legs\n      letters\n      intelligence\n      importance\n      friendly\n      language\n      bringing\n      beheld\n      news\n      exp_var\n    \n    \n      pc_id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      PC0\n      0.058514\n      0.025464\n      -0.067036\n      -0.019332\n      0.007172\n      -0.014322\n      -0.012414\n      0.001326\n      -0.003741\n      -0.025355\n      ...\n      0.017280\n      -0.023452\n      -0.014126\n      -0.022254\n      -0.007510\n      -0.007685\n      -0.006531\n      0.015825\n      -0.022272\n      6.76\n    \n    \n      PC1\n      0.041807\n      -0.020866\n      -0.003918\n      -0.012903\n      -0.011347\n      -0.005595\n      -0.004632\n      -0.011046\n      -0.008214\n      -0.005514\n      ...\n      -0.017599\n      0.008154\n      0.000701\n      -0.003032\n      -0.005168\n      -0.005517\n      -0.006073\n      -0.007949\n      -0.004304\n      4.83\n    \n    \n      PC2\n      0.021089\n      -0.011235\n      0.007565\n      -0.008887\n      -0.018115\n      -0.002006\n      -0.012843\n      -0.020190\n      -0.016949\n      -0.005980\n      ...\n      -0.018892\n      0.003813\n      0.001474\n      0.001985\n      -0.007477\n      -0.008055\n      -0.007372\n      0.002504\n      0.000376\n      2.44\n    \n    \n      PC3\n      0.018494\n      -0.016695\n      -0.005318\n      -0.016437\n      0.007463\n      -0.010396\n      -0.000396\n      -0.010676\n      -0.012665\n      -0.014995\n      ...\n      0.004438\n      -0.008724\n      -0.003128\n      -0.012323\n      0.001028\n      -0.000311\n      -0.003498\n      0.017253\n      -0.008014\n      2.14\n    \n    \n      PC4\n      0.014653\n      0.022959\n      0.020740\n      -0.003929\n      -0.023905\n      0.014753\n      0.001149\n      -0.019546\n      0.000154\n      0.003641\n      ...\n      0.001534\n      0.001759\n      -0.011353\n      0.004259\n      -0.014967\n      -0.012810\n      -0.002653\n      -0.018950\n      0.010852\n      1.69\n    \n    \n      PC5\n      0.010784\n      -0.002610\n      0.003244\n      -0.023678\n      -0.014752\n      -0.010093\n      -0.037056\n      -0.001699\n      -0.006622\n      0.001409\n      ...\n      -0.010118\n      0.008527\n      -0.012733\n      0.002179\n      -0.012624\n      -0.010170\n      -0.010502\n      -0.026189\n      0.003022\n      1.25\n    \n    \n      PC6\n      0.010105\n      -0.011341\n      -0.017860\n      -0.021591\n      -0.009454\n      -0.027353\n      0.014510\n      -0.010530\n      -0.016920\n      0.005669\n      ...\n      -0.027058\n      -0.001864\n      0.015417\n      0.011523\n      -0.009159\n      0.001370\n      0.012411\n      0.028743\n      -0.001243\n      1.17\n    \n    \n      PC7\n      0.009265\n      0.018864\n      0.025726\n      -0.005846\n      0.017307\n      0.023400\n      0.018551\n      0.031843\n      -0.013151\n      -0.025529\n      ...\n      0.022367\n      -0.000566\n      -0.014205\n      -0.013140\n      0.006111\n      -0.026389\n      -0.003718\n      0.009759\n      0.006359\n      1.07\n    \n    \n      PC8\n      0.007981\n      -0.002146\n      -0.040698\n      -0.027483\n      -0.032146\n      -0.029445\n      -0.021648\n      -0.030951\n      -0.023475\n      -0.009824\n      ...\n      -0.011260\n      -0.021345\n      -0.011769\n      -0.016637\n      -0.009403\n      -0.015491\n      -0.011390\n      0.003570\n      -0.013889\n      0.92\n    \n    \n      PC9\n      0.007616\n      0.004930\n      0.024524\n      -0.008227\n      -0.007031\n      -0.015488\n      0.018952\n      -0.047057\n      -0.021748\n      0.000693\n      ...\n      -0.011225\n      0.010583\n      0.003850\n      0.004006\n      -0.009216\n      -0.004459\n      0.002056\n      0.033520\n      0.007615\n      0.88\n    \n  \n\n10 rows × 1002 columns\n\n\n\n\n# COMPS.iloc[:,1:-1].T.sort_values('PC1', ascending=False).head(10).index"
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#see-projected-components-onto-vocabulary-loadings",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#see-projected-components-onto-vocabulary-loadings",
    "title": "PCA from Scratch",
    "section": "See Projected Components onto Vocabulary (Loadings)",
    "text": "See Projected Components onto Vocabulary (Loadings)\nLoadings sow the contribution of each term to the component.\nWe’ll just look at the topi 10 words for the first two components in the Book version.\n\nLOADINGS = COMPS[COV.index].T\nLOADINGS.index.name = 'term_str'\n\n\nLOADINGS.head(10).style.background_gradient(cmap=colors)\n\n\n\n\n  \n    \n      pc_id\n      PC0\n      PC1\n      PC2\n      PC3\n      PC4\n      PC5\n      PC6\n      PC7\n      PC8\n      PC9\n    \n    \n      term_str\n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n  \n  \n    \n      hands\n      0.025464\n      -0.020866\n      -0.011235\n      -0.016695\n      0.022959\n      -0.002610\n      -0.011341\n      0.018864\n      -0.002146\n      0.004930\n    \n    \n      sure\n      -0.067036\n      -0.003918\n      0.007565\n      -0.005318\n      0.020740\n      0.003244\n      -0.017860\n      0.025726\n      -0.040698\n      0.024524\n    \n    \n      short\n      -0.019332\n      -0.012903\n      -0.008887\n      -0.016437\n      -0.003929\n      -0.023678\n      -0.021591\n      -0.005846\n      -0.027483\n      -0.008227\n    \n    \n      new\n      0.007172\n      -0.011347\n      -0.018115\n      0.007463\n      -0.023905\n      -0.014752\n      -0.009454\n      0.017307\n      -0.032146\n      -0.007031\n    \n    \n      word\n      -0.014322\n      -0.005595\n      -0.002006\n      -0.010396\n      0.014753\n      -0.010093\n      -0.027353\n      0.023400\n      -0.029445\n      -0.015488\n    \n    \n      passed\n      -0.012414\n      -0.004632\n      -0.012843\n      -0.000396\n      0.001149\n      -0.037056\n      0.014510\n      0.018551\n      -0.021648\n      0.018952\n    \n    \n      fine\n      0.001326\n      -0.011046\n      -0.020190\n      -0.010676\n      -0.019546\n      -0.001699\n      -0.010530\n      0.031843\n      -0.030951\n      -0.047057\n    \n    \n      name\n      -0.003741\n      -0.008214\n      -0.016949\n      -0.012665\n      0.000154\n      -0.006622\n      -0.016920\n      -0.013151\n      -0.023475\n      -0.021748\n    \n    \n      given\n      -0.025355\n      -0.005514\n      -0.005980\n      -0.014995\n      0.003641\n      0.001409\n      0.005669\n      -0.025529\n      -0.009824\n      0.000693\n    \n    \n      sight\n      0.016960\n      -0.020856\n      -0.009140\n      -0.005342\n      -0.003083\n      -0.046244\n      0.006083\n      0.028484\n      -0.008082\n      0.021153\n    \n  \n\n\n\n\ntop_terms = []\nfor i in range(10):\n    for j in [0, 1]:\n        comp_str = ' '.join(LOADINGS.sort_values(f'PC{i}', ascending=bool(j)).head(10).index.to_list())\n        top_terms.append((f\"PC{i}\", j, comp_str))\nCOMP_GLOSS = pd.DataFrame(top_terms).set_index([0,1]).unstack()\nCOMP_GLOSS.index.name = 'comp_id'\nCOMP_GLOSS.columns = COMP_GLOSS.columns.droplevel(0) \nCOMP_GLOSS = COMP_GLOSS.rename(columns={0:'pos', 1:'neg'})\n\n\nCOMP_GLOSS\n\n\n\n\n\n  \n    \n      1\n      pos\n      neg\n    \n    \n      comp_id\n      \n      \n    \n  \n  \n    \n      PC0\n      pierre whale thou sea ship ye thee deck thy boat\n      mr mrs miss lady am sister dear think sir letter\n    \n    \n      PC1\n      pierre thou thee thy mother sister brother sou...\n      whale captain ship deck sea boat sailors ships...\n    \n    \n      PC2\n      whale whales thou mr boat fish boats thee mrs ye\n      captain sailors doctor war deck officers mate ...\n    \n    \n      PC3\n      lord thou king thy thee ye kings wine cried oh\n      pierre captain whale ship deck sailors boat ma...\n    \n    \n      PC4\n      thou thee thy captain ye mr deck sir ship mate\n      island king lord pierre whale doctor fish king...\n    \n    \n      PC5\n      lord war king sir officers captain american ki...\n      doctor thou thee boat island trees house thy b...\n    \n    \n      PC6\n      mr mrs sea war king land miss officers island ...\n      sir dont doctor confidence ye yes said strange...\n    \n    \n      PC7\n      mr miss lord pierre mrs ye cried said king dont\n      lady sir thou letter thee mother daughter affe...\n    \n    \n      PC8\n      mr sir deck dont top yard confidence doctor ma...\n      captain lord king whale thou sister miss islan...\n    \n    \n      PC9\n      boat ye mother sail boats letter sea calm sist...\n      mr doctor thou war thee american officers thy ...\n    \n  \n\n\n\n\n\n# COMPS.join(COMP_GLOSS)"
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#project-docs-onto-components",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#project-docs-onto-components",
    "title": "PCA from Scratch",
    "section": "Project Docs onto Components",
    "text": "Project Docs onto Components\nWe get the dot product of the DTM matrix and the new component matrix, which we will call DCM – for document-component matrix.\nThis has the effect of replacing the features of the DTM with the features of the transposed component matrix.\n\n# We use the index to get only PC features, so matrices align\nDCM = TFIDF_L2.dot(COMPS[COV.index].T) \n\nWe add metadata to our new, reduced matrices for display purposes.\n\nDCM = DCM.join(LIB[LIB_COLS], on='book_id')\n\nWe define a doc field to name each chapter.\n\nDCM['doc'] = DCM.apply(lambda x: f\"{x.label} {str(x.name[1]).zfill(2)}\", 1)\n\n\nDCM.doc\n\nbook_id  chap_num\n105      1                         Austen: Persuasion (105) 01\n         2                         Austen: Persuasion (105) 02\n         3                         Austen: Persuasion (105) 03\n         4                         Austen: Persuasion (105) 04\n         5                         Austen: Persuasion (105) 05\n                                       ...                    \n53861    8           Melville: The Apple Tree Table (53861) 08\n         11          Melville: The Apple Tree Table (53861) 11\n         13          Melville: The Apple Tree Table (53861) 13\n         14          Melville: The Apple Tree Table (53861) 14\n         15          Melville: The Apple Tree Table (53861) 15\nName: doc, Length: 1122, dtype: object\n\n\n\nDCM\n\n\n\n\n\n  \n    \n      \n      \n      PC0\n      PC1\n      PC2\n      PC3\n      PC4\n      PC5\n      PC6\n      PC7\n      PC8\n      PC9\n      author\n      title\n      label\n      genre\n      mode\n      doc\n    \n    \n      book_id\n      chap_num\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      105\n      1\n      -0.258198\n      0.021857\n      0.000676\n      -0.006438\n      -0.050156\n      0.108198\n      -0.047005\n      -0.273663\n      0.130826\n      -0.027749\n      AUSTEN, JANE\n      PERSUASION\n      Austen: Persuasion (105)\n      domestic\n      romance\n      Austen: Persuasion (105) 01\n    \n    \n      2\n      -0.282111\n      0.005754\n      0.010752\n      -0.012096\n      0.004779\n      0.083151\n      -0.098515\n      -0.264169\n      0.172837\n      -0.034563\n      AUSTEN, JANE\n      PERSUASION\n      Austen: Persuasion (105)\n      domestic\n      romance\n      Austen: Persuasion (105) 02\n    \n    \n      3\n      -0.333652\n      -0.025880\n      -0.005081\n      -0.016205\n      0.099866\n      0.145003\n      -0.114721\n      -0.000025\n      0.229109\n      -0.145779\n      AUSTEN, JANE\n      PERSUASION\n      Austen: Persuasion (105)\n      domestic\n      romance\n      Austen: Persuasion (105) 03\n    \n    \n      4\n      -0.218828\n      0.001516\n      -0.024165\n      -0.036815\n      -0.065350\n      0.019598\n      -0.002876\n      -0.279571\n      -0.070277\n      0.039148\n      AUSTEN, JANE\n      PERSUASION\n      Austen: Persuasion (105)\n      domestic\n      romance\n      Austen: Persuasion (105) 04\n    \n    \n      5\n      -0.418764\n      -0.008377\n      -0.007638\n      -0.041823\n      0.008903\n      -0.042988\n      -0.024161\n      -0.013665\n      -0.049183\n      -0.029242\n      AUSTEN, JANE\n      PERSUASION\n      Austen: Persuasion (105)\n      domestic\n      romance\n      Austen: Persuasion (105) 05\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      53861\n      8\n      0.011858\n      -0.035620\n      -0.073109\n      0.089403\n      -0.063246\n      0.021552\n      -0.157739\n      0.064319\n      -0.049673\n      -0.007093\n      MELVILLE, HERMAN\n      THE APPLE TREE TABLE AND OTHER SKETCHES\n      Melville: The Apple Tree Table (53861)\n      mixed\n      mixed\n      Melville: The Apple Tree Table (53861) 08\n    \n    \n      11\n      -0.031019\n      -0.029827\n      -0.059515\n      0.075617\n      -0.017630\n      0.012006\n      -0.267711\n      -0.006523\n      0.072311\n      -0.002350\n      MELVILLE, HERMAN\n      THE APPLE TREE TABLE AND OTHER SKETCHES\n      Melville: The Apple Tree Table (53861)\n      mixed\n      mixed\n      Melville: The Apple Tree Table (53861) 11\n    \n    \n      13\n      0.067225\n      -0.066853\n      -0.087447\n      0.158183\n      -0.103171\n      0.104258\n      -0.171316\n      0.082089\n      0.007237\n      -0.042221\n      MELVILLE, HERMAN\n      THE APPLE TREE TABLE AND OTHER SKETCHES\n      Melville: The Apple Tree Table (53861)\n      mixed\n      mixed\n      Melville: The Apple Tree Table (53861) 13\n    \n    \n      14\n      0.044617\n      -0.017264\n      -0.012534\n      0.064447\n      -0.025153\n      0.024341\n      -0.093316\n      -0.044480\n      0.113007\n      0.046377\n      MELVILLE, HERMAN\n      THE APPLE TREE TABLE AND OTHER SKETCHES\n      Melville: The Apple Tree Table (53861)\n      mixed\n      mixed\n      Melville: The Apple Tree Table (53861) 14\n    \n    \n      15\n      0.145190\n      -0.178561\n      -0.150977\n      -0.141539\n      0.004961\n      0.041952\n      -0.029813\n      -0.034090\n      -0.115813\n      -0.123482\n      MELVILLE, HERMAN\n      THE APPLE TREE TABLE AND OTHER SKETCHES\n      Melville: The Apple Tree Table (53861)\n      mixed\n      mixed\n      Melville: The Apple Tree Table (53861) 15\n    \n  \n\n1122 rows × 16 columns"
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#pc-0-and-1",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#pc-0-and-1",
    "title": "PCA from Scratch",
    "section": "PC 0 and 1",
    "text": "PC 0 and 1\n\nAuthor\n\nvis_pcs(DCM, 0, 1)\n\n\n                                                \n\n\n\n\nLoadings\n\nvis_loadings(0,1)\n\n\n                                                \n\n\n\nvis_pcs(DCM, 0, 1, label='label')\n\n\n                                                \n\n\n\n\nGenre\n\nvis_pcs(DCM, 0, 1, label='genre')\n\n\n                                                \n\n\n\n\nMode\n\nvis_pcs(DCM, 0, 1, label='mode')\n\n\n                                                \n\n\n\n\nTitle\n\nvis_pcs(DCM, 0, 1, label='label')"
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#pc-1-and-2",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#pc-1-and-2",
    "title": "PCA from Scratch",
    "section": "PC 1 and 2",
    "text": "PC 1 and 2\n\nAuthor\n\nvis_pcs(DCM, 1, 2)\n\n\n                                                \n\n\n\n\nLoadings\n\nvis_loadings(1, 2)\n\n\n                                                \n\n\n\n\nGenre\n\nvis_pcs(DCM, 1, 2, label='genre')\n\n\n                                                \n\n\n\n\nMode\n\nvis_pcs(DCM, 1, 2, label='mode')\n\n\n                                                \n\n\n\n\nTitle\n\nvis_pcs(DCM, 1, 2, label='label')"
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#pc-2-and-3",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#pc-2-and-3",
    "title": "PCA from Scratch",
    "section": "PC 2 and 3",
    "text": "PC 2 and 3\n\nAuthor\n\nvis_pcs(DCM, 2, 3)\n\n\n                                                \n\n\n\n\nLoadings\n\nvis_loadings(2, 3)\n\n\n                                                \n\n\n\n\nGenre\n\nvis_pcs(DCM, 2, 3, label='genre')\n\n\n                                                \n\n\n\n\nMode\n\nvis_pcs(DCM, 2, 3, label='mode', symbol='author')\n\n\n                                                \n\n\n\n\nTitle\n\nvis_pcs(DCM, 2, 3, label='label')"
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#pc-3-and-4",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#pc-3-and-4",
    "title": "PCA from Scratch",
    "section": "PC 3 and 4",
    "text": "PC 3 and 4\n\nAuthor\n\nvis_pcs(DCM, 3, 4, label='author')\n\n\n                                                \n\n\n\n\nLoadings\n\nvis_loadings(3, 4)\n\n\n                                                \n\n\n\n\nGenre\n\nvis_pcs(DCM, 3, 4, label='genre')\n\n\n                                                \n\n\n\n\nMode\n\nvis_pcs(DCM, 3, 4, label='mode')\n\n\n                                                \n\n\n\n\nTitle\n\nvis_pcs(DCM, 3, 4, label='label')"
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#dendrograms",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#dendrograms",
    "title": "PCA from Scratch",
    "section": "Dendrograms",
    "text": "Dendrograms\n\nimport plotly.figure_factory as ff\nfrom plotly.figure_factory import create_dendrogram # Built on top SciPy\n\n\nimport sys\nsys.path.append(\"../lib/\")\nfrom hac import HAC\n\n\nX = DCM[COMPS.index].groupby('book_id').mean()\ntitles = LIB.loc[list(X.index)].apply(lambda x: x.label + ' ' + x.genre.upper(), 1)\nX.index = titles\n\n\nX.style.background_gradient(cmap=colors)\n\n\n\n\n  \n    \n       \n      PC0\n      PC1\n      PC2\n      PC3\n      PC4\n      PC5\n      PC6\n      PC7\n      PC8\n      PC9\n    \n  \n  \n    \n      Austen: Persuasion (105) DOMESTIC\n      -0.337338\n      -0.030630\n      -0.031834\n      -0.082466\n      0.068222\n      0.022072\n      -0.023854\n      -0.044779\n      -0.102746\n      -0.029806\n    \n    \n      Austen: Northanger Abbey (121) GOTHIC\n      -0.311267\n      0.002747\n      0.000296\n      -0.007682\n      -0.003312\n      -0.048675\n      -0.018886\n      0.012589\n      -0.077055\n      0.017762\n    \n    \n      Austen: Mansfield Park (141) DOMESTIC\n      -0.393960\n      0.004976\n      0.014748\n      -0.019171\n      0.025015\n      -0.018458\n      -0.040241\n      -0.006563\n      -0.034164\n      0.017252\n    \n    \n      Austen: Emma (158) DOMESTIC\n      -0.437672\n      0.013328\n      0.053743\n      -0.013375\n      0.054108\n      -0.001813\n      0.065007\n      0.135890\n      0.042653\n      -0.052688\n    \n    \n      Austen: Sense And Sensibilit (161) DOMESTIC\n      -0.345483\n      0.018129\n      0.018381\n      0.000060\n      -0.017634\n      -0.022766\n      0.005883\n      -0.063928\n      -0.078032\n      0.088286\n    \n    \n      Austen: Lady Susan (946) DOMESTIC\n      -0.219796\n      0.028187\n      0.039862\n      0.023409\n      -0.025045\n      0.073707\n      0.033773\n      -0.151976\n      0.096857\n      0.045405\n    \n    \n      Austen: Love And Freindship  (1212) DOMESTIC\n      -0.157251\n      0.025022\n      0.010777\n      0.048865\n      -0.051134\n      0.054717\n      -0.005936\n      -0.156109\n      0.027730\n      0.061192\n    \n    \n      Austen: Pride And Prejudice (1342) DOMESTIC\n      -0.393163\n      0.021152\n      0.039614\n      -0.010023\n      0.022737\n      0.013272\n      0.062141\n      0.019623\n      0.039693\n      -0.017768\n    \n    \n      Melville: Typee A Romance Of T (1900) TRAVEL\n      0.050602\n      -0.113090\n      -0.086828\n      -0.011318\n      -0.124808\n      -0.209410\n      0.028849\n      -0.041354\n      -0.079963\n      -0.024421\n    \n    \n      Melville: Moby Dick Or The Wha (2701) SEA\n      0.207722\n      -0.116362\n      0.246548\n      -0.044811\n      0.045002\n      -0.003460\n      -0.032640\n      0.011940\n      0.002617\n      0.009036\n    \n    \n      Melville: Omoo Adventures In T (4045) TRAVEL\n      0.113730\n      -0.131832\n      -0.122581\n      -0.044993\n      -0.053661\n      -0.115615\n      -0.013449\n      -0.001140\n      -0.002051\n      -0.084682\n    \n    \n      Melville: Redburn His First Vo (8118) SEA\n      0.111456\n      -0.137905\n      -0.118752\n      -0.084791\n      0.029049\n      -0.032181\n      -0.022607\n      0.028245\n      -0.010472\n      0.016600\n    \n    \n      Melville: White Jacket Or The  (10712) SEA\n      0.163040\n      -0.150442\n      -0.144829\n      -0.118246\n      0.074115\n      0.137773\n      0.056299\n      -0.018316\n      0.020970\n      -0.032055\n    \n    \n      Melville: Mardi And A Voyage T (13720) SEA\n      0.155868\n      -0.065011\n      -0.011586\n      0.096270\n      -0.047143\n      -0.034296\n      0.069926\n      0.001371\n      0.024956\n      0.042478\n    \n    \n      Melville: Mardi And A Voyage T (13721) SEA\n      0.152284\n      -0.010575\n      -0.012132\n      0.265305\n      -0.023752\n      0.073684\n      0.032188\n      0.055779\n      -0.044970\n      0.022121\n    \n    \n      Melville: Israel Potter His Fi (15422) HISTORICAL\n      0.090946\n      -0.107954\n      -0.099394\n      -0.005492\n      -0.027475\n      -0.040343\n      -0.082576\n      0.004041\n      0.009294\n      -0.035022\n    \n    \n      Melville: The Piazza Tales (15859) MIXED\n      0.211265\n      -0.234142\n      -0.116719\n      -0.081839\n      -0.039766\n      -0.225599\n      0.063152\n      0.063479\n      -0.148360\n      0.087199\n    \n    \n      Melville: The Confidence Man H (21816) TRAVEL\n      -0.000728\n      -0.034659\n      -0.040667\n      0.060716\n      -0.055736\n      0.016295\n      -0.242591\n      -0.054343\n      0.082842\n      -0.020222\n    \n    \n      Melville: Pierre Or The Ambigu (34970) GOTHIC\n      0.183238\n      0.530211\n      -0.015613\n      -0.075967\n      -0.006886\n      -0.007649\n      -0.001682\n      0.014112\n      -0.009297\n      -0.004343\n    \n    \n      Melville: The Apple Tree Table (53861) MIXED\n      0.019533\n      -0.059372\n      -0.079978\n      0.054460\n      -0.057489\n      -0.004625\n      -0.137826\n      0.017834\n      -0.006835\n      -0.042148\n    \n  \n\n\n\n\nHAC(X).plot();\n\n<Figure size 640x480 with 0 Axes>\n\n\n\n\n\n\n# fig = create_dendrogram(X, labels=X.index, orientation='left', color_threshold=.3)\n# fig.update_layout(width=1000, height=800)\n# fig.show()\n\n\nXV = LOADINGS.loc[VSHORT.sort_values('dfidf', ascending=False).head(100).index]\n\n\nHAC(XV).plot()\n\n<Figure size 640x480 with 0 Axes>\n\n\n\n\n\n\n# fig = create_dendrogram(XV, labels=XV.index, orientation='left', color_threshold=.05)\n# fig.update_layout(width=1000, height=1200)\n# fig.show()\n\n\nXV2 = LOADINGS.loc[VSHORT.sample(50).index]\n\n\nHAC(XV2).plot()\n\n<Figure size 640x480 with 0 Axes>\n\n\n\n\n\n\n# fig = create_dendrogram(XV2, labels=XV2.index, orientation='left', color_threshold=.05)\n# fig.update_layout(width=1000, height=1200)\n# fig.show()"
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#pc-0-and-1-1",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#pc-0-and-1-1",
    "title": "PCA from Scratch",
    "section": "PC 0 and 1",
    "text": "PC 0 and 1\n\nvis_pcs(DCM_sk, 0, 1)"
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#pc-1-and-2-1",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#pc-1-and-2-1",
    "title": "PCA from Scratch",
    "section": "PC 1 and 2",
    "text": "PC 1 and 2\n\nvis_pcs(DCM_sk, 1, 2)"
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#pc-2-and-3-1",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#pc-2-and-3-1",
    "title": "PCA from Scratch",
    "section": "PC 2 and 3",
    "text": "PC 2 and 3\n\nvis_pcs(DCM_sk, 2, 3)\n\n\n                                                \n\n\n\n# px.scatter_3d(DCM_sk, 'PC0', 'PC1','PC2', color='author', hover_name='doc', height=1000, width=1200)\n\n\n# px.scatter_3d(DCM_sk, 'PC0', 'PC1','PC2', color='mode', hover_name='doc', hover_data=['genre'], symbol='author', height=1000, width=1200)"
  },
  {
    "objectID": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#get-loadings",
    "href": "lessons/M07_FeaturesAndComponents/M07_01_PCA.html#get-loadings",
    "title": "PCA from Scratch",
    "section": "Get Loadings",
    "text": "Get Loadings\n\nLOADINGS_sk = pd.DataFrame(pca_engine.components_.T * np.sqrt(pca_engine.explained_variance_))\nLOADINGS_sk.columns = [\"PC{}\".format(i) for i in LOADINGS_sk.columns]\n\n\nLOADINGS_sk.index = TFIDF_L2.columns\nLOADINGS_sk.index.name = 'term_str'\n\n\nLOADINGS.sort_values('PC0').head(10)\n\n\n\n\n\n  \n    \n      pc_id\n      PC0\n      PC1\n      PC2\n      PC3\n      PC4\n      PC5\n      PC6\n      PC7\n      PC8\n      PC9\n    \n    \n      term_str\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      mr\n      -0.359128\n      0.019165\n      0.098625\n      -0.057492\n      0.169153\n      0.047867\n      0.221916\n      0.368976\n      0.372105\n      -0.303542\n    \n    \n      mrs\n      -0.316631\n      0.035972\n      0.068743\n      -0.048707\n      0.086132\n      -0.024039\n      0.179579\n      0.160556\n      -0.076624\n      0.067634\n    \n    \n      miss\n      -0.262562\n      0.022949\n      0.057432\n      -0.030832\n      0.098568\n      -0.050132\n      0.083151\n      0.283403\n      -0.097490\n      -0.038719\n    \n    \n      lady\n      -0.123531\n      0.009982\n      0.010485\n      -0.016548\n      0.001285\n      0.043660\n      0.023335\n      -0.185016\n      0.054639\n      0.042653\n    \n    \n      am\n      -0.115588\n      0.014959\n      0.031036\n      0.043617\n      0.037739\n      0.064141\n      -0.073542\n      -0.014117\n      -0.020349\n      0.064700\n    \n    \n      sister\n      -0.106057\n      0.041650\n      0.017279\n      -0.022183\n      0.020500\n      -0.007084\n      0.015023\n      -0.064137\n      -0.102880\n      0.099418\n    \n    \n      dear\n      -0.103157\n      0.021991\n      0.022068\n      0.008442\n      0.018862\n      0.039353\n      -0.049509\n      -0.045827\n      0.021539\n      0.063086\n    \n    \n      think\n      -0.084971\n      -0.000624\n      0.010928\n      -0.001211\n      0.028707\n      0.015273\n      -0.073419\n      0.028306\n      -0.052887\n      0.014157\n    \n    \n      sir\n      -0.079848\n      -0.003705\n      -0.010291\n      -0.009192\n      0.142197\n      0.158333\n      -0.344740\n      -0.176467\n      0.267253\n      -0.035865\n    \n    \n      letter\n      -0.079789\n      0.028131\n      0.020864\n      -0.015864\n      0.002925\n      0.039368\n      0.010996\n      -0.136351\n      -0.057243\n      0.113951\n    \n  \n\n\n\n\n\n# LOADINGS_sk['term_str'] = LOADINGS_sk.apply(lambda x: VOCAB.loc[x.name], 1)\n\n\ntop_terms_sk= {}\nfor i in [0, 1]:\n    for j in [0, 1]:\n      top_terms_sk[f\"{i}_{j}\"] = ' '.join(LOADINGS_sk.sort_values(f'PC{i}', ascending=bool(j)).head(10).index.to_list())\n\n\ntop_terms_sk\n\n{'0_0': 'mr mrs miss lady am sister dear think sir letter',\n '0_1': 'pierre whale thou sea ship ye thee deck thy boat',\n '1_0': 'pierre thou thee thy mother sister brother soul mrs sweet',\n '1_1': 'whale captain ship deck sea boat sailors ships whales mate'}"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_01_GibbsSampler.html",
    "href": "lessons/M08_TopicModels/M08_01_GibbsSampler.html",
    "title": "Gibbs Sampler",
    "section": "",
    "text": "Setup\nWe want to convert any given F1 corpus (DOC) into unannotated TOKEN and VOCAB tables.\nThis is so we can work with ad hoc training data.\nWe sample each document and word combination in the BOW table. In each case, we are looking for two values:\nWe combine these values in order to align the label of the current word with the rest of the data.\nIf a the topic is highly associated with both the word and the document, then that topic will get a high value.\nNote that all that is going on here is a sorting operation – the random assignment does not predict anything.\nInstead, we are just gathering words under topics and topics under documents.\nWe use a toy example to see if the method works.\nBecause our codd is not vert efficient, we just"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_01_GibbsSampler.html#data",
    "href": "lessons/M08_TopicModels/M08_01_GibbsSampler.html#data",
    "title": "Gibbs Sampler",
    "section": "Data",
    "text": "Data\nA small F1 corpus.\n\nraw_docs = \"\"\"\nI ate a banana and a spinach smoothie for breakfast.\nI like to eat broccoli and bananas.\nChinchillas and kittens are cute.\nMy sister adopted a kitten yesterday.\nLook at this cute hamster munching on a piece of broccoli.\n\"\"\".split(\"\\n\")[1:-1]"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_01_GibbsSampler.html#process",
    "href": "lessons/M08_TopicModels/M08_01_GibbsSampler.html#process",
    "title": "Gibbs Sampler",
    "section": "Process",
    "text": "Process\n\ncp1, tm1 = do_all(raw_docs, k=5, iters=500)\n\n100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:19<00:00, 25.32it/s]\n\n\n\ntm1.TOPIC\n\n\n\n\n\n  \n    \n      \n      top_terms\n      n\n    \n    \n      topic_id\n      \n      \n    \n  \n  \n    \n      T0\n      breakfast broccoli cute yesterday\n      4\n    \n    \n      T1\n      ate cute kittens piece smoothie\n      5\n    \n    \n      T2\n      adopted broccoli eat kitten sister spinach\n      6\n    \n    \n      T3\n      bananas chinchillas like munching\n      4\n    \n    \n      T4\n      banana hamster look\n      3\n    \n  \n\n\n\n\n\ncp1.DOC.join(tm1.THETA.astype('int')).style.background_gradient(axis=None)\n\n\n\n\n  \n    \n       \n      doc_str\n      T0\n      T1\n      T2\n      T3\n      T4\n    \n    \n      doc_id\n       \n       \n       \n       \n       \n       \n    \n  \n  \n    \n      0\n      I ate a banana and a spinach smoothie for breakfast.\n      1\n      2\n      1\n      0\n      1\n    \n    \n      1\n      I like to eat broccoli and bananas.\n      1\n      0\n      1\n      2\n      0\n    \n    \n      2\n      Chinchillas and kittens are cute.\n      1\n      1\n      0\n      1\n      0\n    \n    \n      3\n      My sister adopted a kitten yesterday.\n      1\n      0\n      3\n      0\n      0\n    \n    \n      4\n      Look at this cute hamster munching on a piece of broccoli.\n      0\n      2\n      1\n      1\n      2"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_01_GibbsSampler.html#data-1",
    "href": "lessons/M08_TopicModels/M08_01_GibbsSampler.html#data-1",
    "title": "Gibbs Sampler",
    "section": "Data",
    "text": "Data\n\nsome_documents = [\n    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n    [\"statistics\", \"R\", \"statsmodels\"],\n    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n    [\"pandas\", \"R\", \"Python\"],\n    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n    [\"libsvm\", \"regression\", \"support vector machines\"]\n]\nraw_docs2  = [' '.join(item) for item in some_documents]"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_01_GibbsSampler.html#process-1",
    "href": "lessons/M08_TopicModels/M08_01_GibbsSampler.html#process-1",
    "title": "Gibbs Sampler",
    "section": "Process",
    "text": "Process\n\ncp2, tm2 = do_all(raw_docs2, k=10, iters=500)\n\n100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [01:13<00:00,  6.76it/s]\n\n\n\ntm2.TOPIC\n\n\n\n\n\n  \n    \n      \n      top_terms\n      n\n    \n    \n      topic_id\n      \n      \n    \n  \n  \n    \n      T00\n      big data hbase java languages learning postgre...\n      11\n    \n    \n      T01\n      data databases libsvm neural python support\n      6\n    \n    \n      T02\n      c decision deep mysql statsmodels storm vector\n      9\n    \n    \n      T03\n      artificial haskell neural probability python s...\n      6\n    \n    \n      T04\n      deep learning machine networks pandas postgres r\n      7\n    \n    \n      T05\n      hadoop machines mapreduce nosql probability sc...\n      6\n    \n    \n      T06\n      data hbase java learning mathematics networks ...\n      12\n    \n    \n      T07\n      big cassandra hbase learning machine mongodb s...\n      8\n    \n    \n      T08\n      artificial cassandra hadoop libsvm mongodb spa...\n      7\n    \n    \n      T09\n      intelligence java mahout numpy python r regres...\n      10\n    \n  \n\n\n\n\n\ncp2.DOC.join(tm2.THETA.astype('int')).style.background_gradient(axis=None)\n\n\n\n\n  \n    \n       \n      doc_str\n      T00\n      T01\n      T02\n      T03\n      T04\n      T05\n      T06\n      T07\n      T08\n      T09\n    \n    \n      doc_id\n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n  \n  \n    \n      0\n      Hadoop Big Data HBase Java Spark Storm Cassandra\n      1\n      1\n      1\n      0\n      0\n      0\n      1\n      2\n      2\n      0\n    \n    \n      1\n      NoSQL MongoDB Cassandra HBase Postgres\n      1\n      0\n      0\n      0\n      0\n      1\n      1\n      1\n      1\n      0\n    \n    \n      2\n      Python scikit-learn scipy numpy statsmodels pandas\n      0\n      0\n      1\n      1\n      1\n      0\n      0\n      0\n      0\n      3\n    \n    \n      3\n      R Python statistics regression probability\n      0\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      1\n    \n    \n      4\n      machine learning regression decision trees libsvm\n      0\n      0\n      1\n      0\n      0\n      0\n      1\n      2\n      2\n      0\n    \n    \n      5\n      Python R Java C++ Haskell programming languages\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n      0\n      3\n    \n    \n      6\n      statistics probability mathematics theory\n      2\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      7\n      machine learning scikit-learn Mahout neural networks\n      0\n      0\n      0\n      1\n      3\n      1\n      0\n      0\n      0\n      1\n    \n    \n      8\n      neural networks deep learning Big Data artificial intelligence\n      1\n      1\n      1\n      1\n      0\n      0\n      3\n      0\n      0\n      1\n    \n    \n      9\n      Hadoop Java MapReduce Big Data\n      2\n      0\n      0\n      0\n      0\n      2\n      0\n      1\n      0\n      0\n    \n    \n      10\n      statistics R statsmodels\n      0\n      0\n      1\n      0\n      0\n      0\n      1\n      1\n      0\n      0\n    \n    \n      11\n      C++ deep learning artificial intelligence probability\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      1\n    \n    \n      12\n      pandas R Python\n      0\n      0\n      0\n      1\n      0\n      0\n      2\n      0\n      0\n      0\n    \n    \n      13\n      databases HBase Postgres MySQL MongoDB\n      1\n      1\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n    \n    \n      14\n      libsvm regression support vector machines\n      1\n      2\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      0"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html",
    "href": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html",
    "title": "LDA with SciKit Learn",
    "section": "",
    "text": "Set Up\nWe run Scikit Learn’s LatentDirichletAllocation algorithm and extract the THETA and PHI tables.\nUse the LIB table to get author info.\nCreate a short label for each author for display purposes.\nAdd mean topic weight for each topic by author.\nView by topic order.\nSort by Melville’s top topics.\nSort by Austen’s top topics."
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html#imports",
    "href": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html#imports",
    "title": "LDA with SciKit Learn",
    "section": "Imports",
    "text": "Imports\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA\nimport plotly_express as px"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html#config",
    "href": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html#config",
    "title": "LDA with SciKit Learn",
    "section": "Config",
    "text": "Config\n\nimport configparser\nconfig = configparser.ConfigParser()\nconfig.read('../env.ini')\ndata_home = config['DEFAULT']['data_home']\noutput_dir = config['DEFAULT']['output_dir']\n\n\ndata_prefix = 'austen-melville'\ncolors = \"YlGnBu\"\n\n\nngram_range = (1, 2)\nn_terms = 4000\nn_topics = 40\n# n_topics = 20\nmax_iter = 20\nn_top_terms = 9\n\n\nOHCO = ['book_id', 'chap_num', 'para_num']\nPARA = OHCO[:3]\nCHAP = OHCO[:2]\nBOOK = OHCO[:1]\n\n\nBAG = CHAP"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html#pragmas",
    "href": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html#pragmas",
    "title": "LDA with SciKit Learn",
    "section": "Pragmas",
    "text": "Pragmas\n\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html#convert-tokens-back-to-docs",
    "href": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html#convert-tokens-back-to-docs",
    "title": "LDA with SciKit Learn",
    "section": "Convert TOKENS back to DOCS",
    "text": "Convert TOKENS back to DOCS\nScikit Learn wants an F1 style corpus. We create onefrom our annotated TOKEN table, keeping only regular nouns.\n\nTOKENS = pd.read_csv(f'{output_dir}/{data_prefix}-TOKEN2.csv')\n\n\nTOKENS.head()\n\n\n\n\n\n  \n    \n      \n      book_id\n      chap_num\n      para_num\n      sent_num\n      token_num\n      pos_tuple\n      pos\n      token_str\n      term_str\n    \n  \n  \n    \n      0\n      158\n      1\n      1\n      0\n      0\n      ('Emma', 'NNP')\n      NNP\n      Emma\n      emma\n    \n    \n      1\n      158\n      1\n      1\n      0\n      1\n      ('Woodhouse,', 'NNP')\n      NNP\n      Woodhouse,\n      woodhouse\n    \n    \n      2\n      158\n      1\n      1\n      0\n      2\n      ('handsome,', 'NN')\n      NN\n      handsome,\n      handsome\n    \n    \n      3\n      158\n      1\n      1\n      0\n      3\n      ('clever,', 'NN')\n      NN\n      clever,\n      clever\n    \n    \n      4\n      158\n      1\n      1\n      0\n      4\n      ('and', 'CC')\n      CC\n      and\n      and\n    \n  \n\n\n\n\n\nBAG\n\n['book_id', 'chap_num']"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html#filter-for-nouns",
    "href": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html#filter-for-nouns",
    "title": "LDA with SciKit Learn",
    "section": "Filter for Nouns",
    "text": "Filter for Nouns\n\nDOCS = TOKENS[TOKENS.pos.str.match(r'^NNS?$')]\\\n    .groupby(BAG).term_str\\\n    .apply(lambda x: ' '.join(x))\\\n    .to_frame()\\\n    .rename(columns={'term_str':'doc_str'})\n\n\nDOCS.head()\n\n\n\n\n\n  \n    \n      \n      \n      doc_str\n    \n    \n      book_id\n      chap_num\n      \n    \n  \n  \n    \n      105\n      1\n      man who amusement book occupation hour consola...\n    \n    \n      2\n      civil lawyer hold views anybody else hint refe...\n    \n    \n      3\n      i leave morning newspaper juncture favour peac...\n    \n    \n      4\n      curate appearances brother commander consequen...\n    \n    \n      5\n      morning walk way opportunity them meeting part..."
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html#create-vector-space",
    "href": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html#create-vector-space",
    "title": "LDA with SciKit Learn",
    "section": "Create Vector Space",
    "text": "Create Vector Space\nWe use Scikit Learn’s CountVectorizer to convert our F1 corpus of paragraphs into a document-term vector space of word counts.\n\ncount_engine = CountVectorizer(max_features=n_terms, ngram_range=ngram_range, stop_words='english')\ncount_model = count_engine.fit_transform(DOCS.doc_str)\nTERMS = count_engine.get_feature_names_out()\n\n\nVOCAB = pd.DataFrame(index=TERMS)\nVOCAB.index.name = 'term_str'\n\n\nDTM = pd.DataFrame(count_model.toarray(), index=DOCS.index, columns=TERMS)\n\n\n# DTM\n\n\nVOCAB['doc_count'] = DTM.astype('bool').astype('int').sum()\nDOCS['term_count'] = DTM.sum(1)\n\n\n# VOCAB\n\n\nDOCS.term_count.describe()\n\ncount    1122.000000\nmean      278.416221\nstd       273.096844\nmin         5.000000\n25%       140.250000\n50%       231.000000\n75%       351.750000\nmax      4481.000000\nName: term_count, dtype: float64"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html#topic-names",
    "href": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html#topic-names",
    "title": "LDA with SciKit Learn",
    "section": "Topic Names",
    "text": "Topic Names\n\nTNAMES = [f\"T{str(x).zfill(len(str(n_topics)))}\" for x in range(n_topics)]\n\n\n# TNAMES"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html#theta",
    "href": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html#theta",
    "title": "LDA with SciKit Learn",
    "section": "THETA",
    "text": "THETA\n\nlda_model = lda_engine.fit_transform(count_model)\n\n\nTHETA = pd.DataFrame(lda_model, index=DOCS.index)\nTHETA.columns.name = 'topic_id'\nTHETA.columns = TNAMES\n\n\nTHETA.sample(20).style.background_gradient(cmap=colors, axis=None)\n\n\n\n\n  \n    \n       \n       \n      T00\n      T01\n      T02\n      T03\n      T04\n      T05\n      T06\n      T07\n      T08\n      T09\n      T10\n      T11\n      T12\n      T13\n      T14\n      T15\n      T16\n      T17\n      T18\n      T19\n      T20\n      T21\n      T22\n      T23\n      T24\n      T25\n      T26\n      T27\n      T28\n      T29\n      T30\n      T31\n      T32\n      T33\n      T34\n      T35\n      T36\n      T37\n      T38\n      T39\n    \n    \n      book_id\n      chap_num\n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n  \n  \n    \n      946\n      7\n      0.000263\n      0.000263\n      0.000263\n      0.000263\n      0.000263\n      0.089625\n      0.000263\n      0.000263\n      0.000263\n      0.000263\n      0.000263\n      0.000263\n      0.000263\n      0.000263\n      0.000263\n      0.000263\n      0.000263\n      0.000263\n      0.000263\n      0.000263\n      0.350168\n      0.190557\n      0.000263\n      0.000263\n      0.000263\n      0.000263\n      0.000263\n      0.000263\n      0.000263\n      0.000263\n      0.000263\n      0.000263\n      0.000263\n      0.000263\n      0.000263\n      0.000263\n      0.000263\n      0.000263\n      0.360176\n      0.000263\n    \n    \n      161\n      9\n      0.000091\n      0.000091\n      0.000091\n      0.000091\n      0.000091\n      0.000091\n      0.000091\n      0.162656\n      0.000091\n      0.000091\n      0.059650\n      0.000091\n      0.000091\n      0.000091\n      0.000091\n      0.000091\n      0.000091\n      0.000091\n      0.000091\n      0.000091\n      0.159524\n      0.166741\n      0.000091\n      0.000091\n      0.000091\n      0.000091\n      0.000091\n      0.000091\n      0.000091\n      0.021659\n      0.000091\n      0.000091\n      0.000091\n      0.000091\n      0.000091\n      0.000091\n      0.000091\n      0.000091\n      0.426691\n      0.000091\n    \n    \n      13721\n      42\n      0.000138\n      0.000138\n      0.000138\n      0.000138\n      0.000138\n      0.000138\n      0.000138\n      0.000138\n      0.000138\n      0.061525\n      0.000138\n      0.000138\n      0.000138\n      0.000138\n      0.593351\n      0.000138\n      0.000138\n      0.136264\n      0.000138\n      0.000138\n      0.000138\n      0.000138\n      0.000138\n      0.000138\n      0.000138\n      0.000138\n      0.126630\n      0.000138\n      0.000138\n      0.000138\n      0.000138\n      0.000138\n      0.000138\n      0.000138\n      0.000138\n      0.000138\n      0.000138\n      0.077395\n      0.000138\n      0.000138\n    \n    \n      946\n      35\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.965179\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n      0.000893\n    \n    \n      4045\n      38\n      0.000082\n      0.000082\n      0.000082\n      0.000082\n      0.000082\n      0.000082\n      0.000082\n      0.057632\n      0.000082\n      0.000082\n      0.099106\n      0.000082\n      0.078775\n      0.026834\n      0.000082\n      0.000082\n      0.000082\n      0.000082\n      0.000082\n      0.000082\n      0.000082\n      0.000082\n      0.000082\n      0.019744\n      0.000082\n      0.000082\n      0.000082\n      0.000082\n      0.000082\n      0.000082\n      0.000082\n      0.000082\n      0.000082\n      0.000082\n      0.648239\n      0.000082\n      0.000082\n      0.000082\n      0.066964\n      0.000082\n    \n    \n      10712\n      47\n      0.000120\n      0.000120\n      0.000120\n      0.000120\n      0.000120\n      0.000120\n      0.000120\n      0.000120\n      0.070798\n      0.000120\n      0.000120\n      0.000120\n      0.000120\n      0.000120\n      0.009716\n      0.176842\n      0.000120\n      0.000120\n      0.000120\n      0.000120\n      0.000120\n      0.000120\n      0.000120\n      0.000120\n      0.000120\n      0.000120\n      0.000120\n      0.000120\n      0.000120\n      0.225975\n      0.000120\n      0.038027\n      0.000120\n      0.000120\n      0.426456\n      0.000120\n      0.000120\n      0.000120\n      0.048220\n      0.000120\n    \n    \n      34970\n      47\n      0.000071\n      0.000071\n      0.000071\n      0.000071\n      0.000071\n      0.134155\n      0.000071\n      0.000071\n      0.178581\n      0.000071\n      0.000071\n      0.000071\n      0.000071\n      0.000071\n      0.000071\n      0.117543\n      0.000071\n      0.000071\n      0.009746\n      0.029197\n      0.057492\n      0.084491\n      0.000071\n      0.000071\n      0.000071\n      0.000071\n      0.000071\n      0.000071\n      0.000071\n      0.000071\n      0.000071\n      0.000071\n      0.000071\n      0.145303\n      0.000071\n      0.000071\n      0.000071\n      0.241297\n      0.000071\n      0.000071\n    \n    \n      161\n      4\n      0.000080\n      0.000080\n      0.000080\n      0.000080\n      0.000080\n      0.036490\n      0.000080\n      0.000080\n      0.000080\n      0.000080\n      0.000080\n      0.000080\n      0.000080\n      0.000080\n      0.000080\n      0.000080\n      0.000080\n      0.000080\n      0.000080\n      0.000080\n      0.412270\n      0.032985\n      0.000080\n      0.000080\n      0.000080\n      0.000080\n      0.000080\n      0.000080\n      0.000080\n      0.000080\n      0.000080\n      0.000080\n      0.000080\n      0.000080\n      0.000080\n      0.000080\n      0.000080\n      0.000080\n      0.515371\n      0.000080\n    \n    \n      2701\n      81\n      0.000094\n      0.000094\n      0.000094\n      0.000094\n      0.000094\n      0.000094\n      0.000094\n      0.000094\n      0.131608\n      0.000094\n      0.000094\n      0.000094\n      0.292444\n      0.000094\n      0.000094\n      0.000094\n      0.000094\n      0.000094\n      0.000094\n      0.000094\n      0.000094\n      0.000094\n      0.000094\n      0.000094\n      0.000094\n      0.359080\n      0.000094\n      0.000094\n      0.000094\n      0.000094\n      0.213471\n      0.000094\n      0.000094\n      0.000094\n      0.000094\n      0.000094\n      0.000094\n      0.000094\n      0.000094\n      0.000094\n    \n    \n      4045\n      82\n      0.000087\n      0.000087\n      0.000087\n      0.087563\n      0.000087\n      0.000087\n      0.000087\n      0.032739\n      0.000087\n      0.000087\n      0.247579\n      0.000087\n      0.000087\n      0.010801\n      0.000087\n      0.078193\n      0.000087\n      0.000087\n      0.000087\n      0.000087\n      0.000087\n      0.000087\n      0.000087\n      0.000087\n      0.000087\n      0.062747\n      0.000087\n      0.000087\n      0.000087\n      0.000087\n      0.153066\n      0.000087\n      0.000087\n      0.000087\n      0.324544\n      0.000087\n      0.000087\n      0.000087\n      0.000087\n      0.000087\n    \n    \n      21816\n      38\n      0.000255\n      0.000255\n      0.000255\n      0.888826\n      0.000255\n      0.000255\n      0.101480\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n      0.000255\n    \n    \n      2701\n      12\n      0.000043\n      0.000043\n      0.000043\n      0.042469\n      0.000043\n      0.000043\n      0.000043\n      0.000043\n      0.000043\n      0.000043\n      0.000043\n      0.000043\n      0.000043\n      0.000043\n      0.155795\n      0.000043\n      0.000043\n      0.000043\n      0.223584\n      0.000043\n      0.000043\n      0.000043\n      0.000043\n      0.000043\n      0.000043\n      0.081301\n      0.000043\n      0.000043\n      0.006823\n      0.000043\n      0.416236\n      0.000043\n      0.016280\n      0.000043\n      0.021548\n      0.000043\n      0.012025\n      0.022652\n      0.000043\n      0.000043\n    \n    \n      21816\n      5\n      0.000107\n      0.000107\n      0.000107\n      0.881832\n      0.000107\n      0.000107\n      0.000107\n      0.000107\n      0.000107\n      0.000107\n      0.000107\n      0.000107\n      0.000107\n      0.007591\n      0.000107\n      0.000107\n      0.000107\n      0.000107\n      0.000107\n      0.000107\n      0.000107\n      0.000107\n      0.000107\n      0.000107\n      0.000107\n      0.000107\n      0.000107\n      0.000107\n      0.059206\n      0.000107\n      0.000107\n      0.000107\n      0.000107\n      0.000107\n      0.000107\n      0.000107\n      0.000107\n      0.000107\n      0.000107\n      0.047509\n    \n    \n      158\n      32\n      0.000049\n      0.000049\n      0.000049\n      0.000049\n      0.000049\n      0.000049\n      0.000049\n      0.000049\n      0.000049\n      0.000049\n      0.000049\n      0.000049\n      0.000049\n      0.000049\n      0.000049\n      0.000049\n      0.065771\n      0.000049\n      0.000049\n      0.000049\n      0.000049\n      0.123373\n      0.000049\n      0.051141\n      0.000049\n      0.000049\n      0.000049\n      0.000049\n      0.000049\n      0.000049\n      0.000049\n      0.000049\n      0.000049\n      0.000049\n      0.000049\n      0.000049\n      0.000049\n      0.000049\n      0.757950\n      0.000049\n    \n    \n      15422\n      26\n      0.000063\n      0.100834\n      0.000063\n      0.038703\n      0.000063\n      0.083677\n      0.000063\n      0.046820\n      0.000063\n      0.000063\n      0.000063\n      0.000063\n      0.000063\n      0.000063\n      0.441847\n      0.000063\n      0.000063\n      0.012748\n      0.102575\n      0.000063\n      0.000063\n      0.000063\n      0.000063\n      0.000063\n      0.000063\n      0.000063\n      0.000063\n      0.000063\n      0.000063\n      0.013329\n      0.108817\n      0.000063\n      0.000063\n      0.000063\n      0.000063\n      0.000063\n      0.048756\n      0.000063\n      0.000063\n      0.000063\n    \n    \n      34970\n      17\n      0.000124\n      0.068180\n      0.000124\n      0.000124\n      0.000124\n      0.075156\n      0.000124\n      0.094790\n      0.276090\n      0.000124\n      0.000124\n      0.000124\n      0.000124\n      0.000124\n      0.000124\n      0.000124\n      0.000124\n      0.000124\n      0.149269\n      0.000124\n      0.000124\n      0.000124\n      0.000124\n      0.000124\n      0.000124\n      0.000124\n      0.000124\n      0.000124\n      0.061446\n      0.000124\n      0.000124\n      0.000124\n      0.000124\n      0.000124\n      0.000124\n      0.000124\n      0.000124\n      0.270985\n      0.000124\n      0.000124\n    \n    \n      2701\n      7\n      0.000102\n      0.000102\n      0.000102\n      0.046214\n      0.000102\n      0.000102\n      0.087149\n      0.000102\n      0.253690\n      0.000102\n      0.063125\n      0.000102\n      0.000102\n      0.000102\n      0.000102\n      0.000102\n      0.000102\n      0.000102\n      0.000102\n      0.000102\n      0.000102\n      0.000102\n      0.000102\n      0.000102\n      0.000102\n      0.000102\n      0.000102\n      0.000102\n      0.000102\n      0.000102\n      0.179034\n      0.000102\n      0.000102\n      0.000102\n      0.000102\n      0.165189\n      0.121774\n      0.000102\n      0.080573\n      0.000102\n    \n    \n      33\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.695962\n      0.284650\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n      0.000510\n    \n    \n      21816\n      26\n      0.000051\n      0.000051\n      0.000051\n      0.526455\n      0.000051\n      0.038731\n      0.000051\n      0.000051\n      0.000051\n      0.000051\n      0.084686\n      0.000051\n      0.093559\n      0.000051\n      0.091729\n      0.000051\n      0.000051\n      0.000051\n      0.000051\n      0.098244\n      0.064899\n      0.000051\n      0.000051\n      0.000051\n      0.000051\n      0.000051\n      0.000051\n      0.000051\n      0.000051\n      0.000051\n      0.000051\n      0.000051\n      0.000051\n      0.000051\n      0.000051\n      0.000051\n      0.000051\n      0.000051\n      0.000051\n      0.000051\n    \n    \n      2701\n      101\n      0.000136\n      0.000136\n      0.000136\n      0.000136\n      0.000136\n      0.000136\n      0.000136\n      0.000136\n      0.000136\n      0.000136\n      0.000136\n      0.000136\n      0.000136\n      0.000136\n      0.000136\n      0.000136\n      0.000136\n      0.000136\n      0.000136\n      0.000136\n      0.000136\n      0.000136\n      0.000136\n      0.034775\n      0.000136\n      0.318045\n      0.000136\n      0.000136\n      0.000136\n      0.000136\n      0.406954\n      0.000136\n      0.000136\n      0.023483\n      0.138676\n      0.049279\n      0.000136\n      0.024304\n      0.000136\n      0.000136"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html#phi",
    "href": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html#phi",
    "title": "LDA with SciKit Learn",
    "section": "PHI",
    "text": "PHI\n\nPHI = pd.DataFrame(lda_engine.components_, columns=TERMS, index=TNAMES)\nPHI.index.name = 'topic_id'\nPHI.columns.name  = 'term_str'\n\n\nPHI.T.sample(20).style.background_gradient(cmap=colors, axis=None)\n\n\n\n\n  \n    \n      topic_id\n      T00\n      T01\n      T02\n      T03\n      T04\n      T05\n      T06\n      T07\n      T08\n      T09\n      T10\n      T11\n      T12\n      T13\n      T14\n      T15\n      T16\n      T17\n      T18\n      T19\n      T20\n      T21\n      T22\n      T23\n      T24\n      T25\n      T26\n      T27\n      T28\n      T29\n      T30\n      T31\n      T32\n      T33\n      T34\n      T35\n      T36\n      T37\n      T38\n      T39\n    \n    \n      term_str\n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n  \n  \n    \n      embrace\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      1.114971\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      5.452017\n      3.287904\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      1.025000\n      0.025000\n      0.025000\n      3.012196\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      3.257911\n      0.025000\n      0.025000\n    \n    \n      symptoms\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      3.382590\n      0.025000\n      0.025000\n      0.025000\n      9.007312\n      0.025000\n      3.790109\n      0.025000\n      3.126013\n      0.025000\n      4.952831\n      0.025000\n      2.440253\n      0.025000\n      11.463303\n      1.705146\n      0.025000\n      4.027055\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      2.673696\n      0.025000\n      0.025000\n      0.025000\n      2.706692\n    \n    \n      tragedy\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      7.516019\n      0.025000\n      0.025000\n      0.025000\n      4.992964\n      0.025000\n      4.108531\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      1.156838\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      1.350648\n    \n    \n      bloom\n      0.025000\n      9.370408\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      6.724330\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      11.620916\n      0.025000\n      11.174538\n      2.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      3.757612\n      0.025000\n      0.025000\n      6.502197\n      0.025000\n    \n    \n      son\n      0.025000\n      3.270763\n      0.025000\n      0.025000\n      0.025000\n      119.680524\n      2.225584\n      0.025000\n      1.980706\n      4.820222\n      4.480456\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      7.635108\n      10.124464\n      29.014152\n      21.617867\n      6.752591\n      14.752363\n      26.897276\n      5.022748\n      4.521318\n      0.025000\n      8.408252\n      0.025000\n      1.593901\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      6.177165\n      0.025000\n      6.599011\n      0.025000\n      69.925528\n      0.025000\n    \n    \n      attendant\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      2.693550\n      0.025000\n      0.025000\n      0.025000\n      7.912566\n      0.025000\n      1.068714\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      1.240869\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      3.209301\n      0.025000\n    \n    \n      start\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      9.465952\n      0.025000\n      0.025000\n      0.025000\n      0.804311\n      0.025000\n      4.649306\n      0.025000\n      0.025000\n      0.025000\n      11.963310\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      20.089389\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      5.177732\n      0.025000\n      0.025000\n    \n    \n      objections\n      0.025000\n      1.240146\n      0.025000\n      2.102977\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      18.314442\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      1.025000\n      0.025000\n      1.471354\n      0.025000\n      1.052518\n      19.953684\n      3.039880\n    \n    \n      cocoanut\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      1.025000\n      0.025000\n      0.025000\n      0.025000\n      61.540300\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      1.025000\n      0.025000\n      0.025000\n      0.025000\n      2.509700\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n    \n    \n      wheels\n      0.025000\n      9.788020\n      6.843174\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      7.443805\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      2.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n    \n    \n      tin\n      0.025000\n      0.063513\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      4.824762\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      2.227592\n      0.025000\n      0.025231\n      5.514842\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      6.858302\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      19.660759\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n    \n    \n      fishery\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      2.025000\n      0.025000\n      0.025000\n      55.505324\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      1.544676\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n    \n    \n      shot\n      0.025000\n      0.025000\n      2.696324\n      1.771227\n      3.070785\n      1.996653\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      11.791020\n      0.025000\n      15.450478\n      0.025000\n      0.025000\n      1.676142\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      8.598578\n      2.363091\n      0.025000\n      0.025000\n      44.919127\n      31.354051\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      1.286701\n      0.025000\n      1.350824\n    \n    \n      platform\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      1.026891\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      1.547586\n      2.216107\n      0.025000\n      0.025000\n      0.025000\n      3.226010\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      7.051479\n      7.081928\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n    \n    \n      captains\n      0.025000\n      1.135045\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      19.493475\n      0.025000\n      5.404934\n      9.163709\n      0.025000\n      1.303632\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      1.203336\n      0.025000\n      7.671628\n      0.025000\n      1.117773\n      0.025000\n      15.588135\n      2.592852\n      0.025000\n      0.025000\n      0.025000\n      68.102977\n      1.142791\n      9.404712\n      0.025000\n      0.025000\n      0.025000\n    \n    \n      sprung\n      0.025000\n      2.809499\n      0.025000\n      1.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      2.362386\n      0.025000\n      0.025000\n      0.025000\n      1.210455\n      1.956391\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      1.302512\n      0.025000\n      2.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      4.508757\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n    \n    \n      breaker\n      0.025000\n      0.025000\n      0.025000\n      1.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      1.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      21.025000\n    \n    \n      blade\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.098352\n      3.377184\n      0.025000\n      0.025000\n      2.296719\n      0.025000\n      5.068525\n      0.025000\n      0.025000\n      0.025000\n      0.533988\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      3.025000\n      7.988176\n      0.025000\n      4.682128\n      0.025000\n      0.025000\n      1.520852\n      0.025000\n      0.025000\n      0.025000\n      1.813113\n      0.025000\n      9.870963\n      0.025000\n      0.025000\n      0.025000\n    \n    \n      generals\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      23.779960\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      1.108032\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      3.187008\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n    \n    \n      roses\n      0.025000\n      7.384633\n      0.025000\n      0.025000\n      0.025000\n      8.868190\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      1.254452\n      6.674807\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      1.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      0.025000\n      1.114305\n      0.025000\n      1.111849\n      12.766764\n      0.025000\n      0.025000\n      0.025000\n      0.025000"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html#create-topics-and-get-top-terms-per-topic",
    "href": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html#create-topics-and-get-top-terms-per-topic",
    "title": "LDA with SciKit Learn",
    "section": "Create TOPICS and get Top Terms per Topic",
    "text": "Create TOPICS and get Top Terms per Topic\n\nTOPICS = PHI.stack().to_frame('topic_weight').groupby('topic_id')\\\n    .apply(lambda x: x.sort_values('topic_weight', ascending=False)\\\n        .head(n_top_terms).reset_index().drop('topic_id', axis=1)['term_str'])\n\n\nTOPICS\n\n\n\n\n\n  \n    \n      term_str\n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n    \n    \n      topic_id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      T00\n      passengers\n      emigrants\n      steerage\n      cabin\n      mess\n      cook\n      organ\n      water\n      head\n    \n    \n      T01\n      lord\n      man\n      men\n      things\n      wine\n      ha\n      day\n      way\n      ah\n    \n    \n      T02\n      sea\n      air\n      fish\n      round\n      day\n      trees\n      groves\n      water\n      flowers\n    \n    \n      T03\n      man\n      way\n      friend\n      nature\n      sort\n      stranger\n      confidence\n      kind\n      case\n    \n    \n      T04\n      sir\n      herb\n      confidence\n      man\n      herb doctor\n      ugh\n      doctor\n      dont\n      boy\n    \n    \n      T05\n      father\n      mother\n      family\n      years\n      brother\n      time\n      aunt\n      child\n      daughter\n    \n    \n      T06\n      landlord\n      abbey\n      gallery\n      generals\n      harpooneer\n      general\n      man\n      nose\n      bed\n    \n    \n      T07\n      doctor\n      day\n      country\n      village\n      horse\n      morning\n      road\n      house\n      cocoa\n    \n    \n      T08\n      room\n      door\n      house\n      moment\n      time\n      bed\n      night\n      hand\n      street\n    \n    \n      T09\n      gods\n      god\n      image\n      lord\n      idols\n      images\n      wine\n      arms\n      king\n    \n    \n      T10\n      valley\n      natives\n      time\n      island\n      house\n      place\n      islanders\n      feet\n      fruit\n    \n    \n      T11\n      whiteness\n      teeth\n      kings\n      royal\n      milk\n      bull\n      lord\n      item\n      issue\n    \n    \n      T12\n      sea\n      time\n      land\n      water\n      ship\n      air\n      day\n      sight\n      ships\n    \n    \n      T13\n      man\n      canoe\n      time\n      face\n      cook\n      island\n      head\n      sort\n      strangers\n    \n    \n      T14\n      men\n      time\n      world\n      things\n      years\n      people\n      land\n      times\n      man\n    \n    \n      T15\n      captain\n      ship\n      cabin\n      voyage\n      dollars\n      ships\n      man\n      board\n      hand\n    \n    \n      T16\n      thing\n      oh\n      body\n      way\n      day\n      world\n      sort\n      word\n      voice\n    \n    \n      T17\n      kings\n      house\n      king\n      heart\n      mind\n      time\n      life\n      people\n      man\n    \n    \n      T18\n      boy\n      mother\n      room\n      door\n      man\n      guide\n      eyes\n      world\n      hand\n    \n    \n      T19\n      world\n      things\n      soul\n      man\n      heart\n      life\n      love\n      men\n      thing\n    \n    \n      T20\n      time\n      feelings\n      heart\n      mind\n      moment\n      letter\n      happiness\n      subject\n      friend\n    \n    \n      T21\n      man\n      barber\n      gentleman\n      men\n      sort\n      world\n      time\n      life\n      person\n    \n    \n      T22\n      reign\n      time\n      men\n      head\n      day\n      sailor\n      sea\n      country\n      watches\n    \n    \n      T23\n      oh\n      ball\n      sir\n      evening\n      way\n      yes\n      room\n      partner\n      table\n    \n    \n      T24\n      man\n      day\n      time\n      consul\n      boat\n      way\n      ships\n      tailor\n      wives\n    \n    \n      T25\n      whale\n      boat\n      whales\n      ship\n      boats\n      head\n      sea\n      line\n      way\n    \n    \n      T26\n      isle\n      lord\n      king\n      minstrel\n      spears\n      land\n      royal\n      throne\n      shore\n    \n    \n      T27\n      skeleton\n      lord\n      feet\n      bones\n      sea\n      things\n      skull\n      ribs\n      brain\n    \n    \n      T28\n      book\n      thing\n      volume\n      paper\n      books\n      reading\n      work\n      man\n      hand\n    \n    \n      T29\n      man\n      men\n      war\n      gun\n      guns\n      deck\n      jacket\n      man war\n      wars\n    \n    \n      T30\n      sea\n      ship\n      deck\n      night\n      man\n      sail\n      wind\n      time\n      hand\n    \n    \n      T31\n      friendship\n      jacket\n      auction\n      need\n      loan\n      friend\n      bags\n      way\n      goods\n    \n    \n      T32\n      law\n      man\n      court\n      punishment\n      laws\n      code\n      time\n      cases\n      flogging\n    \n    \n      T33\n      guitar\n      head\n      mystery\n      oil\n      girl\n      whale\n      sounds\n      thing\n      hair\n    \n    \n      T34\n      man\n      deck\n      men\n      ship\n      sailors\n      time\n      captain\n      mate\n      sea\n    \n    \n      T35\n      wife\n      chimney\n      table\n      time\n      cock\n      man\n      house\n      room\n      sort\n    \n    \n      T36\n      boots\n      glass\n      place\n      sailors\n      jacket\n      friend\n      sort\n      sailor\n      home\n    \n    \n      T37\n      thou\n      thee\n      art\n      brother\n      oh\n      face\n      eyes\n      heart\n      thing\n    \n    \n      T38\n      time\n      day\n      house\n      thing\n      room\n      sister\n      man\n      pleasure\n      home\n    \n    \n      T39\n      box\n      gentlemen\n      man\n      uncle\n      leg\n      operation\n      water\n      hand\n      patient\n    \n  \n\n\n\n\n\nTOPICS['label'] = TOPICS.apply(lambda x: x.name + ' ' + ', '.join(x[:n_top_terms]), 1)\n\n\nprint(TOPICS.label.values)\n\n['T00 passengers, emigrants, steerage, cabin, mess, cook, organ, water, head'\n 'T01 lord, man, men, things, wine, ha, day, way, ah'\n 'T02 sea, air, fish, round, day, trees, groves, water, flowers'\n 'T03 man, way, friend, nature, sort, stranger, confidence, kind, case'\n 'T04 sir, herb, confidence, man, herb doctor, ugh, doctor, dont, boy'\n 'T05 father, mother, family, years, brother, time, aunt, child, daughter'\n 'T06 landlord, abbey, gallery, generals, harpooneer, general, man, nose, bed'\n 'T07 doctor, day, country, village, horse, morning, road, house, cocoa'\n 'T08 room, door, house, moment, time, bed, night, hand, street'\n 'T09 gods, god, image, lord, idols, images, wine, arms, king'\n 'T10 valley, natives, time, island, house, place, islanders, feet, fruit'\n 'T11 whiteness, teeth, kings, royal, milk, bull, lord, item, issue'\n 'T12 sea, time, land, water, ship, air, day, sight, ships'\n 'T13 man, canoe, time, face, cook, island, head, sort, strangers'\n 'T14 men, time, world, things, years, people, land, times, man'\n 'T15 captain, ship, cabin, voyage, dollars, ships, man, board, hand'\n 'T16 thing, oh, body, way, day, world, sort, word, voice'\n 'T17 kings, house, king, heart, mind, time, life, people, man'\n 'T18 boy, mother, room, door, man, guide, eyes, world, hand'\n 'T19 world, things, soul, man, heart, life, love, men, thing'\n 'T20 time, feelings, heart, mind, moment, letter, happiness, subject, friend'\n 'T21 man, barber, gentleman, men, sort, world, time, life, person'\n 'T22 reign, time, men, head, day, sailor, sea, country, watches'\n 'T23 oh, ball, sir, evening, way, yes, room, partner, table'\n 'T24 man, day, time, consul, boat, way, ships, tailor, wives'\n 'T25 whale, boat, whales, ship, boats, head, sea, line, way'\n 'T26 isle, lord, king, minstrel, spears, land, royal, throne, shore'\n 'T27 skeleton, lord, feet, bones, sea, things, skull, ribs, brain'\n 'T28 book, thing, volume, paper, books, reading, work, man, hand'\n 'T29 man, men, war, gun, guns, deck, jacket, man war, wars'\n 'T30 sea, ship, deck, night, man, sail, wind, time, hand'\n 'T31 friendship, jacket, auction, need, loan, friend, bags, way, goods'\n 'T32 law, man, court, punishment, laws, code, time, cases, flogging'\n 'T33 guitar, head, mystery, oil, girl, whale, sounds, thing, hair'\n 'T34 man, deck, men, ship, sailors, time, captain, mate, sea'\n 'T35 wife, chimney, table, time, cock, man, house, room, sort'\n 'T36 boots, glass, place, sailors, jacket, friend, sort, sailor, home'\n 'T37 thou, thee, art, brother, oh, face, eyes, heart, thing'\n 'T38 time, day, house, thing, room, sister, man, pleasure, home'\n 'T39 box, gentlemen, man, uncle, leg, operation, water, hand, patient']"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html#sort-topics-by-doc-weight",
    "href": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html#sort-topics-by-doc-weight",
    "title": "LDA with SciKit Learn",
    "section": "Sort Topics by Doc Weight",
    "text": "Sort Topics by Doc Weight\n\nTOPICS['doc_weight_sum'] = THETA.sum()\nTOPICS['term_freq'] = PHI.sum(1) / PHI.sum(1).sum()\n\n\nTOPICS\n\n\n\n\n\n  \n    \n      term_str\n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      label\n      doc_weight_sum\n      term_freq\n    \n    \n      topic_id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      T00\n      passengers\n      emigrants\n      steerage\n      cabin\n      mess\n      cook\n      organ\n      water\n      head\n      T00 passengers, emigrants, steerage, cabin, me...\n      5.170136\n      0.004492\n    \n    \n      T01\n      lord\n      man\n      men\n      things\n      wine\n      ha\n      day\n      way\n      ah\n      T01 lord, man, men, things, wine, ha, day, way...\n      39.761247\n      0.033112\n    \n    \n      T02\n      sea\n      air\n      fish\n      round\n      day\n      trees\n      groves\n      water\n      flowers\n      T02 sea, air, fish, round, day, trees, groves,...\n      31.197993\n      0.015807\n    \n    \n      T03\n      man\n      way\n      friend\n      nature\n      sort\n      stranger\n      confidence\n      kind\n      case\n      T03 man, way, friend, nature, sort, stranger, ...\n      36.842409\n      0.034729\n    \n    \n      T04\n      sir\n      herb\n      confidence\n      man\n      herb doctor\n      ugh\n      doctor\n      dont\n      boy\n      T04 sir, herb, confidence, man, herb doctor, u...\n      12.614978\n      0.009658\n    \n    \n      T05\n      father\n      mother\n      family\n      years\n      brother\n      time\n      aunt\n      child\n      daughter\n      T05 father, mother, family, years, brother, ti...\n      45.796196\n      0.041761\n    \n    \n      T06\n      landlord\n      abbey\n      gallery\n      generals\n      harpooneer\n      general\n      man\n      nose\n      bed\n      T06 landlord, abbey, gallery, generals, harpoo...\n      9.341365\n      0.006017\n    \n    \n      T07\n      doctor\n      day\n      country\n      village\n      horse\n      morning\n      road\n      house\n      cocoa\n      T07 doctor, day, country, village, horse, morn...\n      21.689520\n      0.018303\n    \n    \n      T08\n      room\n      door\n      house\n      moment\n      time\n      bed\n      night\n      hand\n      street\n      T08 room, door, house, moment, time, bed, nigh...\n      23.370074\n      0.022998\n    \n    \n      T09\n      gods\n      god\n      image\n      lord\n      idols\n      images\n      wine\n      arms\n      king\n      T09 gods, god, image, lord, idols, images, win...\n      9.555879\n      0.005022\n    \n    \n      T10\n      valley\n      natives\n      time\n      island\n      house\n      place\n      islanders\n      feet\n      fruit\n      T10 valley, natives, time, island, house, plac...\n      51.542008\n      0.065748\n    \n    \n      T11\n      whiteness\n      teeth\n      kings\n      royal\n      milk\n      bull\n      lord\n      item\n      issue\n      T11 whiteness, teeth, kings, royal, milk, bull...\n      5.696191\n      0.003043\n    \n    \n      T12\n      sea\n      time\n      land\n      water\n      ship\n      air\n      day\n      sight\n      ships\n      T12 sea, time, land, water, ship, air, day, si...\n      40.775572\n      0.041555\n    \n    \n      T13\n      man\n      canoe\n      time\n      face\n      cook\n      island\n      head\n      sort\n      strangers\n      T13 man, canoe, time, face, cook, island, head...\n      18.480897\n      0.014471\n    \n    \n      T14\n      men\n      time\n      world\n      things\n      years\n      people\n      land\n      times\n      man\n      T14 men, time, world, things, years, people, l...\n      41.079327\n      0.034970\n    \n    \n      T15\n      captain\n      ship\n      cabin\n      voyage\n      dollars\n      ships\n      man\n      board\n      hand\n      T15 captain, ship, cabin, voyage, dollars, shi...\n      10.188563\n      0.009078\n    \n    \n      T16\n      thing\n      oh\n      body\n      way\n      day\n      world\n      sort\n      word\n      voice\n      T16 thing, oh, body, way, day, world, sort, wo...\n      14.255702\n      0.014171\n    \n    \n      T17\n      kings\n      house\n      king\n      heart\n      mind\n      time\n      life\n      people\n      man\n      T17 kings, house, king, heart, mind, time, lif...\n      16.939014\n      0.011831\n    \n    \n      T18\n      boy\n      mother\n      room\n      door\n      man\n      guide\n      eyes\n      world\n      hand\n      T18 boy, mother, room, door, man, guide, eyes,...\n      26.694058\n      0.022308\n    \n    \n      T19\n      world\n      things\n      soul\n      man\n      heart\n      life\n      love\n      men\n      thing\n      T19 world, things, soul, man, heart, life, lov...\n      26.295566\n      0.018776\n    \n    \n      T20\n      time\n      feelings\n      heart\n      mind\n      moment\n      letter\n      happiness\n      subject\n      friend\n      T20 time, feelings, heart, mind, moment, lette...\n      126.611068\n      0.120277\n    \n    \n      T21\n      man\n      barber\n      gentleman\n      men\n      sort\n      world\n      time\n      life\n      person\n      T21 man, barber, gentleman, men, sort, world, ...\n      27.036913\n      0.025558\n    \n    \n      T22\n      reign\n      time\n      men\n      head\n      day\n      sailor\n      sea\n      country\n      watches\n      T22 reign, time, men, head, day, sailor, sea, ...\n      5.511883\n      0.002269\n    \n    \n      T23\n      oh\n      ball\n      sir\n      evening\n      way\n      yes\n      room\n      partner\n      table\n      T23 oh, ball, sir, evening, way, yes, room, pa...\n      15.830761\n      0.018105\n    \n    \n      T24\n      man\n      day\n      time\n      consul\n      boat\n      way\n      ships\n      tailor\n      wives\n      T24 man, day, time, consul, boat, way, ships, ...\n      8.127865\n      0.004650\n    \n    \n      T25\n      whale\n      boat\n      whales\n      ship\n      boats\n      head\n      sea\n      line\n      way\n      T25 whale, boat, whales, ship, boats, head, se...\n      42.638291\n      0.038034\n    \n    \n      T26\n      isle\n      lord\n      king\n      minstrel\n      spears\n      land\n      royal\n      throne\n      shore\n      T26 isle, lord, king, minstrel, spears, land, ...\n      8.566671\n      0.003839\n    \n    \n      T27\n      skeleton\n      lord\n      feet\n      bones\n      sea\n      things\n      skull\n      ribs\n      brain\n      T27 skeleton, lord, feet, bones, sea, things, ...\n      13.658475\n      0.006766\n    \n    \n      T28\n      book\n      thing\n      volume\n      paper\n      books\n      reading\n      work\n      man\n      hand\n      T28 book, thing, volume, paper, books, reading...\n      8.864196\n      0.006510\n    \n    \n      T29\n      man\n      men\n      war\n      gun\n      guns\n      deck\n      jacket\n      man war\n      wars\n      T29 man, men, war, gun, guns, deck, jacket, ma...\n      23.457972\n      0.020567\n    \n    \n      T30\n      sea\n      ship\n      deck\n      night\n      man\n      sail\n      wind\n      time\n      hand\n      T30 sea, ship, deck, night, man, sail, wind, t...\n      64.832434\n      0.049921\n    \n    \n      T31\n      friendship\n      jacket\n      auction\n      need\n      loan\n      friend\n      bags\n      way\n      goods\n      T31 friendship, jacket, auction, need, loan, f...\n      3.832761\n      0.002336\n    \n    \n      T32\n      law\n      man\n      court\n      punishment\n      laws\n      code\n      time\n      cases\n      flogging\n      T32 law, man, court, punishment, laws, code, t...\n      9.116354\n      0.004869\n    \n    \n      T33\n      guitar\n      head\n      mystery\n      oil\n      girl\n      whale\n      sounds\n      thing\n      hair\n      T33 guitar, head, mystery, oil, girl, whale, s...\n      4.393569\n      0.002077\n    \n    \n      T34\n      man\n      deck\n      men\n      ship\n      sailors\n      time\n      captain\n      mate\n      sea\n      T34 man, deck, men, ship, sailors, time, capta...\n      79.256877\n      0.073374\n    \n    \n      T35\n      wife\n      chimney\n      table\n      time\n      cock\n      man\n      house\n      room\n      sort\n      T35 wife, chimney, table, time, cock, man, hou...\n      12.821117\n      0.025619\n    \n    \n      T36\n      boots\n      glass\n      place\n      sailors\n      jacket\n      friend\n      sort\n      sailor\n      home\n      T36 boots, glass, place, sailors, jacket, frie...\n      15.187992\n      0.014458\n    \n    \n      T37\n      thou\n      thee\n      art\n      brother\n      oh\n      face\n      eyes\n      heart\n      thing\n      T37 thou, thee, art, brother, oh, face, eyes, ...\n      28.214003\n      0.018196\n    \n    \n      T38\n      time\n      day\n      house\n      thing\n      room\n      sister\n      man\n      pleasure\n      home\n      T38 time, day, house, thing, room, sister, man...\n      126.849768\n      0.126781\n    \n    \n      T39\n      box\n      gentlemen\n      man\n      uncle\n      leg\n      operation\n      water\n      hand\n      patient\n      T39 box, gentlemen, man, uncle, leg, operation...\n      9.904336\n      0.007944\n    \n  \n\n\n\n\n\nTOPICS.sort_values('doc_weight_sum', ascending=True).plot.barh(y='doc_weight_sum', x='label', figsize=(5, n_topics/2));\n\n\n\n\n\nTOPICS.sort_values('term_freq', ascending=True).plot.barh(y='term_freq', x='label', figsize=(5,n_topics/2));\n\n\n\n\n\nTOPICS.plot.scatter('doc_weight_sum', 'term_freq');"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html#add-authors-to-topics",
    "href": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html#add-authors-to-topics",
    "title": "LDA with SciKit Learn",
    "section": "Add Authors to Topics",
    "text": "Add Authors to Topics\nAssociate authors with topics, i.e. assign author for whom topic has highest mean value.\n\nTOPICS['author'] = TOPICS[AUTHORS].idxmax(1)\n\n\nTOPICS.iloc[:,n_top_terms:].sort_values(['author','doc_weight_sum'], ascending=[True,False]).style.background_gradient(cmap=colors)\n\n\n\n\n  \n    \n      term_str\n      label\n      doc_weight_sum\n      term_freq\n      austen\n      melville\n      author\n    \n    \n      topic_id\n       \n       \n       \n       \n       \n       \n    \n  \n  \n    \n      T38\n      T38 time, day, house, thing, room, sister, man, pleasure, home\n      126.849768\n      0.126781\n      0.357309\n      0.009528\n      austen\n    \n    \n      T20\n      T20 time, feelings, heart, mind, moment, letter, happiness, subject, friend\n      126.611068\n      0.120277\n      0.343993\n      0.014870\n      austen\n    \n    \n      T05\n      T05 father, mother, family, years, brother, time, aunt, child, daughter\n      45.796196\n      0.041761\n      0.112273\n      0.010529\n      austen\n    \n    \n      T21\n      T21 man, barber, gentleman, men, sort, world, time, life, person\n      27.036913\n      0.025558\n      0.038971\n      0.017793\n      austen\n    \n    \n      T08\n      T08 room, door, house, moment, time, bed, night, hand, street\n      23.370074\n      0.022998\n      0.028895\n      0.017410\n      austen\n    \n    \n      T23\n      T23 oh, ball, sir, evening, way, yes, room, partner, table\n      15.830761\n      0.018105\n      0.033419\n      0.005925\n      austen\n    \n    \n      T16\n      T16 thing, oh, body, way, day, world, sort, word, voice\n      14.255702\n      0.014171\n      0.023458\n      0.008148\n      austen\n    \n    \n      T34\n      T34 man, deck, men, ship, sailors, time, captain, mate, sea\n      79.256877\n      0.073374\n      0.002328\n      0.099593\n      melville\n    \n    \n      T30\n      T30 sea, ship, deck, night, man, sail, wind, time, hand\n      64.832434\n      0.049921\n      0.000556\n      0.082039\n      melville\n    \n    \n      T10\n      T10 valley, natives, time, island, house, place, islanders, feet, fruit\n      51.542008\n      0.065748\n      0.001799\n      0.064646\n      melville\n    \n    \n      T25\n      T25 whale, boat, whales, ship, boats, head, sea, line, way\n      42.638291\n      0.038034\n      0.000326\n      0.053971\n      melville\n    \n    \n      T14\n      T14 men, time, world, things, years, people, land, times, man\n      41.079327\n      0.034970\n      0.001014\n      0.051701\n      melville\n    \n    \n      T12\n      T12 sea, time, land, water, ship, air, day, sight, ships\n      40.775572\n      0.041555\n      0.000964\n      0.051337\n      melville\n    \n    \n      T01\n      T01 lord, man, men, things, wine, ha, day, way, ah\n      39.761247\n      0.033112\n      0.001860\n      0.049670\n      melville\n    \n    \n      T03\n      T03 man, way, friend, nature, sort, stranger, confidence, kind, case\n      36.842409\n      0.034729\n      0.003035\n      0.045468\n      melville\n    \n    \n      T02\n      T02 sea, air, fish, round, day, trees, groves, water, flowers\n      31.197993\n      0.015807\n      0.000782\n      0.039260\n      melville\n    \n    \n      T37\n      T37 thou, thee, art, brother, oh, face, eyes, heart, thing\n      28.214003\n      0.018196\n      0.000745\n      0.035489\n      melville\n    \n    \n      T18\n      T18 boy, mother, room, door, man, guide, eyes, world, hand\n      26.694058\n      0.022308\n      0.005155\n      0.031691\n      melville\n    \n    \n      T19\n      T19 world, things, soul, man, heart, life, love, men, thing\n      26.295566\n      0.018776\n      0.004146\n      0.031612\n      melville\n    \n    \n      T29\n      T29 man, men, war, gun, guns, deck, jacket, man war, wars\n      23.457972\n      0.020567\n      0.000267\n      0.029656\n      melville\n    \n    \n      T07\n      T07 doctor, day, country, village, horse, morning, road, house, cocoa\n      21.689520\n      0.018303\n      0.011965\n      0.022454\n      melville\n    \n    \n      T13\n      T13 man, canoe, time, face, cook, island, head, sort, strangers\n      18.480897\n      0.014471\n      0.000251\n      0.023347\n      melville\n    \n    \n      T17\n      T17 kings, house, king, heart, mind, time, life, people, man\n      16.939014\n      0.011831\n      0.000951\n      0.021093\n      melville\n    \n    \n      T36\n      T36 boots, glass, place, sailors, jacket, friend, sort, sailor, home\n      15.187992\n      0.014458\n      0.001023\n      0.018840\n      melville\n    \n    \n      T27\n      T27 skeleton, lord, feet, bones, sea, things, skull, ribs, brain\n      13.658475\n      0.006766\n      0.000419\n      0.017156\n      melville\n    \n    \n      T35\n      T35 wife, chimney, table, time, cock, man, house, room, sort\n      12.821117\n      0.025619\n      0.001691\n      0.015554\n      melville\n    \n    \n      T04\n      T04 sir, herb, confidence, man, herb doctor, ugh, doctor, dont, boy\n      12.614978\n      0.009658\n      0.001432\n      0.015402\n      melville\n    \n    \n      T15\n      T15 captain, ship, cabin, voyage, dollars, ships, man, board, hand\n      10.188563\n      0.009078\n      0.000379\n      0.012769\n      melville\n    \n    \n      T39\n      T39 box, gentlemen, man, uncle, leg, operation, water, hand, patient\n      9.904336\n      0.007944\n      0.001042\n      0.012127\n      melville\n    \n    \n      T09\n      T09 gods, god, image, lord, idols, images, wine, arms, king\n      9.555879\n      0.005022\n      0.001484\n      0.011498\n      melville\n    \n    \n      T06\n      T06 landlord, abbey, gallery, generals, harpooneer, general, man, nose, bed\n      9.341365\n      0.006017\n      0.005437\n      0.009550\n      melville\n    \n    \n      T32\n      T32 law, man, court, punishment, laws, code, time, cases, flogging\n      9.116354\n      0.004869\n      0.001890\n      0.010768\n      melville\n    \n    \n      T28\n      T28 book, thing, volume, paper, books, reading, work, man, hand\n      8.864196\n      0.006510\n      0.005876\n      0.008758\n      melville\n    \n    \n      T26\n      T26 isle, lord, king, minstrel, spears, land, royal, throne, shore\n      8.566671\n      0.003839\n      0.000290\n      0.010749\n      melville\n    \n    \n      T24\n      T24 man, day, time, consul, boat, way, ships, tailor, wives\n      8.127865\n      0.004650\n      0.000233\n      0.010216\n      melville\n    \n    \n      T11\n      T11 whiteness, teeth, kings, royal, milk, bull, lord, item, issue\n      5.696191\n      0.003043\n      0.000259\n      0.007119\n      melville\n    \n    \n      T22\n      T22 reign, time, men, head, day, sailor, sea, country, watches\n      5.511883\n      0.002269\n      0.003312\n      0.005591\n      melville\n    \n    \n      T00\n      T00 passengers, emigrants, steerage, cabin, mess, cook, organ, water, head\n      5.170136\n      0.004492\n      0.000397\n      0.006393\n      melville\n    \n    \n      T33\n      T33 guitar, head, mystery, oil, girl, whale, sounds, thing, hair\n      4.393569\n      0.002077\n      0.000178\n      0.005500\n      melville\n    \n    \n      T31\n      T31 friendship, jacket, auction, need, loan, friend, bags, way, goods\n      3.832761\n      0.002336\n      0.000191\n      0.004783\n      melville"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html#topics-in-author-space",
    "href": "lessons/M08_TopicModels/M08_02_LDASciKitLearn.html#topics-in-author-space",
    "title": "LDA with SciKit Learn",
    "section": "Topics in Author Space",
    "text": "Topics in Author Space\nSee how topics are associated with author dimensions.\n\npx.scatter(TOPICS.reset_index(), 'austen', 'melville', \n           hover_name='label', text='topic_id', \n           size=(TOPICS.austen * TOPICS.melville),\n           color='author',\n           width=1000, height=350)"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_03_TopicSimilarity.html",
    "href": "lessons/M08_TopicModels/M08_03_TopicSimilarity.html",
    "title": "Topic Similarity",
    "section": "",
    "text": "Set Up\nCreate a short label for each author for display purposes.\nUse PHI and THETA as vector spaces to cluster topics."
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_03_TopicSimilarity.html#imports",
    "href": "lessons/M08_TopicModels/M08_03_TopicSimilarity.html#imports",
    "title": "Topic Similarity",
    "section": "Imports",
    "text": "Imports\n\nimport pandas as pd\nimport numpy as np\nimport plotly_express as px\n\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import normalize\n\n\nimport sys; sys.path.append(local_lib)\nfrom hac import HAC"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_03_TopicSimilarity.html#config",
    "href": "lessons/M08_TopicModels/M08_03_TopicSimilarity.html#config",
    "title": "Topic Similarity",
    "section": "Config",
    "text": "Config\n\nimport configparser\nconfig = configparser.ConfigParser()\nconfig.read(\"../env.ini\")\ndata_home = config['DEFAULT']['data_home']\noutput_dir = config['DEFAULT']['output_dir']\nlocal_lib = config['DEFAULT']['local_lib']\n\n\n# data_in = '../data/output'\n# data_out = '../data/output'\n# local_lib = \"../lib/\"\n\ndata_prefix = 'austen-melville'\ncolors = \"YlGnBu\"\nn_topics = 40\nOHCO = ['book_id','chap_num']"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_03_TopicSimilarity.html#by-phi",
    "href": "lessons/M08_TopicModels/M08_03_TopicSimilarity.html#by-phi",
    "title": "Topic Similarity",
    "section": "By PHI",
    "text": "By PHI\n\nHAC(PHI, labels=labels).plot();\n\n<Figure size 640x480 with 0 Axes>"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_03_TopicSimilarity.html#by-theta",
    "href": "lessons/M08_TopicModels/M08_03_TopicSimilarity.html#by-theta",
    "title": "Topic Similarity",
    "section": "By THETA",
    "text": "By THETA\n\nHAC(THETA.T, labels=labels).plot();\n\n<Figure size 640x480 with 0 Axes>"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_03_TopicSimilarity.html#by-phi-1",
    "href": "lessons/M08_TopicModels/M08_03_TopicSimilarity.html#by-phi-1",
    "title": "Topic Similarity",
    "section": "By PHI",
    "text": "By PHI\n\npca_engine_phi = PCA(2)\n\n\nPHI_COMPS = pd.DataFrame(pca_engine_phi.fit_transform(normalize(PHI, norm='l2', axis=1)), index=PHI.index)\n\n\nHAC(PHI_COMPS, labels=labels).plot()\n\n<Figure size 640x480 with 0 Axes>\n\n\n\n\n\n\npx.scatter(PHI_COMPS.reset_index(), 0, 1, \n           size=TOPICS.term_freq, \n           color=TOPICS.author, \n           text='topic_id', hover_name=TOPICS.label, height=600, width=700)\n\n\n                                                \n\n\n\nPHI_LOADINGS = pd.DataFrame(pca_engine_phi.components_.T * np.sqrt(pca_engine_phi.explained_variance_), index=PHI.T.index)\nPHI_LOADINGS.index.name = 'term_str'\n\n\npx.scatter(PHI_LOADINGS.reset_index(), 0, 1, text='term_str', height=600, width=700)"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_03_TopicSimilarity.html#by-theta-1",
    "href": "lessons/M08_TopicModels/M08_03_TopicSimilarity.html#by-theta-1",
    "title": "Topic Similarity",
    "section": "By THETA",
    "text": "By THETA\n\npca_engine_theta = PCA(5)\n\n\nTHETA_COMPS = pd.DataFrame(pca_engine_theta.fit_transform(normalize(THETA.T.values, norm='l2', axis=1)), index=THETA.T.index)\nTHETA_COMPS.index.name = 'topic_id'\n\n\nHAC(THETA_COMPS, labels=labels).plot()\n\n<Figure size 640x480 with 0 Axes>\n\n\n\n\n\n\npx.scatter(THETA_COMPS.reset_index(), 0, 1, \n           size=TOPICS.doc_weight_sum, \n           color=TOPICS.author, \n           text='topic_id', \n           hover_name=TOPICS.label, \n           height=600, width=700)\n\n\n                                                \n\n\n\nTHETA_LOADINGS = pd.DataFrame(pca_engine_theta.components_.T * np.sqrt(pca_engine_theta.explained_variance_), index=THETA.index)\n\n\nDOCS['doc_label'] = DOCS.apply(lambda x: f\"{LIB.loc[x.name[0]].label}-{x.name[1]}\", axis=1)\nDOCS['book'] = DOCS.apply(lambda x: f\"{LIB.loc[x.name[0]].label}\", axis=1)\nDOCS['n_chars'] = DOCS.doc_str.str.len()\n\n\npx.scatter(THETA_LOADINGS.reset_index(), 0, 1, \n           size=DOCS.n_chars,\n           color=DOCS.book, height=600, width=900)"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_04_TopicContiguity.html",
    "href": "lessons/M08_TopicModels/M08_04_TopicContiguity.html",
    "title": "Topic Contiguity",
    "section": "",
    "text": "Set Up\nCreate a short label for each author for display purposes.\nOne way to think of topic contiguity is as the statistical correlation between topic pairs.\nWe use Kendall’s method because it is non-parametric.\nBelow, we create a table of topic pairs \\(X\\) and add two correlation features, one for each embedding.\nAdd topic labels to the table for display purposes.\nCorrelate the correlations.\nTopic gravity is a way to measure the weight of a pair in similarity space.\nWe will use this value in our visualizations.\nRecall that gravity \\(G = \\large \\frac{M_1 M_2}{d^2}\\).\nLook at correlations and gravity.\nMutual informtion \\(I\\) obeys the following formula:\nIt is the expectation of pointwise mutual information \\(i(x,y)\\).\n\\(\\Large i(x, y) = \\Large log(\\frac{p(x,y)}{p(x)p(y)})\\)\nPer Bouma 2009, we normalize \\(i(x,y)\\) to avoid ovder-valuing rare co-occurrences.\n\\(\\Large i_n(x,y) = \\Large \\frac{i(x,y)}{log(\\frac{1}{p(x,y)})}\\)\nWe compare mutual information to correlation."
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_04_TopicContiguity.html#config",
    "href": "lessons/M08_TopicModels/M08_04_TopicContiguity.html#config",
    "title": "Topic Contiguity",
    "section": "Config",
    "text": "Config\n\nimport configparser\nconfig = configparser.ConfigParser()\nconfig.read('../env.ini')\ndata_home = config['DEFAULT']['data_home']\noutput_dir = config['DEFAULT']['output_dir']\nlocal_lib = config['DEFAULT']['local_lib']"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_04_TopicContiguity.html#imports",
    "href": "lessons/M08_TopicModels/M08_04_TopicContiguity.html#imports",
    "title": "Topic Contiguity",
    "section": "Imports",
    "text": "Imports\n\nimport pandas as pd\nimport numpy as np\nimport plotly_express as px\n\n\nimport sys; sys.path.append(local_lib)\nfrom hac import HAC\n\n\ndata_prefix = 'austen-melville'\nlocal_lib = \"../lib/\"\ncolors = \"YlGnBu\"\nn_topics = 40\nOHCO = ['book_id','chap_num']"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_05_PyLDAVis.html",
    "href": "lessons/M08_TopicModels/M08_05_PyLDAVis.html",
    "title": "PyLDAVis",
    "section": "",
    "text": "Setup\nFrom Sievert and Shirley 2014"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_05_PyLDAVis.html#config",
    "href": "lessons/M08_TopicModels/M08_05_PyLDAVis.html#config",
    "title": "PyLDAVis",
    "section": "Config",
    "text": "Config\n\nimport configparser\nconfig = configparser.ConfigParser()\nconfig.read(\"../env.ini\")\noutput_dir = config['DEFAULT']['output_dir']\ndata_prefix = 'austen-melville'"
  },
  {
    "objectID": "lessons/M08_TopicModels/M08_05_PyLDAVis.html#import",
    "href": "lessons/M08_TopicModels/M08_05_PyLDAVis.html#import",
    "title": "PyLDAVis",
    "section": "Import",
    "text": "Import\n\nimport pandas as pd\nimport numpy as np\nimport pyLDAvis\n# import pyLDAvis.sklearn\nimport pyLDAvis.lda_model\nimport pickle"
  },
  {
    "objectID": "lessons/M08_TopicModels/Untitled.html",
    "href": "lessons/M08_TopicModels/Untitled.html",
    "title": "n_topics = 20",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA\nimport plotly_express as px\nfrom glob import glob\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nngram_range = (1, 2)\nn_terms = 4000\nn_topics = 40\nmax_iter = 20\nn_top_terms = 9\n\n\nimport configparser\nconfig = configparser.ConfigParser()\nconfig.read(\"../env.ini\")\ndata_home = config['DEFAULT']['data_home']\noutput_dir = config['DEFAULT']['output_dir']\nlocal_lib = config['DEFAULT']['local_lib']\n\n\ndata_dir = f\"{data_home}/newsgroups/20news-18828/\"\n\n\ndata = []\nfor dir in glob(f\"{data_dir}/*\"):\n    cat = dir.split('/')[-1]\n    for doc in glob(f\"{dir}/*\"):\n        text = open(doc, 'r', encoding=\"utf8\", errors=\"ignore\").read().split(\"\\n\")\n        doc_from = text[0]\n        doc_subj = text[1]\n        doc_str = \" \".join(text[2:])\n        data.append((cat, doc, doc_from, doc_subj, doc_str))\n\n\nDOCS = pd.DataFrame(data, columns=['cat','file','from', 'subj', 'doc_str']) \nDOCS.doc_str = DOCS.doc_str.str.replace(r\"\\s+\", \" \", regex=True)\nDOCS['from'] = DOCS['from'].str.replace(r\"^From: \", \"\", regex=True) \nDOCS['subj'] = DOCS['subj'].str.replace(r\"^Subject: \", \"\", regex=True) \n\n\nDOCS.to_csv(f\"{output_dir}/newsgroups-corpus.csv\")\n\n\ncount_engine = CountVectorizer(max_features=n_terms, ngram_range=ngram_range, stop_words='english')\ncount_model = count_engine.fit_transform(DOCS.doc_str)\nTERMS = count_engine.get_feature_names_out()\n\n\nVOCAB = pd.DataFrame(index=TERMS)\nVOCAB.index.name = 'term_str'\n\n\nDTM = pd.DataFrame(count_model.toarray(), index=DOCS.index, columns=TERMS)\n\n\nVOCAB['doc_count'] = DTM.astype('bool').astype('int').sum()\nDOCS['term_count'] = DTM.sum(1)\n\n\nDOCS.term_count.describe()\n\ncount    18828.000000\nmean       104.581262\nstd        427.003977\nmin          0.000000\n25%         34.000000\n50%         58.000000\n75%         99.250000\nmax      24647.000000\nName: term_count, dtype: float64\n\n\n\nlda_engine = LDA(n_components=n_topics, max_iter=max_iter, learning_offset=50., random_state=0)\n\n\nTNAMES = [f\"T{str(x).zfill(len(str(n_topics)))}\" for x in range(n_topics)]\n\n\nlda_model = lda_engine.fit_transform(count_model)\n\n\nTHETA = pd.DataFrame(lda_model, index=DOCS.index)\nTHETA.columns.name = 'topic_id'\nTHETA.columns = TNAMES\n\n\nPHI = pd.DataFrame(lda_engine.components_, columns=TERMS, index=TNAMES)\nPHI.index.name = 'topic_id'\nPHI.columns.name  = 'term_str'\n\n\nTOPICS = PHI.stack().to_frame('topic_weight').groupby('topic_id')\\\n    .apply(lambda x: x.sort_values('topic_weight', ascending=False)\\\n        .head(n_top_terms).reset_index().drop('topic_id', axis=1)['term_str'])\n\n\nTOPICS.T\n\n\n\n\n\n  \n    \n      topic_id\n      T00\n      T01\n      T02\n      T03\n      T04\n      T05\n      T06\n      T07\n      T08\n      T09\n      ...\n      T30\n      T31\n      T32\n      T33\n      T34\n      T35\n      T36\n      T37\n      T38\n      T39\n    \n    \n      term_str\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      00\n      edu\n      windows\n      armenian\n      window\n      game\n      edu\n      god\n      gun\n      uk\n      ...\n      __\n      ax\n      space\n      entry\n      university\n      edu\n      drive\n      don\n      said\n      israel\n    \n    \n      1\n      cx\n      hockey\n      dos\n      turkish\n      mit\n      team\n      writes\n      jesus\n      guns\n      edu\n      ...\n      ___\n      ax ax\n      nasa\n      disease\n      fax\n      ftp\n      scsi\n      people\n      time\n      jews\n    \n    \n      2\n      50\n      new\n      card\n      armenians\n      server\n      games\n      article\n      christ\n      crime\n      ac\n      ...\n      _____\n      max\n      gov\n      medical\n      au\n      software\n      disk\n      just\n      went\n      war\n    \n    \n      3\n      w7\n      cmu\n      os\n      armenia\n      use\n      year\n      ca\n      lord\n      weapons\n      writes\n      ...\n      __ __\n      max ax\n      nasa gov\n      use\n      computer\n      available\n      hard\n      like\n      years\n      israeli\n    \n    \n      4\n      mv\n      cmu edu\n      ms\n      turkey\n      motif\n      play\n      writes article\n      sin\n      ibm\n      article\n      ...\n      wire\n      ax max\n      earth\n      output\n      0d\n      mail\n      bus\n      think\n      didn\n      jewish\n    \n    \n      5\n      hz\n      san\n      use\n      turks\n      mit edu\n      players\n      org\n      love\n      control\n      ac uk\n      ...\n      ____\n      g9v\n      launch\n      patients\n      science\n      graphics\n      drives\n      know\n      told\n      arab\n    \n    \n      6\n      1st\n      andrew\n      run\n      people\n      set\n      baseball\n      com\n      life\n      firearms\n      cs\n      ...\n      ground\n      b8f\n      orbit\n      treatment\n      department\n      list\n      controller\n      want\n      ago\n      peace\n    \n    \n      7\n      ah\n      buffalo\n      mouse\n      soviet\n      using\n      season\n      cs\n      man\n      ibm com\n      keith\n      ...\n      ___ ___\n      a86\n      shuttle\n      doctor\n      engineering\n      pub\n      ide\n      good\n      day\n      muslims\n    \n    \n      8\n      c_\n      colorado\n      driver\n      greek\n      application\n      win\n      know\n      bible\n      police\n      caltech\n      ...\n      mil\n      g9v g9v\n      moon\n      food\n      _o\n      send\n      floppy\n      ve\n      saw\n      writes\n    \n  \n\n9 rows × 40 columns\n\n\n\n\nTOPICS['label'] = TOPICS.apply(lambda x: x.name + ' ' + ', '.join(x[:n_top_terms]), 1)\n\n\nTOPICS['doc_weight_sum'] = THETA.sum()\nTOPICS['term_freq'] = PHI.sum(1) / PHI.sum(1).sum()\n\n\nTOPICS.sort_values('doc_weight_sum', ascending=True).plot.barh(y='doc_weight_sum', x='label', figsize=(5, n_topics/2));"
  },
  {
    "objectID": "lessons/M08a_Visualization/M08a_01_Gensim.html",
    "href": "lessons/M08a_Visualization/M08a_01_Gensim.html",
    "title": "Other Tools",
    "section": "",
    "text": "Set Up\nCreate a set of frequent words\nLowercase each document, split it by white space, remove non-alphanumeric characters, and filter out stopwords\nCount word frequencies\nOnly keep words that appear more than once\nCreate a “dictionary,” which associates a term string with a numeric identifier.\nCreate the BOW corpus from the text using the dictionary."
  },
  {
    "objectID": "lessons/M08a_Visualization/M08a_01_Gensim.html#config",
    "href": "lessons/M08a_Visualization/M08a_01_Gensim.html#config",
    "title": "Other Tools",
    "section": "Config",
    "text": "Config\n\nimport configparser\nconfig = configparser.ConfigParser()\nconfig.read(\"../env.ini\")\ndata_home = config['DEFAULT']['data_home']\noutput_dir = config['DEFAULT']['output_dir']\nlocal_lib = config['DEFAULT']['local_lib']\n\n\nnum_topics = 100\ndata_dir = f\"{data_home}/newsgroups/20news-18828\""
  },
  {
    "objectID": "lessons/M08a_Visualization/M08a_01_Gensim.html#imports",
    "href": "lessons/M08a_Visualization/M08a_01_Gensim.html#imports",
    "title": "Other Tools",
    "section": "Imports",
    "text": "Imports\n\nimport pandas as pd\nimport numpy as np\nfrom gensim import corpora, models\nfrom collections import defaultdict\nimport plotly_express as px\nfrom glob import glob\nimport re"
  },
  {
    "objectID": "lessons/M08a_Visualization/M08a_01_Gensim.html#tfidf",
    "href": "lessons/M08a_Visualization/M08a_01_Gensim.html#tfidf",
    "title": "Other Tools",
    "section": "TFIDF",
    "text": "TFIDF\n\ntfidf = models.TfidfModel(bow_corpus)\n\n\n# tfidf[bow_corpus[5]]"
  },
  {
    "objectID": "lessons/M08a_Visualization/M08a_01_Gensim.html#lda",
    "href": "lessons/M08a_Visualization/M08a_01_Gensim.html#lda",
    "title": "Other Tools",
    "section": "LDA",
    "text": "LDA\n\nmodel = models.LdaModel(bow_corpus, id2word=dictionary, num_topics=num_topics)\n\n\nmodel2 = models.HdpModel(bow_corpus, id2word=dictionary)"
  },
  {
    "objectID": "lessons/M08a_Visualization/M08a_01_Gensim.html#vocab",
    "href": "lessons/M08a_Visualization/M08a_01_Gensim.html#vocab",
    "title": "Other Tools",
    "section": "VOCAB",
    "text": "VOCAB\n\nVOCAB = pd.DataFrame([(k, v) for k, v in dictionary.token2id.items()], columns=['term_str','term_id']) #.set_index('term_id')\nVOCAB['n'] = VOCAB.term_str.map(lambda x: frequency[x])\nVOCAB = VOCAB.set_index('term_id').sort_index()\n\n\nVOCAB.sample(5)\n\n\n\n\n\n  \n    \n      \n      term_str\n      n\n    \n    \n      term_id\n      \n      \n    \n  \n  \n    \n      17700\n      isolates\n      2\n    \n    \n      13222\n      lac\n      3\n    \n    \n      18841\n      imminent\n      22\n    \n    \n      62735\n      jyangsscvx1bitnet\n      2\n    \n    \n      35044\n      oquendos\n      3"
  },
  {
    "objectID": "lessons/M08a_Visualization/M08a_01_Gensim.html#tfidf-1",
    "href": "lessons/M08a_Visualization/M08a_01_Gensim.html#tfidf-1",
    "title": "Other Tools",
    "section": "TFIDF",
    "text": "TFIDF\n\ntfidf_data = []\nfor doc_id, doc in enumerate(bow_corpus):\n    for term in tfidf[doc]:\n        tfidf_data.append((doc_id, term[0], term[1]))\nTFIDF = pd.DataFrame(tfidf_data, columns=['doc_id','term_id', 'tfidf']).set_index(['doc_id','term_id'])\n\n\nTFIDF.tfidf.unstack(fill_value=0)\n\n\n\n\n\n  \n    \n      term_id\n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      ...\n      79154\n      79155\n      79156\n      79157\n      79158\n      79159\n      79160\n      79161\n      79162\n      79163\n    \n    \n      doc_id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0.121893\n      0.042943\n      0.014431\n      0.066946\n      0.041293\n      0.013847\n      0.013055\n      0.054541\n      0.064667\n      0.011687\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      1\n      0.000000\n      0.000000\n      0.039125\n      0.000000\n      0.000000\n      0.056313\n      0.035394\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      2\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      3\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.012670\n      0.000000\n      0.000000\n      0.000000\n      0.021386\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      4\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.031419\n      0.014811\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      18823\n      0.000000\n      0.000000\n      0.020391\n      0.000000\n      0.000000\n      0.000000\n      0.018447\n      0.000000\n      0.000000\n      0.049539\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      18824\n      0.000000\n      0.000000\n      0.031593\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.012792\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      18825\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.012472\n      0.000000\n      0.000000\n      0.000000\n      0.031579\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      18826\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      18827\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.020961\n      0.000000\n      0.000000\n      0.018764\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n18791 rows × 79164 columns"
  },
  {
    "objectID": "lessons/M08a_Visualization/M08a_01_Gensim.html#bow",
    "href": "lessons/M08a_Visualization/M08a_01_Gensim.html#bow",
    "title": "Other Tools",
    "section": "BOW",
    "text": "BOW\n\nbow_data = []\nfor i, doc in enumerate(bow_corpus):\n    for term in doc:\n        bow_data.append((i, term[0], term[1]))\nBOW = pd.DataFrame(bow_data, columns=['doc_id','term_id', 'n']).set_index(['doc_id','term_id'])     \nDTM = BOW.n.unstack(fill_value=0)\n\n\nBOW.head()\n\n\n\n\n\n  \n    \n      \n      \n      n\n    \n    \n      doc_id\n      term_id\n      \n    \n  \n  \n    \n      0\n      0\n      1\n    \n    \n      1\n      1\n    \n    \n      2\n      1\n    \n    \n      3\n      1\n    \n    \n      4\n      1\n    \n  \n\n\n\n\n\nDTM.head()\n\n\n\n\n\n  \n    \n      term_id\n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      ...\n      79154\n      79155\n      79156\n      79157\n      79158\n      79159\n      79160\n      79161\n      79162\n      79163\n    \n    \n      doc_id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      0\n      0\n      2\n      0\n      0\n      3\n      2\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      2\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      0\n      0\n      0\n      0\n      0\n      2\n      1\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n5 rows × 79164 columns"
  },
  {
    "objectID": "lessons/M08a_Visualization/M08a_01_Gensim.html#lda-1",
    "href": "lessons/M08a_Visualization/M08a_01_Gensim.html#lda-1",
    "title": "Other Tools",
    "section": "LDA",
    "text": "LDA\n\nPHI\n\nPHI = pd.DataFrame(model.get_topics()).T\nPHI.index.name = 'term_id'\n\n\nPHI\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      ...\n      90\n      91\n      92\n      93\n      94\n      95\n      96\n      97\n      98\n      99\n    \n    \n      term_id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0.000004\n      0.000007\n      1.209821e-07\n      0.000007\n      8.446320e-08\n      0.000006\n      0.000008\n      0.000008\n      0.000006\n      4.388106e-07\n      ...\n      3.999588e-08\n      0.000006\n      6.887807e-07\n      0.000002\n      7.444429e-08\n      6.166574e-07\n      9.159745e-07\n      0.000002\n      7.711055e-07\n      0.000005\n    \n    \n      1\n      0.000020\n      0.000034\n      1.436248e-03\n      0.000046\n      9.286494e-07\n      0.000025\n      0.000026\n      0.000032\n      0.000018\n      8.289085e-05\n      ...\n      2.190440e-05\n      0.000012\n      3.785224e-04\n      0.000009\n      1.471395e-03\n      6.170237e-04\n      2.884323e-05\n      0.000023\n      8.576413e-04\n      0.000012\n    \n    \n      2\n      0.000351\n      0.000369\n      2.993358e-03\n      0.001102\n      2.379572e-03\n      0.000974\n      0.000525\n      0.000290\n      0.000284\n      3.591591e-03\n      ...\n      2.677884e-03\n      0.000186\n      4.007390e-03\n      0.001588\n      3.450958e-03\n      6.141013e-04\n      1.822793e-03\n      0.001482\n      3.273964e-03\n      0.001092\n    \n    \n      3\n      0.000004\n      0.000012\n      1.414892e-07\n      0.000009\n      4.547562e-05\n      0.000006\n      0.000010\n      0.000063\n      0.000014\n      1.592202e-06\n      ...\n      4.810859e-05\n      0.000005\n      8.219026e-07\n      0.000003\n      1.853169e-04\n      1.632976e-05\n      1.333521e-06\n      0.000007\n      3.112860e-04\n      0.000006\n    \n    \n      4\n      0.000015\n      0.000060\n      9.328199e-04\n      0.000149\n      9.394193e-05\n      0.000059\n      0.000179\n      0.000013\n      0.000033\n      2.474635e-04\n      ...\n      5.921134e-04\n      0.000052\n      4.065890e-04\n      0.000058\n      1.309366e-04\n      2.917023e-05\n      6.910593e-04\n      0.000736\n      1.616540e-04\n      0.000020\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      79159\n      0.000004\n      0.000007\n      1.209821e-07\n      0.000007\n      8.446320e-08\n      0.000006\n      0.000008\n      0.000008\n      0.000006\n      3.876675e-07\n      ...\n      3.999588e-08\n      0.000005\n      4.806589e-07\n      0.000002\n      7.444429e-08\n      3.146401e-07\n      9.159745e-07\n      0.000001\n      1.640756e-07\n      0.000005\n    \n    \n      79160\n      0.000004\n      0.000007\n      1.209821e-07\n      0.000007\n      8.446320e-08\n      0.000006\n      0.000008\n      0.000008\n      0.000006\n      3.876675e-07\n      ...\n      3.999588e-08\n      0.000005\n      4.806589e-07\n      0.000002\n      7.444429e-08\n      3.146401e-07\n      9.159745e-07\n      0.000001\n      1.640756e-07\n      0.000005\n    \n    \n      79161\n      0.000004\n      0.000007\n      1.209821e-07\n      0.000007\n      8.446320e-08\n      0.000006\n      0.000008\n      0.000008\n      0.000006\n      3.876675e-07\n      ...\n      3.999588e-08\n      0.000005\n      4.806589e-07\n      0.000002\n      7.444429e-08\n      3.146401e-07\n      9.159745e-07\n      0.000001\n      1.640756e-07\n      0.000005\n    \n    \n      79162\n      0.000004\n      0.000007\n      1.209821e-07\n      0.000007\n      8.446320e-08\n      0.000006\n      0.000008\n      0.000008\n      0.000006\n      3.876675e-07\n      ...\n      3.999588e-08\n      0.000005\n      4.806589e-07\n      0.000002\n      7.444429e-08\n      3.146401e-07\n      9.159745e-07\n      0.000001\n      1.640756e-07\n      0.000005\n    \n    \n      79163\n      0.000004\n      0.000007\n      1.209821e-07\n      0.000007\n      8.446320e-08\n      0.000006\n      0.000008\n      0.000008\n      0.000006\n      3.876675e-07\n      ...\n      3.999588e-08\n      0.000005\n      4.806589e-07\n      0.000002\n      7.444429e-08\n      3.146401e-07\n      9.159745e-07\n      0.000001\n      1.640756e-07\n      0.000005\n    \n  \n\n79164 rows × 100 columns\n\n\n\n\n\nTHETA\n\ntheta_data = []\nfor doc_id, doc_bow in enumerate(bow_corpus):\n    for topic in model.get_document_topics(doc_bow):\n        theta_data.append((doc_id, topic[0], topic[1]))\nTHETA = pd.DataFrame(theta_data, columns=['doc_id', 'topic_id', 'topic_weight']).set_index(['doc_id','topic_id']).unstack(fill_value=0)\n\n\nTHETA\n\n\n\n\n\n  \n    \n      \n      topic_weight\n    \n    \n      topic_id\n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      ...\n      90\n      91\n      92\n      93\n      94\n      95\n      96\n      97\n      98\n      99\n    \n    \n      doc_id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.023057\n      0.0\n      0.0\n      0.0\n      0.0\n      0.068088\n      ...\n      0.494118\n      0.0\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.000000\n      0.307738\n      0.0\n    \n    \n      1\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.000000\n      ...\n      0.377174\n      0.0\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.000000\n      0.000000\n      0.0\n    \n    \n      2\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.000000\n      ...\n      0.328368\n      0.0\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.000000\n      0.000000\n      0.0\n    \n    \n      3\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.047245\n      0.0\n      0.0\n      0.0\n      0.0\n      0.044121\n      ...\n      0.096579\n      0.0\n      0.000000\n      0.000000\n      0.026952\n      0.0\n      0.0\n      0.015918\n      0.000000\n      0.0\n    \n    \n      4\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.000000\n      ...\n      0.157478\n      0.0\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.000000\n      0.096272\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      18823\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.000000\n      ...\n      0.000000\n      0.0\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.000000\n      0.000000\n      0.0\n    \n    \n      18824\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.104636\n      0.0\n      0.0\n      0.0\n      0.0\n      0.000000\n      ...\n      0.000000\n      0.0\n      0.032088\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.000000\n      0.000000\n      0.0\n    \n    \n      18825\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.000000\n      ...\n      0.000000\n      0.0\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.000000\n      0.000000\n      0.0\n    \n    \n      18826\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.000000\n      ...\n      0.174497\n      0.0\n      0.000000\n      0.000000\n      0.018394\n      0.0\n      0.0\n      0.000000\n      0.000000\n      0.0\n    \n    \n      18827\n      0.0\n      0.0\n      0.048384\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.000000\n      ...\n      0.000000\n      0.0\n      0.000000\n      0.015222\n      0.000000\n      0.0\n      0.0\n      0.089847\n      0.000000\n      0.0\n    \n  \n\n18790 rows × 100 columns\n\n\n\n\n\nTOPIC\n\ntopic_data = []\nfor t in range(num_topics):\n    for term_rank, term in enumerate(model.get_topic_terms(t)):\n        term_id = term[0]\n        topic_data.append((t, term_rank, dictionary.id2token[term_id]))\n\n\nTOPIC = pd.DataFrame(topic_data, columns=['topic_id', 'term_rank', 'term_str'])\\\n    .set_index(['topic_id','term_rank']).term_str.unstack()\n\n\nTOPIC.head(20)\n\n\n\n\n\n  \n    \n      term_rank\n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n    \n      topic_id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      image\n      color\n      ho\n      formats\n      palette\n      size\n      outputs\n      replies\n      display\n      header\n    \n    \n      1\n      joy\n      canadian\n      bullets\n      rubber\n      rifle\n      censorship\n      onethird\n      sluggish\n      oversized\n      soul\n    \n    \n      2\n      have\n      my\n      with\n      or\n      \n      me\n      if\n      can\n      get\n      but\n    \n    \n      3\n      gays\n      seller\n      buffalo\n      lynn\n      suny\n      traded\n      cones\n      \n      peninsula\n      towel\n    \n    \n      4\n      god\n      jesus\n      christ\n      bible\n      lord\n      christians\n      he\n      we\n      christian\n      christianity\n    \n    \n      5\n      film\n      greg\n      instructions\n      allocation\n      festival\n      269\n      debris\n      712\n      drug\n      gathering\n    \n    \n      6\n      7th\n      beneficial\n      leak\n      init\n      til\n      income\n      tagged\n      \n      was\n      fines\n    \n    \n      7\n      102\n      hack\n      carpet\n      distinctions\n      commenting\n      killers\n      episode\n      143\n      sloan\n      architect\n    \n    \n      8\n      bds\n      export\n      japanese\n      intensive\n      transparent\n      inflated\n      gibson\n      straightforward\n      fuer\n      growth\n    \n    \n      9\n      \n      radio\n      air\n      by\n      tube\n      at\n      with\n      or\n      from\n      280\n    \n    \n      10\n      was\n      he\n      they\n      out\n      when\n      my\n      at\n      were\n      me\n      tape\n    \n    \n      11\n      object\n      circle\n      maker\n      exhibit\n      hoped\n      strawman\n      approved\n      methodology\n      failures\n      cleaned\n    \n    \n      12\n      diamond\n      lightning\n      simms\n      offers\n      swept\n      meg\n      star\n      80ns\n      attended\n      viper\n    \n    \n      13\n      church\n      christian\n      catholic\n      persecution\n      their\n      doctrine\n      by\n      not\n      as\n      roman\n    \n    \n      14\n      not\n      have\n      but\n      he\n      as\n      with\n      what\n      was\n      if\n      do\n    \n    \n      15\n      todd\n      stamps\n      lousy\n      deposit\n      cheesy\n      \n      they\n      from\n      superhuman\n      was\n    \n    \n      16\n      \n      will\n      or\n      as\n      can\n      format\n      with\n      if\n      copy\n      which\n    \n    \n      17\n      1\n      2\n      3\n      0\n      4\n      \n      5\n      6\n      8\n      12\n    \n    \n      18\n      temperature\n      750\n      angels\n      500\n      youths\n      incredible\n      02\n      curve\n      fills\n      06\n    \n    \n      19\n      irrational\n      guitar\n      ee\n      \n      pens\n      cigarette\n      ama\n      infiltrated\n      horribly\n      physician"
  },
  {
    "objectID": "lessons/M08a_Visualization/M08a_02_GensimRunCompareLDA.html",
    "href": "lessons/M08a_Visualization/M08a_02_GensimRunCompareLDA.html",
    "title": "Metadata",
    "section": "",
    "text": "Set Up\nAcquire and clean up the 20 Newsgroups dataset.\nConvert a documents to list of tokens.\nCreate Gensim dictionary.\nConvert list of tokens to bag of word representation\nWe use two slightly different visualization methods depending on how you’re running this tutorial.\nGensim can help you visualise the differences between topics. For this purpose, you can use the diff() method of LdaModel.\ndiff() returns a matrix with distances mdiff and a matrix with annotations annotation. Read the docstring for more detailed info.\nIn each mdiff[i][j] cell you’ll find a distance between topic_i from the first model and topic_j from the second model.\nIn each annotation[i][j] cell you’ll find [tokens from intersection, tokens from difference between topic_i from first model and topic_j from the second model."
  },
  {
    "objectID": "lessons/M08a_Visualization/M08a_02_GensimRunCompareLDA.html#config",
    "href": "lessons/M08a_Visualization/M08a_02_GensimRunCompareLDA.html#config",
    "title": "Metadata",
    "section": "Config",
    "text": "Config\n\nnum_topics = 15"
  },
  {
    "objectID": "lessons/M08a_Visualization/M08a_02_GensimRunCompareLDA.html#imports",
    "href": "lessons/M08a_Visualization/M08a_02_GensimRunCompareLDA.html#imports",
    "title": "Metadata",
    "section": "Imports",
    "text": "Imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nimport plotly.offline as py\n\n\nfrom string import punctuation\nfrom nltk import RegexpTokenizer\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn.datasets import fetch_20newsgroups\nfrom gensim.corpora import Dictionary\nfrom gensim.models import LdaMulticore"
  },
  {
    "objectID": "lessons/M08a_Visualization/M08a_02_GensimRunCompareLDA.html#case-1-how-topics-within-one-model-correlate-with-each-other.",
    "href": "lessons/M08a_Visualization/M08a_02_GensimRunCompareLDA.html#case-1-how-topics-within-one-model-correlate-with-each-other.",
    "title": "Metadata",
    "section": "Case 1: How topics within ONE model correlate with each other.",
    "text": "Case 1: How topics within ONE model correlate with each other.\nShort description:\n\nx-axis - topic1\ny-axis - topic2\nBLUE: strongly decorrelated topics\nRED: strongly correlated topics\n\nIn an ideal world, we would like to see different topics decorrelated between themselves.\nIn this case, our matrix would look like this:\n\nmdiff = np.ones((num_topics, num_topics))\nnp.fill_diagonal(mdiff, 0.)\n\n\nplot_difference(mdiff, title=\"Topic difference (one model) in ideal world\")\n\n\n                                                \n\n\nUnfortunately, in real life, not everything is so good, and the matrix looks different.\nShort description (interactive annotations only):\n\n+++ make, world, well - words from the intersection of topics = present in both topics;\n--- money, day, still - words from the symmetric difference of topics = present in one topic but not the other.\n\n\nmdiff, annotation = lda_fst.diff(lda_fst, distance='jaccard', num_words=50)\n\n\nplot_difference(mdiff, title=\"Topic difference (one model) [jaccard distance]\", annotation=annotation)\n\n\n                                                \n\n\nIf you compare a model with itself, you want to see as many red elements as possible (except on the diagonal). With this picture, you can look at the “not very red elements” and understand which topics in the model are very similar and why (you can read annotation if you move your pointer to cell).\nJaccard is a stable and robust distance function, but sometimes not sensitive enough. Let’s try to use the Hellinger distance instead.\n\nmdiff, annotation = lda_fst.diff(lda_fst, distance='hellinger', num_words=50)\nplot_difference(mdiff, title=\"Topic difference (one model)[hellinger distance]\", annotation=annotation)\n\n\n                                                \n\n\nYou see that everything has become worse, but remember that everything depends on the task.\nChoose a distance function that matches your upstream task better: what kind of “similarity” is relevant to you. From my (Ivan’s) experience, Jaccard is fine."
  },
  {
    "objectID": "lessons/M08a_Visualization/M08a_02_GensimRunCompareLDA.html#case-2-how-topics-from-different-models-correlate-with-each-other.",
    "href": "lessons/M08a_Visualization/M08a_02_GensimRunCompareLDA.html#case-2-how-topics-from-different-models-correlate-with-each-other.",
    "title": "Metadata",
    "section": "Case 2: How topics from DIFFERENT models correlate with each other.",
    "text": "Case 2: How topics from DIFFERENT models correlate with each other.\nSometimes, we want to look at the patterns between two different models and compare them.\nYou can do this by constructing a matrix with the difference.\n\nmdiff, annotation = lda_fst.diff(lda_snd, distance='jaccard', num_words=50)\nplot_difference(mdiff, title=\"Topic difference (two models)[jaccard distance]\", annotation=annotation)\n\n\n                                                \n\n\nLooking at this matrix, you can find similar and different topics between the two models. The plot also includes relevant tokens describing the topics’ intersection and difference."
  },
  {
    "objectID": "lessons/M08a_Visualization/M08a_03_MALLET.html",
    "href": "lessons/M08a_Visualization/M08a_03_MALLET.html",
    "title": "Metadata",
    "section": "",
    "text": "Notes\nSee Appendix below information on how download and install MALLET.\nDownload MALLET here | Mazo, a wrapper around MALLET to organize its output.\nWebsite: https://mimno.github.io/Mallet/\nMALLET is a Java-based package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text.\nMALLET includes sophisticated tools for document classification: efficient routines for converting text to “features”, a wide variety of algorithms (including Naïve Bayes, Maximum Entropy, and Decision Trees), and code for evaluating classifier performance using several commonly used metrics.\nIn addition to classification, MALLET includes tools for sequence tagging for applications such as named-entity extraction from text. Algorithms include Hidden Markov Models, Maximum Entropy Markov Models, and Conditional Random Fields. These methods are implemented in an extensible system for finite state transducers.\nTopic models are useful for analyzing large collections of unlabeled text. The MALLET topic modeling toolkit contains efficient, sampling-based implementations of Latent Dirichlet Allocation, Pachinko Allocation, and Hierarchical LDA.\nMany of the algorithms in MALLET depend on numerical optimization. MALLET includes an efficient implementation of Limited Memory BFGS, among many other optimization methods.\nIn addition to sophisticated Machine Learning applications, MALLET includes routines for transforming text documents into numerical representations that can then be processed efficiently. This process is implemented through a flexible system of “pipes”, which handle distinct tasks such as tokenizing strings, removing stopwords, and converting sequences into count vectors.\nAn add-on package to MALLET, called GRMM, contains support for inference in general graphical models, and training of CRFs with arbitrary graphical structure."
  },
  {
    "objectID": "lessons/M08a_Visualization/M08a_03_MALLET.html#dump-corpus-to-csv-file",
    "href": "lessons/M08a_Visualization/M08a_03_MALLET.html#dump-corpus-to-csv-file",
    "title": "Metadata",
    "section": "Dump corpus to CSV file",
    "text": "Dump corpus to CSV file\n\nmallet_corpus = DOC.join(LIB)[['doc_str','author_id']]\nmallet_corpus.columns = 'doc_content doc_label'.split()\nmallet_corpus[['doc_label','doc_content']].to_csv('corpus/novels-corpus.csv', index=False)"
  },
  {
    "objectID": "lessons/M08a_Visualization/M08a_03_MALLET.html#mallet-time",
    "href": "lessons/M08a_Visualization/M08a_03_MALLET.html#mallet-time",
    "title": "Metadata",
    "section": "MALLET Time",
    "text": "MALLET Time\n\nShow MALLET options\n\nmallet_home = \"/Users/rca2t1/opt/mallet/bin\"\n\n\n! {mallet_home}/mallet \n\nUnrecognized command: \nMallet 2.0 commands: \n\n  import-dir         load the contents of a directory into mallet instances (one per file)\n  import-file        load a single file into mallet instances (one per line)\n  import-svmlight    load SVMLight format data files into Mallet instances\n  info               get information about Mallet instances\n  train-classifier   train a classifier from Mallet data files\n  classify-dir       classify data from a single file with a saved classifier\n  classify-file      classify the contents of a directory with a saved classifier\n  classify-svmlight  classify data from a single file in SVMLight format\n  train-topics       train a topic model from Mallet data files\n  infer-topics       use a trained topic model to infer topics for new documents\n  evaluate-topics    estimate the probability of new documents under a trained model\n  prune              remove features based on frequency or information gain\n  split              divide data into testing, training, and validation portions\n  bulk-load          for big input files, efficiently prune vocabulary and import docs\n\nInclude --help with any option for more information\n\n\n\n\nImport corpus\n\n!{mallet_home}/mallet import-file --input corpus/novels-corpus.csv --output mallet/novels-corpus.mallet --keep-sequence TRUE\n\n\n\nTrain topics\n\n!{mallet_home}/mallet train-topics --input mallet/novels-corpus.mallet --num-topics {num_topics} --num-iterations {num_iters} \\\n--output-doc-topics mallet/novels-doc-topics.txt \\\n--output-topic-keys mallet/novels-topic-keys.txt \\\n--word-topic-counts-file mallet/novels-word-topic-counts-file.txt \\\n--topic-word-weights-file mallet/novels-topic-word-weights-file.txt \\\n--xml-topic-report mallet/novels-topic-report.xml \\\n--xml-topic-phrase-report mallet/novels-topic-phrase-report.xml \\\n--show-topics-interval {show_interval} \\\n--use-symmetric-alpha false  \\\n--optimize-interval 100 \\\n--diagnostics-file mallet/novels-diagnostics.xml\n\nMallet LDA: 20 topics, 5 topic bits, 11111 topic mask\nData loaded.\nmax tokens: 15717\ntotal tokens: 1164070\n<10> LL/token: -9.08018\n<20> LL/token: -8.59597\n<30> LL/token: -8.41396\n<40> LL/token: -8.31237\n<50> LL/token: -8.24817\n<60> LL/token: -8.20836\n<70> LL/token: -8.17174\n<80> LL/token: -8.15141\n<90> LL/token: -8.12778\n\n0   0.25    the manfred said thou not this project isabella gutenberg and his matilda thy with lord for work theodore thee any \n1   0.25    the and was had been from which with that some into his have where even part years doubt possible house \n2   0.25    the and his him said sir they lord that their with them all scrooge was were for baron philip father \n3   0.25    that and you the was his holmes had not but have very for would are him yes said all there \n4   0.25    and that was but not had she could her might father for when should own day before did after only \n5   0.25    the you and tuppence tommy was that said but dont she for her with sir they know julius well what \n6   0.25    the and that they with was which where them count now aubert this their had after over for upon often \n7   0.25    and that for all not the his him when with her but have was there shall see are them van \n8   0.25    the was she her had door and room out then with were not into could but night hand there one \n9   0.25    you that the have his said him your which was and what when time man one can had sir has \n10  0.25    the and was they were with its from their but which when again had yet that towards there saw who \n11  0.25    her she and the emily that had was said not for but this you montoni have would valancourt with who \n12  0.25    the and had for miss her that with was out sergeant franklin all this she rachel diamond house from betteredge \n13  0.25    the and his which had that him with for when from while was those mind never since much once how \n14  0.25    the her was she with and his for upon him had which their them who that himself without mother were \n15  0.25    the and upon his was with which for had there our man out could that were down one all over \n16  0.25    her and not she very for the they was you with all but would their are catherine were could much \n17  0.25    his her and from the that this not with all now upon would every more still heart death him which \n18  0.25    you your will and have that but the not shall are for this what him should said may then with \n19  0.25    the that have this not been but for any and from are all will has would one very death its \n\n<100> LL/token: -8.10698\n<110> LL/token: -8.09333\n<120> LL/token: -8.07761\n<130> LL/token: -8.06214\n<140> LL/token: -8.05182\n<150> LL/token: -8.04228\n<160> LL/token: -8.03262\n<170> LL/token: -8.02137\n<180> LL/token: -8.01462\n<190> LL/token: -8.00618\n\n0   0.25    the said manfred thou his not project isabella gutenberg this and thy lord matilda with for theodore work thee hippolita \n1   0.25    the and was with that been had which have from very other some own who his were into such house \n2   0.25    and the his him said lord they sir edmund all scrooge their that them baron for with philip father oswald \n3   0.25    the that you and holmes was not his have but had for very all yes sir poirot she would are \n4   0.25    and was but had that not father for might when could did should more life all with most friend were \n5   0.25    the you she tuppence her tommy and but that said julius dont was they all out know sir thats for \n6   0.25    the and they with that now upon aubert their where was them some count while over woods often whose chateau \n7   0.25    and the that all for her not his she him when with are there have but van see helsing our \n8   0.25    the was and had not but door out room were could his then into there hand with night back them \n9   0.25    you that his have him said the what and this your one time which when any there more would are \n10  0.25    the and they its which from this were their with that but was who one these saw when more like \n11  0.25    her she and the emily that was had for not but said this montoni would valancourt who annette madame with \n12  0.25    the and her had she for all sergeant franklin miss with out rachel diamond betteredge house was lady that see \n13  0.25    the his and which had with from him for that when than much was while these time too mind those \n14  0.25    her the she was and this with for had their them herself upon himself antonia which were till ambrosio being \n15  0.25    the and his upon which with for there had man was our all out down that could see were two \n16  0.25    her she and not the for very they all catherine was but could miss their would mrs them such with \n17  0.25    his and that the him upon with from not every all your more now death still agnes would lorenzo myself \n18  0.25    you have your and will that not the but this for shall are what said should him then must can \n19  0.25    the that have this not but been all for are its would will upon one now has from there any \n\n<200> LL/token: -7.99409\n<210> LL/token: -7.98469\n<220> LL/token: -7.9812\n<230> LL/token: -7.97392\n<240> LL/token: -7.96722\n<250> LL/token: -7.95937\n<260> LL/token: -7.95474\n<270> LL/token: -7.95046\n<280> LL/token: -7.94653\n<290> LL/token: -7.94138\n\n0   0.25    the said manfred thou not his project isabella gutenberg and lord for thy matilda this theodore hippolita princess work thee \n1   0.25    the and was that had been with which from have very who some were other found own first house more \n2   0.25    and the his him said lord they sir edmund scrooge baron them their all upon with for philip that oswald \n3   0.25    the that you holmes not had his have she but sir was and poirot yes will all been very mrs \n4   0.25    and was but not that for when had might all did could life before should father myself only every felt \n5   0.25    the you tuppence she her tommy but all julius said dont they know thats and sir like out got well \n6   0.25    the and they now that with upon their aubert was them some where while woods often seemed chateau count scene \n7   0.25    and the that all her for his not she when him with have are there our see van shall helsing \n8   0.25    the was and had but not door out could for there room were into hand with what then night them \n9   0.25    you that and his him have said for the this when what one about your any little they from can \n10  0.25    the and which with they that from this were their its but had was through these who when more place \n11  0.25    her she the emily and that had was not montoni said for but valancourt this who now would when annette \n12  0.25    the her had and she miss all sergeant franklin out with rachel diamond betteredge for house lady was see blake \n13  0.25    the his and had which with him was from for that than while could when those much whom time these \n14  0.25    her she the and was this their with herself which for himself mother without than till most had being found \n15  0.25    the and his upon there which all that man with our for had down out was very could over would \n16  0.25    her she and not for very was catherine they could all but had miss their such mrs would the were \n17  0.25    his the upon him that with from your not now more and agnes antonia every all still ambrosio lorenzo matilda \n18  0.25    you have that your and not will but this the are what shall for with said may should then can \n19  0.25    the not have that this upon been for but all are its which one there would has any will body \n\n[beta: 0.01804] \n<300> LL/token: -7.93727\n<310> LL/token: -7.93499\n<320> LL/token: -7.93118\n<330> LL/token: -7.92181\n<340> LL/token: -7.91845\n<350> LL/token: -7.91199\n<360> LL/token: -7.91109\n<370> LL/token: -7.90398\n<380> LL/token: -7.90158\n<390> LL/token: -7.90034\n\n0   0.11209 the manfred said thou not isabella project gutenberg his thy matilda lord theodore hippolita with thee princess prince castle this \n1   0.37623 the and that was with had been from this have which some were who very other only more much found \n2   0.15902 and his the said lord him they sir edmund scrooge upon baron their all them for philip with will oswald \n3   0.16959 that you not the sir but was poirot she mrs yes all inglethorp will very his had have did john \n4   0.33021 and but that not was when had for myself did life before all could man eyes our death father might \n5   0.17589 you the tuppence tommy she her but all julius sir dont said out thats they know got get well girl \n6   0.23325 the and with they aubert that now woods their often them where among mountains upon into for count scene blanche \n7   0.19155 and all that for his the her not she when him there van with our helsing see are but shall \n8   0.3668  the was and had not with but could out door for room were into hand him then there eyes down \n9   0.35383 and you that his have him said for when what one there about they was man any now little this \n10  0.3288  the and which they from that with were this was but their its had one when these who through more \n11  0.18207 her she the emily and that had not montoni said but was this valancourt now for who all annette madame \n12  0.16892 the had her all she miss sergeant franklin out which rachel diamond betteredge and house lady blake see them for \n13  0.34336 the his and which had was with for him from would that when than could upon were while made time \n14  0.27176 her she and the was herself this for mother been till their who soon most woman young found might having \n15  0.27995 his and the upon which holmes all there our man had for out down very see that one could with \n16  0.23579 her she and not catherine very was for all they could but miss had mrs their them tilney were with \n17  0.19201 the his upon with from him that and not agnes antonia your all now ambrosio lorenzo still every matilda them \n18  0.35723 you that and have your not but will this the are what shall said for with should him can may \n19  0.23089 the not upon have this all that but been its for now has one will which body than any are \n\n[beta: 0.02116] \n<400> LL/token: -7.89281\n<410> LL/token: -7.88954\n<420> LL/token: -7.88054\n<430> LL/token: -7.8763\n<440> LL/token: -7.87617\n<450> LL/token: -7.87206\n<460> LL/token: -7.86841\n<470> LL/token: -7.86475\n<480> LL/token: -7.86125\n<490> LL/token: -7.86299\n\n0   0.05627 the manfred said thou his isabella gutenberg project not lord thy matilda theodore for hippolita thee this princess any prince \n1   0.49077 the that was and had this been with from have which were some who other not such only their found \n2   0.10597 and his the said lord him sir they edmund scrooge upon baron them their all philip for will oswald lovel \n3   0.12051 that sir not poirot was mrs you but inglethorp yes she all had john moor very will henry did cavendish \n4   0.35184 and but not when was for that had myself could now life upon did even father before eyes yet our \n5   0.12123 tuppence tommy you but she julius the all her sir dont they thats james mrs his get girl know jane \n6   0.20404 the and aubert with they them now woods where their among upon mountains blanche whose along scene after while road \n7   0.14527 and all that for not his her she when him our the van are helsing there but may them came \n8   0.46493 the was and had with not could for out but his there room door were into down been night then \n9   0.43171 and you that his said for him have was what one about now they with there when has any see \n10  0.38694 the and which that they from with was were but their this its who one through had more these some \n11  0.13968 her she the emily and that had not but montoni said was now for valancourt this who when annette madame \n12  0.11579 the had miss sergeant all franklin which rachel diamond betteredge and them own lady house time blake cuff when your \n13  0.41403 and the his which had was with for him that from would than could when time made were one whom \n14  0.28212 her she and was herself mother daughter for lady woman young being only father take first having idea been their \n15  0.2684  his upon and which holmes all there our that have the man very would are could came but two then \n16  0.18482 her and not she catherine for very they all but could was miss their mrs tilney them would such had \n17  0.12366 the his upon with not him your agnes antonia every them ambrosio lorenzo still now matilda moment from more for \n18  0.45444 you that have not but your the and will this are what shall said for with then can should all \n19  0.17833 the upon have not but all for its one been there would which will now has are this any body \n\n[beta: 0.02253] \n<500> LL/token: -7.85965\n<510> LL/token: -7.8541\n<520> LL/token: -7.8527\n<530> LL/token: -7.84896\n<540> LL/token: -7.84638\n<550> LL/token: -7.84432\n<560> LL/token: -7.84488\n<570> LL/token: -7.84635\n<580> LL/token: -7.84587\n<590> LL/token: -7.84478\n\n0   0.03777 the manfred said thou not his isabella gutenberg project thy for matilda lord theodore hippolita thee princess work any prince \n1   0.58962 the that and was had this been from with which have some were who not only other most found such \n2   0.07804 and his the him said lord sir they edmund scrooge upon them baron their for philip all oswald will lovel \n3   0.08505 poirot not sir that inglethorp mrs but you yes had was john she did all moor cavendish very henry been \n4   0.32186 and but was when not for had myself that life upon our every before yet man feelings could being father \n5   0.08916 tuppence tommy julius but you all sir she dont they james mrs thats jane know get girl her well its \n6   0.18003 the and aubert with they their woods them where now blanche upon mountains whose scene among road over along till \n7   0.10786 and all that for not his when him our she her van there helsing may shall must are lucy which \n8   0.54486 the was and had with his not but could for room then door out there them were into back hand \n9   0.49153 and you that for his him have said was with one out when what there about now they any very \n10  0.42004 the and that they which from with were this was but their its one more who have through light some \n11  0.11231 her she the emily and that had not said montoni but was for valancourt now this annette madame count who \n12  0.08802 the had miss all sergeant franklin which rachel diamond betteredge your house own lady blake two cuff godfrey and rosanna \n13  0.46724 and the his was had which with for him from that would when could more than were time too while \n14  0.29674 her she and was herself but been mother before lady daughter woman father young words same only made found mothers \n15  0.21326 his upon which and holmes all our there had very have man two would that could down watson one has \n16  0.13872 not her and she catherine very they for all but could miss had mrs tilney their such them would being \n17  0.08332 the his upon not your him agnes antonia with ambrosio lorenzo every still for them matilda till bosom moment himself \n18  0.53795 you the that and have not but your will this what are said shall with should then can all must \n19  0.13396 the upon have not for its all but which been one would there this are will has any body about \n\n[beta: 0.02316] \n<600> LL/token: -7.84277\n<610> LL/token: -7.84203\n<620> LL/token: -7.84648\n<630> LL/token: -7.84271\n<640> LL/token: -7.84515\n<650> LL/token: -7.84346\n<660> LL/token: -7.84458\n<670> LL/token: -7.84603\n<680> LL/token: -7.84372\n<690> LL/token: -7.84266\n\n0   0.02892 the said manfred thou his isabella gutenberg project not thy matilda lord theodore thee this hippolita with princess prince for \n1   0.66825 the that and was this had been with from which who some have not were them only most their found \n2   0.06176 and his the lord him said sir they edmund scrooge them upon baron philip their all will for with oswald \n3   0.06022 that poirot sir not mrs inglethorp was but yes had you john moor she did cavendish been all henry there \n4   0.27603 and but not had when life was myself eyes one father yet our feelings their might did for now elizabeth \n5   0.06839 tuppence tommy julius you all but she sir they said mrs james dont jane thats her know girl its vandemeyer \n6   0.16123 the and aubert they their now with woods where them mountains over among blanche little along road till then sun \n7   0.08574 and all that for not his when she her him our van there are helsing which shall may but lucy \n8   0.60405 the was and had his with not for but door could room then out were there been into down them \n9   0.53556 and you that his for him said have was with out one about now there what they little any would \n10  0.44628 the and which that they with from were this was but their its upon all who some these one more \n11  0.09074 she her the emily and had that but montoni said not was now valancourt this annette madame who count when \n12  0.07214 the had which miss sergeant franklin all rachel diamond betteredge your time when two blake own cuff house what sir \n13  0.51134 and the his was with had which for him that from would more when could than were one time upon \n14  0.32139 her she and but herself was mother woman lady before young daughter father only again should been found words dear \n15  0.16702 his upon which holmes and all our there that have very you man has had two would one are watson \n16  0.10336 not and her catherine she very for was they but could miss all mrs had such their tilney them being \n17  0.06338 his the upon not your him antonia agnes ambrosio lorenzo every was with matilda them till now still bosom himself \n18  0.60662 you the that not and have but your will this what are said shall for all him with should then \n19  0.10424 the have upon not for but all its been one there any would are will body this about which thus \n\n[beta: 0.02348] \n<700> LL/token: -7.84077\n<710> LL/token: -7.83675\n<720> LL/token: -7.83602\n<730> LL/token: -7.83592\n<740> LL/token: -7.83498\n<750> LL/token: -7.83508\n<760> LL/token: -7.83264\n<770> LL/token: -7.83079\n<780> LL/token: -7.83292\n<790> LL/token: -7.83617\n\n0   0.02438 the manfred said thou not isabella project gutenberg his lord thy matilda this theodore hippolita thee princess prince work for \n1   0.72848 the that and was this been had from have with which not were who some his found them their house \n2   0.053   and his the said lord sir him they edmund scrooge baron philip them upon their all for oswald will lovel \n3   0.04773 that poirot sir not was inglethorp mrs but yes john she did moor all had cavendish henry there you his \n4   0.24035 and but not when myself life yet one our did feelings these every miserable elizabeth before death even their human \n5   0.05627 tuppence tommy but julius you all sir dont they james mrs jane thats then well vandemeyer young know girl said \n6   0.1458  the and aubert with they their where woods now them over mountains along among blanche little whose road into air \n7   0.0735  and all that for not his when our van but her may him she helsing shall which have are lucy \n8   0.63574 the was and had his with not there could room door out then for but them were been into down \n9   0.56895 and that you for was his him said with have one out about what had there when they any now \n10  0.46925 the and that which they with from was were this but their upon its for had who some all through \n11  0.07946 her she the emily that not but had and montoni said now valancourt was this annette madame count then who \n12  0.06546 the had miss sergeant franklin all which rachel diamond betteredge house time own lady room blake cuff two godfrey again \n13  0.54898 and the his was had with which for that him from would when could more than were upon time one \n14  0.34198 her she herself was and mother but lady woman daughter before should young father our again being dear girl child \n15  0.13221 his upon which holmes our all that there and have very has had you but man would two watson case \n16  0.08579 not catherine and very her but they she all for could miss mrs tilney their such had them general isabella \n17  0.05197 the his upon not your him agnes antonia every ambrosio with lorenzo them matilda till bosom himself but now convent \n18  0.66548 you the that have and but your not will this what for are said shall all with him who must \n19  0.0819  the upon have not for but its one all been which this there would any will are now body corpse \n\n[beta: 0.02363] \n<800> LL/token: -7.83773\n<810> LL/token: -7.84024\n<820> LL/token: -7.83549\n<830> LL/token: -7.83586\n<840> LL/token: -7.83614\n<850> LL/token: -7.83538\n<860> LL/token: -7.8357\n<870> LL/token: -7.83683\n<880> LL/token: -7.83582\n<890> LL/token: -7.83568\n\n0   0.02137 the manfred said thou not isabella gutenberg project his matilda thy lord theodore this hippolita thee princess prince with work \n1   0.77249 the and that was this had with been from have which his who some were not them found their house \n2   0.04679 and his the lord said him sir they edmund scrooge baron them philip upon their all with oswald lovel your \n3   0.03831 poirot sir that not inglethorp mrs was yes but john had moor all did cavendish you been his henry one \n4   0.21148 and but not when myself life did one our eyes yet even saw elizabeth being before now only man feelings \n5   0.05278 tuppence tommy but julius you all sir they dont james his mrs thats jane know girl well vandemeyer said got \n6   0.1376  the and aubert with they now woods their where among over them road blanche little air scene mountains after distance \n7   0.06791 and all that his not for when him our but van helsing may shall are there she lucy must have \n8   0.65316 the was and had his not door with could room but then for there been were them hand into back \n9   0.59707 and that you for was him said out with have about one there what had his little any now would \n10  0.4822  the and that which with from they was were this but upon their had all who for its some into \n11  0.07298 she her the emily and had but not that montoni said was now valancourt annette madame count when this who \n12  0.06405 the had which miss sergeant franklin all rachel diamond his betteredge time when your own blake house room cuff godfrey \n13  0.58627 and the his was had with which for that him from would could more when were upon than one heart \n14  0.35952 her she was herself and mother woman before lady but first again out daughter wife father eyes dear child part \n15  0.11438 his which upon holmes and that our all have there man very has would you one are watson sherlock case \n16  0.07561 not and catherine very but they could miss all mrs her tilney for such being them their were she general \n17  0.04397 his the upon your not him antonia agnes ambrosio lorenzo them matilda till every bosom now convent moment being though \n18  0.71708 you the that have not but will and your this what for said are him shall with who all then \n19  0.07067 the have upon not but for all one which would this are its been there will any about body now \n\n[beta: 0.02363] \n<900> LL/token: -7.83509\n<910> LL/token: -7.83524\n<920> LL/token: -7.83645\n<930> LL/token: -7.83813\n<940> LL/token: -7.83812\n<950> LL/token: -7.8392\n<960> LL/token: -7.84014\n<970> LL/token: -7.83786\n<980> LL/token: -7.83861\n<990> LL/token: -7.83869\n\n0   0.0195  the manfred said thou not isabella project gutenberg matilda thy lord his theodore hippolita princess thee prince work this any \n1   0.81121 the and that was with this been have from which his had were some who found not these their they \n2   0.04306 and his the lord said him sir they edmund scrooge baron upon philip them their your will all oswald lovel \n3   0.03456 that poirot sir not mrs inglethorp had but his john all was moor yes did cavendish been henry she you \n4   0.19247 and but not when myself one life did our before eyes miserable elizabeth now father should yet feelings being man \n5   0.04741 tuppence tommy you julius but all sir dont his james thats mrs jane they girl know well vandemeyer young right \n6   0.13556 the and aubert with they their woods where now blanche among little over mountains sun road air along his till \n7   0.06105 and all his that not for when van our him her helsing she which shall may are lucy must there \n8   0.65916 the was and had his not with there door room then been but could out into hand back away seemed \n9   0.62091 and that was for you him said with have one out had about what there now little when time his \n10  0.49278 the and that was which they from were with but this their for had upon who its all one them \n11  0.07077 she her the emily not and that had montoni but said was now valancourt annette madame count who when then \n12  0.06079 the had miss sergeant franklin which all rachel diamond betteredge his own time house blake when lady cuff room godfrey \n13  0.61775 and the his was had with which for that him from would when could more than were upon all heart \n14  0.37329 her she was and herself but out before woman mother again lady young should father daughter dear friends about only \n15  0.10324 his which upon holmes all that and our had you man very there has have would watson well sherlock one \n16  0.06924 not catherine very and they could but all miss mrs tilney for their being been such were general isabella would \n17  0.03931 the his upon not your him with agnes antonia ambrosio lorenzo matilda now them every bosom being till convent though \n18  0.76189 you the that have but not and your will this what for are said shall him all who can has \n19  0.06219 the not upon have but all been for one its which this are would now will body about thus there \n\n[beta: 0.02376] \n<1000> LL/token: -7.83938\n\nTotal time: 3 minutes 6 seconds"
  },
  {
    "objectID": "lessons/M08a_Visualization/M08a_03_MALLET.html#installation",
    "href": "lessons/M08a_Visualization/M08a_03_MALLET.html#installation",
    "title": "Metadata",
    "section": "Installation",
    "text": "Installation\nTo build a Mallet 2.0 development release, you must have the Apache ant build tool installed. From the command prompt, first change to the mallet directory, and then type ant\nIf ant finishes with \"BUILD SUCCESSFUL\", Mallet is now ready to use.\nIf you would like to deploy Mallet as part of a larger application, it is helpful to create a single “.jar” file that contains all of the compiled code. Once you have compiled the individual Mallet class files, use the command: ant jar\nThis process will create a file “mallet.jar” in the “dist” directory within Mallet."
  },
  {
    "objectID": "lessons/M08a_Visualization/M08a_03_MALLET.html#usage",
    "href": "lessons/M08a_Visualization/M08a_03_MALLET.html#usage",
    "title": "Metadata",
    "section": "Usage",
    "text": "Usage\nOnce you have installed Mallet you can use it using the following command:\nbin/mallet [command] --option value --option value ...\nType bin/mallet to get a list of commands, and use the option --help with any command to get a description of valid options.\nFor details about the commands please visit the API documentation and website at: https://mimno.github.io/Mallet/"
  },
  {
    "objectID": "lessons/M08a_Visualization/M08a_03_MALLET.html#list-of-algorithms",
    "href": "lessons/M08a_Visualization/M08a_03_MALLET.html#list-of-algorithms",
    "title": "Metadata",
    "section": "List of Algorithms",
    "text": "List of Algorithms\n\nTopic Modelling\n\nLDA\nParallel LDA\nDMR LDA\nHierarchical LDA\nLabeled LDA\nPolylingual Topic Model\nHierarchical Pachinko Allocation Model (PAM)\nWeighted Topic Model\nLDA with integrated phrase discovery\nWord Embeddings (word2vec) using skip-gram with negative sampling\n\nClassification\n\nAdaBoost\nBagging\nWinnow\nC45 Decision Tree\nEnsemble Trainer\nMaximum Entropy Classifier (Multinomial Logistic Regression)\nNaive Bayes\nRank Maximum Entropy Classifier\nPosterior Regularization Auxiliary Model\n\nClustering\n\nGreedy Agglomerative\nHill Climbing\nK-Means\nK-Best\n\nSequence Prediction Models\n\nConditional Random Fields\nMaximum Entropy Markov Models\nHidden Markov Models\nSemi-Supervised Sequence Prediction Models\n\nLinear Regression"
  },
  {
    "objectID": "lessons/M08a_Visualization/M08a_04_Mazo.html",
    "href": "lessons/M08a_Visualization/M08a_04_Mazo.html",
    "title": "Metadata",
    "section": "",
    "text": "Synopsis"
  },
  {
    "objectID": "lessons/M08a_Visualization/M08a_04_Mazo.html#config",
    "href": "lessons/M08a_Visualization/M08a_04_Mazo.html#config",
    "title": "Metadata",
    "section": "Config",
    "text": "Config\n\n!ls -l output/\n\ntotal 8\ndrwxr-xr-x@ 12 rca2t1  staff  384 Mar 23 13:44 demo-20-16795933002483132\ndrwxr-xr-x@ 12 rca2t1  staff  384 Mar 23 18:15 demo-20-16796095708890848\ndrwxr-xr-x@ 12 rca2t1  staff  384 Mar 23 18:20 demo-20-16796098938625019\ndrwxr-xr-x@ 12 rca2t1  staff  384 Mar 23 13:32 novels-20-16795927188712971\ndrwxr-xr-x@ 12 rca2t1  staff  384 Mar 23 16:10 novels-20-16796021757424011\n-rw-r--r--@  1 rca2t1  staff   41 Mar 23 14:24 readme.txt\n\n\n\nmodel_id = 'demo-20-16796098938625019'\n\n\nmazo_tables = f\"./output/{model_id}/tables/*.*\"\ndb_file = f\"./db/{model_id}.db\""
  },
  {
    "objectID": "lessons/M08a_Visualization/M08a_04_Mazo.html#imports",
    "href": "lessons/M08a_Visualization/M08a_04_Mazo.html#imports",
    "title": "Metadata",
    "section": "Imports",
    "text": "Imports\n\nimport pandas as pd\nimport numpy as np\nfrom glob import glob\nimport sqlite3\n\n\nimport mazo.polite\n\nUsing /Users/rca2t1/opt/mallet/bin/mallet as mallet.\nUsing ./output as output directory.\nPlease enter an integer for the number of topics.\n\n\nAssertionError: \n\n\n\npolite.polite.Polite.schema['PHRASE'].index\n\n['phrase_str']"
  },
  {
    "objectID": "lessons/M09_WordEmbedding/M09_01_GloVe.html",
    "href": "lessons/M09_WordEmbedding/M09_01_GloVe.html",
    "title": "Metadata",
    "section": "",
    "text": "Course:    DS 5001 \nModule:    09 Lab\nTopic:     Using GloVe\nAuthor:    R.C. Alvarado\nDate:      28 March 2023 (revised)\nPurpose: We use some pretrained word vectors from the developers of GloVe.\n\nSet Up\n\ndata_in = '../data/glove'\n# db_file = f'{data_in}/glove2.db'\n\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\n\nImport GloVe data\n\nglove = pd.read_csv(f\"{data_in}/glove.6B/glove.6B.50d.txt\", sep=\"\\s\", header=None)\n\n/var/folders/14/rnyfspnx2q131jp_752t9fc80000gn/T/ipykernel_41125/3908683698.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n  glove = pd.read_csv(f\"{data_in}/glove.6B/glove.6B.50d.txt\", sep=\"\\s\", header=None)\n\n\n\nglove = glove.set_index(0)\n\n\nglove.index.name = 'term_str'\n\n\nglove.index.has_duplicates\n\nTrue\n\n\n\nglove.index.value_counts()\n\nthe           1\nfedden        1\nchlorinate    1\nkalapana      1\n40.64         1\n             ..\nolatunde      1\nalemao        1\nzanca         1\nnatsteel      1\nsandberger    1\nName: term_str, Length: 399997, dtype: int64\n\n\n\nglove.loc[['the','quick','brown','fox','jumped','over','lazy','dogs']]\n\n\n\n\n\n  \n    \n      \n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      ...\n      41\n      42\n      43\n      44\n      45\n      46\n      47\n      48\n      49\n      50\n    \n    \n      term_str\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      the\n      0.41800\n      0.249680\n      -0.41242\n      0.121700\n      0.34527\n      -0.044457\n      -0.49688\n      -0.178620\n      -0.000660\n      -0.65660\n      ...\n      -0.298710\n      -0.157490\n      -0.34758\n      -0.045637\n      -0.442510\n      0.18785\n      0.002785\n      -0.184110\n      -0.11514\n      -0.785810\n    \n    \n      quick\n      0.13967\n      -0.537980\n      -0.18047\n      -0.251420\n      0.16203\n      -0.138680\n      -0.24637\n      0.751110\n      0.272640\n      0.61035\n      ...\n      0.350060\n      0.524650\n      -0.51747\n      0.003471\n      0.736250\n      0.16252\n      0.852790\n      0.852680\n      0.57892\n      0.644830\n    \n    \n      brown\n      -0.88497\n      0.716850\n      -0.40379\n      -0.106980\n      0.81457\n      1.025800\n      -1.26980\n      -0.493820\n      -0.278390\n      -0.92251\n      ...\n      0.037507\n      0.764340\n      -0.64120\n      -0.595940\n      0.465890\n      0.31494\n      -0.340720\n      -0.591670\n      -0.31057\n      0.732740\n    \n    \n      fox\n      0.44206\n      0.059552\n      0.15861\n      0.927770\n      0.18760\n      0.242560\n      -1.59300\n      -0.798470\n      -0.340990\n      -0.24021\n      ...\n      -0.036646\n      0.782510\n      -1.00720\n      -0.590570\n      -0.784900\n      -0.39113\n      -0.497270\n      -0.428300\n      -0.15204\n      1.506400\n    \n    \n      jumped\n      -0.51460\n      -0.591540\n      1.60210\n      0.134290\n      0.88569\n      0.121370\n      -1.15810\n      -0.501000\n      0.053087\n      0.37916\n      ...\n      -0.578110\n      -0.186740\n      0.60172\n      0.004171\n      0.066152\n      -0.46572\n      -0.054225\n      -0.495990\n      0.49307\n      -0.572150\n    \n    \n      over\n      0.12972\n      0.088073\n      0.24375\n      0.078102\n      -0.12783\n      0.278310\n      -0.48693\n      0.196490\n      -0.395580\n      -0.28362\n      ...\n      -0.081068\n      0.391570\n      0.17300\n      0.225400\n      -0.128360\n      0.40951\n      -0.260790\n      0.090912\n      -0.60515\n      -0.982700\n    \n    \n      lazy\n      -0.27611\n      -0.597120\n      -0.49227\n      -1.037200\n      -0.35878\n      -0.097425\n      -0.21014\n      -0.092836\n      -0.054118\n      0.45420\n      ...\n      -0.440960\n      0.097736\n      -0.17589\n      1.179900\n      0.131520\n      -1.07950\n      0.456850\n      -0.633120\n      1.27520\n      1.167200\n    \n    \n      dogs\n      0.66293\n      -0.966960\n      -0.14334\n      -0.756590\n      0.24905\n      0.221720\n      -0.94465\n      -0.381720\n      0.876490\n      -0.65629\n      ...\n      -0.152350\n      -0.135670\n      0.45379\n      1.124500\n      1.659500\n      -0.43319\n      -0.170270\n      -0.685280\n      0.29151\n      0.046649\n    \n  \n\n8 rows × 50 columns\n\n\n\n\nglove.sample(10)\n\n\n\n\n\n  \n    \n      \n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      ...\n      41\n      42\n      43\n      44\n      45\n      46\n      47\n      48\n      49\n      50\n    \n    \n      term_str\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      wrests\n      -0.404540\n      -1.901700\n      1.05480\n      -0.054192\n      -1.059200\n      0.30774\n      0.84635\n      0.822740\n      0.328700\n      0.027596\n      ...\n      0.911030\n      0.202900\n      -0.42547\n      0.697050\n      -1.499700\n      -0.330990\n      1.167700\n      -0.045740\n      0.079461\n      -0.69059\n    \n    \n      footage\n      0.600360\n      -0.089296\n      0.70540\n      0.432460\n      0.075903\n      -0.16065\n      -0.63160\n      -0.061264\n      0.490740\n      0.264070\n      ...\n      0.815910\n      0.506310\n      -0.38727\n      -0.675990\n      0.096804\n      -0.750310\n      -0.156600\n      0.045359\n      -0.341170\n      -0.47274\n    \n    \n      premodern\n      -0.403840\n      -0.500380\n      -0.94482\n      -0.309030\n      -0.403770\n      -0.14626\n      0.80062\n      0.167330\n      -0.108880\n      0.369950\n      ...\n      -0.080176\n      -0.556040\n      0.90006\n      1.069900\n      -0.134950\n      0.197940\n      0.462100\n      0.201380\n      -0.358610\n      -0.21241\n    \n    \n      unimaginable\n      1.018000\n      -0.003556\n      -0.24592\n      -0.627990\n      0.272370\n      -0.19304\n      1.54620\n      0.531560\n      0.038944\n      0.973040\n      ...\n      -1.038800\n      0.036784\n      0.72901\n      0.252020\n      -0.652310\n      0.107100\n      -0.291880\n      0.296530\n      -0.378910\n      -0.13745\n    \n    \n      jetport\n      -1.002200\n      0.690770\n      -0.48382\n      0.911930\n      -1.291100\n      -0.65726\n      -0.22062\n      0.590900\n      0.948180\n      0.631590\n      ...\n      0.036332\n      -0.603570\n      0.19029\n      -0.328620\n      -0.792630\n      0.617450\n      0.162850\n      1.294500\n      0.139440\n      -0.59739\n    \n    \n      outpaces\n      0.059691\n      -0.260820\n      1.19630\n      -0.201200\n      -0.494630\n      -0.25999\n      -0.13380\n      0.307760\n      0.995940\n      0.769630\n      ...\n      -0.663670\n      -0.503100\n      -0.15110\n      -0.044545\n      -0.210350\n      -0.190750\n      0.078001\n      0.748210\n      -0.145590\n      0.37634\n    \n    \n      mehmedovic\n      -0.332350\n      -0.124570\n      0.25102\n      0.167780\n      0.318490\n      0.54145\n      -0.20790\n      0.556040\n      -0.237740\n      -0.093305\n      ...\n      -0.532860\n      -0.167390\n      -0.25954\n      -0.436120\n      0.499320\n      -0.672520\n      -0.593100\n      -0.565790\n      0.287010\n      0.85458\n    \n    \n      boty\n      -0.522280\n      0.300870\n      -0.45771\n      0.435740\n      -0.604350\n      -0.90836\n      0.66205\n      0.979960\n      0.889160\n      0.381550\n      ...\n      0.019760\n      -1.050700\n      0.89501\n      -0.876110\n      0.315780\n      -0.236210\n      0.084520\n      0.199110\n      0.168530\n      -0.20004\n    \n    \n      trances\n      0.916280\n      -1.074100\n      -0.72827\n      -0.425400\n      -0.007542\n      -0.27021\n      0.66726\n      1.148400\n      -0.026326\n      1.402700\n      ...\n      -0.156330\n      -0.418750\n      -0.34330\n      0.493080\n      -0.572970\n      -0.893380\n      1.106500\n      -0.819120\n      -0.366190\n      -1.07140\n    \n    \n      islahi\n      -0.538620\n      -1.036500\n      -0.38347\n      -0.789650\n      0.306730\n      -0.63187\n      0.41165\n      -0.277810\n      0.217770\n      0.581810\n      ...\n      0.232400\n      0.367920\n      -0.90042\n      0.321230\n      -0.041095\n      -0.001336\n      -0.089041\n      -0.577800\n      -0.038594\n      0.25848\n    \n  \n\n10 rows × 50 columns\n\n\n\n\n\nRemove non-words\nThere are a lot of useless tokens in the vocabulary. These may be good for generating the features, but we don’t need them in our queries.\n\n# glove = glove.reset_index()\n# glove = glove[glove.term_str.str.match(r'^[a-z]+$')]\n# glove = glove.set_index('term_str')\n# glove.shape\n\n\n\nDefine some semantic functions\n\ndef get_word_vector(term_str):\n    \"\"\"Get a numpy array from the glove matrix and shape for input into cosine function\"\"\"\n    wv = glove.loc[term_str].values.reshape(-1, 1).T\n    return wv\n\ndef get_sims(term_str, n=10):\n    \"\"\"Get the top n words for a given word based on cosine similarity\"\"\"\n    wv = get_word_vector(term_str)\n    sims = cosine_similarity(glove.values, wv)\n    return pd.DataFrame(sims, index=glove.index, columns=['score'])\\\n        .sort_values('score', ascending=False).head(n)\n\ndef get_nearest_vector(wv):\n    \"\"\"Get the nearest word vector to a given word vector\"\"\"\n    sims = cosine_similarity(glove.values, wv)\n    return pd.DataFrame(sims, index=glove.index, columns=['score'])\\\n        .sort_values('score', ascending=False).head(2).iloc[1]\n\ndef get_analogy(a, b, c):\n    \"\"\"Infer missing analogical term\"\"\"\n    try:\n        A = get_word_vector(a)\n        B = get_word_vector(b)\n        C = get_word_vector(c)\n        D = np.add(np.subtract(B, A), C)\n        X = get_nearest_vector(D)\n        return X.name\n    except ValueError as e:\n        print(e)\n        return None\n\n\n\nTest similarity function\n\nget_sims('queen')\n\n\n\n\n\n  \n    \n      \n      score\n    \n    \n      term_str\n      \n    \n  \n  \n    \n      queen\n      1.000000\n    \n    \n      princess\n      0.851517\n    \n    \n      lady\n      0.805061\n    \n    \n      elizabeth\n      0.787304\n    \n    \n      king\n      0.783904\n    \n    \n      prince\n      0.782186\n    \n    \n      coronation\n      0.769278\n    \n    \n      consort\n      0.762610\n    \n    \n      royal\n      0.744286\n    \n    \n      crown\n      0.738265\n    \n  \n\n\n\n\n\nget_sims('king')\n\n\n\n\n\n  \n    \n      \n      score\n    \n    \n      term_str\n      \n    \n  \n  \n    \n      king\n      1.000000\n    \n    \n      prince\n      0.823618\n    \n    \n      queen\n      0.783904\n    \n    \n      ii\n      0.774623\n    \n    \n      emperor\n      0.773625\n    \n    \n      son\n      0.766719\n    \n    \n      uncle\n      0.762715\n    \n    \n      kingdom\n      0.754216\n    \n    \n      throne\n      0.753991\n    \n    \n      brother\n      0.749241\n    \n  \n\n\n\n\n\nget_sims('milk')\n\n\n\n\n\n  \n    \n      \n      score\n    \n    \n      term_str\n      \n    \n  \n  \n    \n      milk\n      1.000000\n    \n    \n      sugar\n      0.821547\n    \n    \n      drink\n      0.815409\n    \n    \n      soda\n      0.808969\n    \n    \n      peanut\n      0.806213\n    \n    \n      juice\n      0.803572\n    \n    \n      dairy\n      0.792757\n    \n    \n      meat\n      0.791714\n    \n    \n      butter\n      0.789752\n    \n    \n      cream\n      0.784265\n    \n  \n\n\n\n\n\nget_sims('beer')\n\n\n\n\n\n  \n    \n      \n      score\n    \n    \n      term_str\n      \n    \n  \n  \n    \n      beer\n      1.000000\n    \n    \n      drink\n      0.849374\n    \n    \n      drinks\n      0.833444\n    \n    \n      coffee\n      0.804246\n    \n    \n      vodka\n      0.797950\n    \n    \n      beers\n      0.793020\n    \n    \n      champagne\n      0.776821\n    \n    \n      brewed\n      0.772739\n    \n    \n      soda\n      0.768293\n    \n    \n      wine\n      0.764609\n    \n  \n\n\n\n\n\n\nTest analogy functions\n\nget_analogy('dog','male','cat')\n\n'female'\n\n\n\nget_analogy('male','doctor','female')\n\n'nurse'\n\n\n\nget_analogy('king', 'queen', 'male')\n\n'female'\n\n\n\nget_analogy('queen','female','king')\n\n'male'\n\n\n\nget_analogy('female','princess','male')\n\n'duchess'\n\n\n\nget_analogy('right','male','left')\n\n'male'\n\n\n\nget_analogy('male', 'female', 'right')\n\n'put'\n\n\n\nget_analogy('right','left','male')\n\n'male'\n\n\n\nget_analogy('left','right','black')\n\n'white'\n\n\n\nget_analogy('left','right','white')\n\n'black'\n\n\n\nget_analogy('sun','moon','male')\n\n'male'\n\n\n\nget_analogy('day','sun','night')\n\n'sky'\n\n\n\n\nExperiment\nDemonstrate how to project embeddings onto documents\n\nLIB = pd.read_csv(\"../data/output/austen-melville-LIB_FIXED.csv\").set_index('book_id')\n\n\nCORPUS = pd.read_csv(\"../data/output/austen-melville-CORPUS.csv\")\\\n    .merge(glove, on=\"term_str\")\\\n    .set_index(['book_id', 'chap_id', 'para_num', 'sent_num', 'token_num'])\\\n    .iloc[:, 4:]\n\n\nCORPUS\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      \n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      ...\n      41\n      42\n      43\n      44\n      45\n      46\n      47\n      48\n      49\n      50\n    \n    \n      book_id\n      chap_id\n      para_num\n      sent_num\n      token_num\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      105\n      1\n      1\n      0\n      0\n      0.15408\n      0.759330\n      -0.471300\n      -0.684190\n      0.66657\n      0.020698\n      -0.82138\n      0.32687\n      -0.804420\n      -1.34970\n      ...\n      0.84756\n      1.014000\n      -0.72303\n      -0.65163\n      0.135110\n      0.507210\n      -0.820150\n      -1.29290\n      0.28219\n      0.480000\n    \n    \n      4\n      0\n      12\n      0.15408\n      0.759330\n      -0.471300\n      -0.684190\n      0.66657\n      0.020698\n      -0.82138\n      0.32687\n      -0.804420\n      -1.34970\n      ...\n      0.84756\n      1.014000\n      -0.72303\n      -0.65163\n      0.135110\n      0.507210\n      -0.820150\n      -1.29290\n      0.28219\n      0.480000\n    \n    \n      5\n      0\n      87\n      0.15408\n      0.759330\n      -0.471300\n      -0.684190\n      0.66657\n      0.020698\n      -0.82138\n      0.32687\n      -0.804420\n      -1.34970\n      ...\n      0.84756\n      1.014000\n      -0.72303\n      -0.65163\n      0.135110\n      0.507210\n      -0.820150\n      -1.29290\n      0.28219\n      0.480000\n    \n    \n      6\n      0\n      11\n      0.15408\n      0.759330\n      -0.471300\n      -0.684190\n      0.66657\n      0.020698\n      -0.82138\n      0.32687\n      -0.804420\n      -1.34970\n      ...\n      0.84756\n      1.014000\n      -0.72303\n      -0.65163\n      0.135110\n      0.507210\n      -0.820150\n      -1.29290\n      0.28219\n      0.480000\n    \n    \n      7\n      0\n      8\n      0.15408\n      0.759330\n      -0.471300\n      -0.684190\n      0.66657\n      0.020698\n      -0.82138\n      0.32687\n      -0.804420\n      -1.34970\n      ...\n      0.84756\n      1.014000\n      -0.72303\n      -0.65163\n      0.135110\n      0.507210\n      -0.820150\n      -1.29290\n      0.28219\n      0.480000\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      34970\n      114\n      13\n      0\n      10\n      -0.10870\n      -0.780970\n      -0.575230\n      0.006008\n      -0.27756\n      -0.305890\n      0.47459\n      0.17865\n      -0.006944\n      1.51110\n      ...\n      -0.21233\n      0.273010\n      -0.41302\n      0.60088\n      0.146930\n      -0.140220\n      -0.091782\n      0.14428\n      0.28728\n      0.618360\n    \n    \n      2\n      8\n      0.53049\n      -0.636570\n      -0.533140\n      -0.375420\n      0.28821\n      1.237400\n      -0.47467\n      -1.20370\n      0.582090\n      -0.55149\n      ...\n      0.32885\n      0.572850\n      -0.57111\n      0.10893\n      1.090200\n      -0.028394\n      0.784580\n      -0.97332\n      0.36124\n      -0.056677\n    \n    \n      17\n      0\n      28\n      0.37567\n      -1.520800\n      -0.039892\n      -0.432260\n      -0.20000\n      -0.151140\n      0.35699\n      0.75466\n      -0.088619\n      0.15833\n      ...\n      -0.34531\n      -0.032317\n      -0.01209\n      1.44600\n      0.037374\n      -0.030873\n      -0.230760\n      -0.15306\n      0.76482\n      0.010832\n    \n    \n      18\n      3\n      11\n      0.86343\n      0.004638\n      -0.851650\n      -0.316740\n      -0.35847\n      -0.539820\n      0.73907\n      0.66941\n      0.052928\n      0.24664\n      ...\n      -1.24120\n      -0.210910\n      0.15890\n      0.47247\n      -0.268530\n      -0.297720\n      0.777120\n      -0.72784\n      0.24178\n      -0.306210\n    \n    \n      16\n      0.86343\n      0.004638\n      -0.851650\n      -0.316740\n      -0.35847\n      -0.539820\n      0.73907\n      0.66941\n      0.052928\n      0.24664\n      ...\n      -1.24120\n      -0.210910\n      0.15890\n      0.47247\n      -0.268530\n      -0.297720\n      0.777120\n      -0.72784\n      0.24178\n      -0.306210\n    \n  \n\n2038058 rows × 50 columns\n\n\n\n\nBOOKS = CORPUS.groupby('book_id').mean()\n\n\nBOOKS\n\n\n\n\n\n  \n    \n      \n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      ...\n      41\n      42\n      43\n      44\n      45\n      46\n      47\n      48\n      49\n      50\n    \n    \n      book_id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      105\n      0.323545\n      0.130375\n      -0.138879\n      -0.245284\n      0.429670\n      0.207292\n      -0.423724\n      0.076339\n      -0.232307\n      -0.060521\n      ...\n      -0.157819\n      0.088310\n      -0.029153\n      0.150974\n      -0.054749\n      0.006546\n      -0.230543\n      -0.195147\n      -0.052740\n      -0.030568\n    \n    \n      121\n      0.324524\n      0.146049\n      -0.132952\n      -0.247241\n      0.442528\n      0.181426\n      -0.386801\n      0.069392\n      -0.215634\n      -0.025070\n      ...\n      -0.150170\n      0.074479\n      -0.036294\n      0.174485\n      -0.046944\n      0.002776\n      -0.192248\n      -0.197593\n      -0.055893\n      -0.032289\n    \n    \n      141\n      0.316918\n      0.137231\n      -0.128935\n      -0.254841\n      0.430273\n      0.203088\n      -0.401161\n      0.075366\n      -0.222743\n      -0.023045\n      ...\n      -0.153490\n      0.084964\n      -0.035502\n      0.159466\n      -0.050061\n      -0.000043\n      -0.213604\n      -0.182040\n      -0.052641\n      -0.009841\n    \n    \n      158\n      0.316814\n      0.132361\n      -0.130478\n      -0.248837\n      0.447975\n      0.186106\n      -0.394266\n      0.076542\n      -0.231446\n      -0.010712\n      ...\n      -0.154607\n      0.080837\n      -0.038184\n      0.178259\n      -0.035039\n      0.007859\n      -0.213723\n      -0.164035\n      -0.042073\n      0.039832\n    \n    \n      161\n      0.312367\n      0.141866\n      -0.126897\n      -0.262345\n      0.442250\n      0.207670\n      -0.389031\n      0.088340\n      -0.214907\n      -0.020283\n      ...\n      -0.149661\n      0.076487\n      -0.036526\n      0.164505\n      -0.045916\n      -0.011587\n      -0.212383\n      -0.178812\n      -0.045700\n      -0.014388\n    \n    \n      946\n      0.318215\n      0.165525\n      -0.126256\n      -0.265697\n      0.451028\n      0.175482\n      -0.333226\n      0.116844\n      -0.228130\n      0.007189\n      ...\n      -0.132155\n      0.062069\n      -0.036837\n      0.177433\n      -0.036822\n      0.001203\n      -0.206038\n      -0.179400\n      -0.037422\n      0.063258\n    \n    \n      1212\n      0.311875\n      0.162135\n      -0.131199\n      -0.258385\n      0.446084\n      0.172494\n      -0.361823\n      0.074600\n      -0.227629\n      -0.031131\n      ...\n      -0.136514\n      0.059259\n      -0.042337\n      0.166035\n      -0.047415\n      0.021623\n      -0.208847\n      -0.205562\n      -0.044726\n      -0.004992\n    \n    \n      1342\n      0.317907\n      0.138468\n      -0.135627\n      -0.252612\n      0.449312\n      0.199988\n      -0.390621\n      0.096263\n      -0.232130\n      -0.034875\n      ...\n      -0.145135\n      0.083121\n      -0.051874\n      0.154525\n      -0.044942\n      -0.004497\n      -0.220922\n      -0.185624\n      -0.043178\n      0.000160\n    \n    \n      1900\n      0.335349\n      0.109812\n      -0.138289\n      -0.201990\n      0.364711\n      0.169110\n      -0.349099\n      -0.012520\n      -0.167956\n      -0.082440\n      ...\n      -0.181710\n      0.028737\n      -0.022690\n      0.163153\n      -0.061622\n      0.018126\n      -0.138243\n      -0.169704\n      -0.082815\n      -0.235801\n    \n    \n      2701\n      0.309045\n      0.097215\n      -0.111785\n      -0.201452\n      0.362351\n      0.162099\n      -0.346368\n      -0.011339\n      -0.125390\n      -0.074190\n      ...\n      -0.166570\n      0.050467\n      -0.011518\n      0.168697\n      -0.039741\n      0.030372\n      -0.112005\n      -0.191944\n      -0.062654\n      -0.211388\n    \n    \n      4045\n      0.306716\n      0.095793\n      -0.139418\n      -0.195468\n      0.362659\n      0.154994\n      -0.410006\n      -0.002854\n      -0.172801\n      -0.111827\n      ...\n      -0.169492\n      0.061049\n      -0.006736\n      0.147962\n      -0.043980\n      0.015729\n      -0.162320\n      -0.195744\n      -0.057727\n      -0.221459\n    \n    \n      8118\n      0.293717\n      0.115120\n      -0.119652\n      -0.225133\n      0.380321\n      0.139495\n      -0.422799\n      0.009417\n      -0.188256\n      -0.086536\n      ...\n      -0.152497\n      0.072660\n      -0.017735\n      0.164949\n      -0.030472\n      0.032228\n      -0.159284\n      -0.221641\n      -0.057873\n      -0.175120\n    \n    \n      10712\n      0.283606\n      0.098257\n      -0.123281\n      -0.197707\n      0.366372\n      0.151738\n      -0.345851\n      -0.005370\n      -0.146538\n      -0.129115\n      ...\n      -0.147836\n      0.054369\n      0.002465\n      0.162476\n      -0.059051\n      0.017283\n      -0.152163\n      -0.179221\n      -0.059132\n      -0.204963\n    \n    \n      13720\n      0.319063\n      0.106985\n      -0.134747\n      -0.190804\n      0.349920\n      0.149525\n      -0.343138\n      -0.002252\n      -0.144310\n      -0.075306\n      ...\n      -0.171743\n      0.037245\n      -0.015910\n      0.160622\n      -0.054426\n      0.027244\n      -0.128231\n      -0.205901\n      -0.071515\n      -0.250862\n    \n    \n      13721\n      0.293587\n      0.127525\n      -0.124645\n      -0.206225\n      0.373548\n      0.111245\n      -0.304175\n      -0.015548\n      -0.161114\n      -0.018633\n      ...\n      -0.166072\n      0.051955\n      -0.036225\n      0.189710\n      -0.018017\n      0.048836\n      -0.138517\n      -0.208207\n      -0.080305\n      -0.200010\n    \n    \n      15422\n      0.283807\n      0.111660\n      -0.110573\n      -0.223605\n      0.366810\n      0.134835\n      -0.382448\n      0.030381\n      -0.170981\n      -0.105452\n      ...\n      -0.142183\n      0.063590\n      -0.038168\n      0.125249\n      -0.034063\n      0.009587\n      -0.140280\n      -0.196693\n      -0.056008\n      -0.196437\n    \n    \n      15859\n      0.311699\n      0.085635\n      -0.127175\n      -0.198729\n      0.363757\n      0.141646\n      -0.355428\n      0.021661\n      -0.181645\n      -0.073979\n      ...\n      -0.165373\n      0.063418\n      -0.025045\n      0.167156\n      -0.067231\n      0.013878\n      -0.139937\n      -0.175500\n      -0.053438\n      -0.196067\n    \n    \n      21816\n      0.283421\n      0.117710\n      -0.123246\n      -0.226586\n      0.429733\n      0.150555\n      -0.332350\n      -0.005304\n      -0.189355\n      0.021749\n      ...\n      -0.144161\n      0.071773\n      -0.045167\n      0.226195\n      -0.027005\n      0.032009\n      -0.153737\n      -0.114105\n      -0.038590\n      -0.054833\n    \n    \n      34970\n      0.296548\n      0.143517\n      -0.146223\n      -0.240363\n      0.411866\n      0.171806\n      -0.325516\n      0.015607\n      -0.192423\n      0.006447\n      ...\n      -0.145577\n      0.066343\n      -0.036323\n      0.161659\n      -0.056362\n      0.045523\n      -0.162378\n      -0.208462\n      -0.056357\n      -0.154906\n    \n  \n\n19 rows × 50 columns\n\n\n\n\nBOOKS.mean(1).sort_values().plot.barh()\n\n<AxesSubplot: ylabel='book_id'>\n\n\n\n\n\n\nimport sys\nsys.path.append(\"../lib\")\n\n\nfrom hac2 import HAC\n\n\nHAC(BOOKS, labels=BOOKS.join(LIB, how='left').label.to_list()).plot()\n\n<Figure size 640x480 with 0 Axes>"
  },
  {
    "objectID": "lessons/M09_WordEmbedding/M09_02_PMI_SVD.html",
    "href": "lessons/M09_WordEmbedding/M09_02_PMI_SVD.html",
    "title": "Metadata",
    "section": "",
    "text": "Course:    DS 5001 \nModule:    09 Lab\nTopic:     Using SVD\nAuthor:    R.C. Alvarado\nDate:      28 March 2023 (revised)\nPurpose: We create word vectors by applying a singular value decomposition to a pointwise mutual information word-word matrix.\n\nConfiguration\n\ndata_in = \"../data/novels\"\ndata_prefix = 'novels'\n\n\nOHCO = ['genre', 'author', 'book', 'chapter', 'para_num', 'sent_num', 'token_num']\nBAG = OHCO[2:5] # Paragraphs\n\n# Word Embedding\nwindow = 3\n\n\n\nLibraries\n\nimport pandas as pd\nimport numpy as np\nimport scipy as sp\n\n\n\nProcess\n\n# pd.read_csv(f'{data_in}/{data_prefix}-CORPUS.csv')\n\n\nTOKENS = pd.read_csv(f'{data_in}/{data_prefix}-TOKENS.csv')\n\n\nTOKENS\n\n\n\n\n\n  \n    \n      \n      book\n      chapter\n      para_num\n      sent_num\n      token_num\n      pos\n      term_str\n      term_id\n    \n  \n  \n    \n      0\n      secretadversary\n      1\n      0\n      1\n      0\n      DT\n      the\n      24127\n    \n    \n      1\n      secretadversary\n      1\n      0\n      1\n      1\n      NNP\n      young\n      27354\n    \n    \n      2\n      secretadversary\n      1\n      0\n      1\n      2\n      NNP\n      adventurers\n      399\n    \n    \n      3\n      secretadversary\n      1\n      0\n      1\n      3\n      NNP\n      ltd\n      14406\n    \n    \n      4\n      secretadversary\n      1\n      1\n      0\n      0\n      JJ\n      tommy\n      24529\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1500412\n      baskervilles\n      11\n      114\n      1\n      7\n      RBR\n      more\n      15586\n    \n    \n      1500413\n      baskervilles\n      11\n      114\n      1\n      8\n      JJ\n      comfortable\n      4529\n    \n    \n      1500414\n      baskervilles\n      11\n      114\n      1\n      9\n      IN\n      outside\n      16771\n    \n    \n      1500415\n      baskervilles\n      11\n      114\n      1\n      10\n      IN\n      than\n      24112\n    \n    \n      1500416\n      baskervilles\n      11\n      114\n      1\n      11\n      NN\n      in\n      12267\n    \n  \n\n1500417 rows × 8 columns\n\n\n\n\nPARAS = TOKENS.groupby(BAG).apply(lambda x: x.term_str.tolist()).reset_index(drop=True)\n\n\nPARAS.head()\n\n0                            [a, scandal, in, bohemia]\n1                                                  [i]\n2    [to, sherlock, holmes, she, is, always, the, w...\n3    [i, had, seen, little, of, holmes, lately, my,...\n4    [one, night, it, was, on, the, twentieth, of, ...\ndtype: object\n\n\n\ndef get_context_words(x):\n    data = []\n    id  = x.name\n    row = x[0]\n    for i in range(len(row)):\n        data2 = []\n        for j in range(-2,3):\n            a = i + j\n            if a >= 0 and a < len(row):\n                data2.append((j, row[a])) \n        data.append(data2)\n    return data\n\n\nTEST = PARAS.to_frame(0).apply(get_context_words, 1)\n\n\nTEST\n\n0        [[(0, a), (1, scandal), (2, in)], [(-1, a), (0...\n1                                               [[(0, i)]]\n2        [[(0, to), (1, sherlock), (2, holmes)], [(-1, ...\n3        [[(0, i), (1, had), (2, seen)], [(-1, i), (0, ...\n4        [[(0, one), (1, night), (2, it)], [(-1, one), ...\n                               ...                        \n27336    [[(0, and), (1, now), (2, the)], [(-1, and), (...\n27337    [[(0, no), (1, sooner), (2, had)], [(-1, no), ...\n27338    [[(0, not), (1, hear), (2, it)], [(-1, not), (...\n27339    [[(0, as), (1, if), (2, in)], [(-1, as), (0, i...\n27340    [[(0, from), (1, that), (2, chamber)], [(-1, f...\nLength: 27341, dtype: object\n\n\n\nTEST2 = pd.DataFrame([(i, j, item[0], item[1]) \n     for i, row in enumerate(TEST)\n        for j, row2 in enumerate(row)\n            for item in row2])\n\n\nTEST2\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      a\n    \n    \n      1\n      0\n      0\n      1\n      scandal\n    \n    \n      2\n      0\n      0\n      2\n      in\n    \n    \n      3\n      0\n      1\n      -1\n      a\n    \n    \n      4\n      0\n      1\n      0\n      scandal\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      7339002\n      27340\n      174\n      0\n      of\n    \n    \n      7339003\n      27340\n      174\n      1\n      usher\n    \n    \n      7339004\n      27340\n      175\n      -2\n      house\n    \n    \n      7339005\n      27340\n      175\n      -1\n      of\n    \n    \n      7339006\n      27340\n      175\n      0\n      usher\n    \n  \n\n7339007 rows × 4 columns\n\n\n\n\nTEST2.columns = ['bag_id', 'window_id', 'offset', 'term_str']\n\n\nA = TEST2[TEST2.offset == 0].reset_index(drop=True)\nB = TEST2[TEST2.offset != 0].reset_index(drop=True)\nskipgrams = A.merge(B, on=['bag_id','window_id'], how='left')\\\n    .rename(columns={'term_str_x':'target','term_str_y':'probe','offset_y':'dist'})\n\n\nskipgrams = skipgrams[['target','probe','dist']].dropna().sort_values('target').reset_index(drop=True)\nskipgrams['dist'] = skipgrams['dist'].astype('int')\n\n\nskipgrams.head()\n\n\n\n\n\n  \n    \n      \n      target\n      probe\n      dist\n    \n  \n  \n    \n      0\n      a\n      scandal\n      1\n    \n    \n      1\n      a\n      curious\n      1\n    \n    \n      2\n      a\n      extraordinary\n      -1\n    \n    \n      3\n      a\n      was\n      -2\n    \n    \n      4\n      a\n      pace\n      2\n    \n  \n\n\n\n\n\n# skipgrams.set_index(['target','probe'])#.unstack()\n\n\n# TEST2 = TEST2.set_index(['bag_id','window_id', 'offset'])\n# TEST2.unstack().fillna('')\n\n\n\nAdd Skigram weights (as GloVe does)\n\n# skipgrams['glove_weight'] = np.round(np.abs(1 / skipgrams['dist']), 2)\n\n\n# skipgrams.head(10)\n\n\n\nGet Unigram Probabilities\nWe have already computed these in the vocab table.\n\n\nImport vocab table\n\nVOCAB = pd.read_csv(f'{data_in}/{data_prefix}-VOCAB.csv')\n# VOCAB = VOCAB[VOCAB.stop == 0]\n\n\n# vocab = tx.get_table('vocab', db_file, index_col=['term_id'])\n# vocab = vocab[vocab.stop == 0]\n\n\nVOCAB.sort_values('p', ascending=False).head()\n\n\n\n\n\n  \n    \n      \n      term_id\n      term_str\n      n\n      p\n      port_stem\n      stop\n      df\n      idf\n      tfidf_sum\n      tfidf_mean\n      tfidf_max\n      pos_max\n    \n  \n  \n    \n      24127\n      24127\n      the\n      85329\n      0.056870\n      the\n      1\n      320\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      DT\n    \n    \n      24470\n      24470\n      to\n      45176\n      0.030109\n      to\n      1\n      320\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      TO\n    \n    \n      862\n      862\n      and\n      44991\n      0.029986\n      and\n      1\n      320\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      CC\n    \n    \n      16459\n      16459\n      of\n      42638\n      0.028417\n      of\n      1\n      320\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      IN\n    \n    \n      11947\n      11947\n      i\n      32985\n      0.021984\n      i\n      1\n      316\n      0.005463\n      180.193615\n      0.563105\n      3.403384\n      PRP\n    \n  \n\n\n\n\n\n\nGet \\(P(x)\\)\n\np_x = VOCAB[['term_str','p']].reset_index().set_index('term_str')['p']\n\n\np_x.sort_values(ascending=False).head()\n\nterm_str\nthe    0.056870\nto     0.030109\nand    0.029986\nof     0.028417\ni      0.021984\nName: p, dtype: float64\n\n\n\n# skipgrams.groupby('target').target.count() / skipgrams.target.sum()\n\n\n\nCompute Normalized PMI for Skipgrams\nPMI\n\\(log \\dfrac{P(x,y)}{P(x)P(y)}\\)\nNMPI\n\\(\\dfrac{log\\dfrac{P(x,y)}{P(x)P(y)}}{-log P(x,y)}\\)\nSee G. Bouma 2009, eq. 7\n\n\nCreate compressed skipgram table\n\nskipgrams2 = skipgrams.groupby(['target','probe']).probe.count()\\\n    .to_frame().rename(columns={'probe':'n'})\\\n    .reset_index().set_index(['target','probe'])\n\n\nskipgrams2.head(10)\n\n\n\n\n\n  \n    \n      \n      \n      n\n    \n    \n      target\n      probe\n      \n    \n  \n  \n    \n      a\n      a\n      246\n    \n    \n      aback\n      1\n    \n    \n      abandon\n      2\n    \n    \n      abandons\n      1\n    \n    \n      abated\n      1\n    \n    \n      abatement\n      1\n    \n    \n      abbess\n      4\n    \n    \n      abbey\n      2\n    \n    \n      abbot\n      3\n    \n    \n      abc\n      2\n    \n  \n\n\n\n\n\n\nCompute \\(P(x,y)\\)\n\nN = skipgrams2.n.sum() # Might smooth by adding value to pairs with 0-value\n\n\nskipgrams2['p_xy'] = skipgrams2.n / N\n\n\nskipgrams2.head(10)\n\n\n\n\n\n  \n    \n      \n      \n      n\n      p_xy\n    \n    \n      target\n      probe\n      \n      \n    \n  \n  \n    \n      a\n      a\n      246\n      4.213358e-05\n    \n    \n      aback\n      1\n      1.712747e-07\n    \n    \n      abandon\n      2\n      3.425494e-07\n    \n    \n      abandons\n      1\n      1.712747e-07\n    \n    \n      abated\n      1\n      1.712747e-07\n    \n    \n      abatement\n      1\n      1.712747e-07\n    \n    \n      abbess\n      4\n      6.850988e-07\n    \n    \n      abbey\n      2\n      3.425494e-07\n    \n    \n      abbot\n      3\n      5.138241e-07\n    \n    \n      abc\n      2\n      3.425494e-07\n    \n  \n\n\n\n\n\n\nCompute \\(PMI(x;y)\\)\n\nskipgrams2['pmi_xy'] = skipgrams2.apply(lambda row: np.log(row.p_xy / (p_x.loc[row.name[0]] * p_x.loc[row.name[1]])), 1)\n\n\nskipgrams2.sort_values('pmi_xy', ascending=False).head(10)\n\n\n\n\n\n  \n    \n      \n      \n      n\n      p_xy\n      pmi_xy\n    \n    \n      target\n      probe\n      \n      \n      \n    \n  \n  \n    \n      twarnt\n      crows\n      2\n      3.425494e-07\n      12.86251\n    \n    \n      cest\n      loeuvre\n      2\n      3.425494e-07\n      12.86251\n    \n    \n      rien\n      cest\n      2\n      3.425494e-07\n      12.86251\n    \n    \n      charly\n      magne\n      2\n      3.425494e-07\n      12.86251\n    \n    \n      patria\n      nunc\n      2\n      3.425494e-07\n      12.86251\n    \n    \n      crows\n      twarnt\n      2\n      3.425494e-07\n      12.86251\n    \n    \n      wholeman\n      marquand\n      2\n      3.425494e-07\n      12.86251\n    \n    \n      loeuvre\n      cest\n      2\n      3.425494e-07\n      12.86251\n    \n    \n      marquand\n      wholeman\n      2\n      3.425494e-07\n      12.86251\n    \n    \n      nunc\n      fracto\n      2\n      3.425494e-07\n      12.86251\n    \n  \n\n\n\n\n\nskipgrams2['npmi_xy'] = skipgrams2.pmi_xy / -( np.log(skipgrams2.p_xy) )\n\n\nskipgrams2.sort_values('npmi_xy', ascending=False).head()\n\n\n\n\n\n  \n    \n      \n      \n      n\n      p_xy\n      pmi_xy\n      npmi_xy\n    \n    \n      target\n      probe\n      \n      \n      \n      \n    \n  \n  \n    \n      smack\n      smack\n      6\n      1.027648e-06\n      12.457045\n      0.903454\n    \n    \n      ry\n      ve\n      3\n      5.138241e-07\n      12.574828\n      0.868344\n    \n    \n      ve\n      ry\n      3\n      5.138241e-07\n      12.574828\n      0.868344\n    \n    \n      cest\n      rien\n      2\n      3.425494e-07\n      12.862510\n      0.864018\n    \n    \n      marquand\n      wholeman\n      2\n      3.425494e-07\n      12.862510\n      0.864018\n    \n  \n\n\n\n\n\n\nKeep only positives\nChanged since lab.\n\nskipgrams2.loc[skipgrams2.npmi_xy < 0, 'pnpmi_xy'] = 0\nskipgrams2.loc[skipgrams2.npmi_xy >= 0, 'pnpmi_xy'] =  skipgrams2.npmi_xy\n\n\nskipgrams2.head()\n\n\n\n\n\n  \n    \n      \n      \n      n\n      p_xy\n      pmi_xy\n      npmi_xy\n      pnpmi_xy\n    \n    \n      target\n      probe\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      a\n      a\n      246\n      4.213358e-05\n      -2.149792\n      -0.213386\n      0.000000\n    \n    \n      aback\n      1\n      1.712747e-07\n      0.406469\n      0.026089\n      0.026089\n    \n    \n      abandon\n      2\n      3.425494e-07\n      -0.487349\n      -0.032737\n      0.000000\n    \n    \n      abandons\n      1\n      1.712747e-07\n      1.910546\n      0.122628\n      0.122628\n    \n    \n      abated\n      1\n      1.712747e-07\n      0.118787\n      0.007624\n      0.007624\n    \n  \n\n\n\n\n\n\nCreate PNPMI Matrix\n\nSGM = skipgrams2.npmi_xy.unstack().fillna(0)\n\n\nSGM.head()\n\n\n\n\n\n  \n    \n      probe\n      a\n      aback\n      abaft\n      abandon\n      abandoned\n      abandoning\n      abandons\n      abasement\n      abashed\n      abate\n      ...\n      zoöphagy\n      zufalle\n      zum\n      zuniga\n      zusammen\n      à\n      æt\n      ætat\n      ça\n      émeutes\n    \n    \n      target\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      a\n      -0.213386\n      0.026089\n      0.0\n      -0.032737\n      0.0\n      0.0\n      0.122628\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      aback\n      0.026089\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      abaft\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      abandon\n      -0.032737\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      abandoned\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n5 rows × 27378 columns\n\n\n\n\nSGM.loc['man'].sort_values(ascending=False).head()\n\nprobe\nlegged         0.340504\nwounding       0.326470\npersonating    0.326470\nincites        0.326470\nunpractical    0.326470\nName: man, dtype: float64\n\n\n\nskipgrams2.loc['prussian'].sort_values('n', ascending=False)\n\n\n\n\n\n  \n    \n      \n      n\n      p_xy\n      pmi_xy\n      npmi_xy\n      pnpmi_xy\n    \n    \n      probe\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      franco\n      1\n      1.712747e-07\n      12.862510\n      0.825578\n      0.825578\n    \n    \n      it\n      1\n      1.712747e-07\n      3.039961\n      0.195120\n      0.195120\n    \n    \n      the\n      1\n      1.712747e-07\n      1.508240\n      0.096806\n      0.096806\n    \n    \n      war\n      1\n      1.712747e-07\n      8.599830\n      0.551979\n      0.551979\n    \n  \n\n\n\n\n\n\nSVD\n\nfrom scipy import sparse\nimport scipy.sparse.linalg as linalg\n\n\nsparse = sparse.csr_matrix(SGM.values)\n\n\nSVD = linalg.svds(sparse, k=256)\n\n\nU, S, V = SVD\n\n\nU.shape, S.shape, V.shape\n\n((27378, 256), (256,), (256, 27378))\n\n\n\nword_vecs = U + V.T\nword_vecs_norm = word_vecs / np.sqrt(np.sum(word_vecs * word_vecs, axis=1, keepdims=True))\n\n\nWE = pd.DataFrame(word_vecs_norm, index=SGM.index)\nWE.index.name = 'word_str'\n\n\nWE.head()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      ...\n      246\n      247\n      248\n      249\n      250\n      251\n      252\n      253\n      254\n      255\n    \n    \n      word_str\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      a\n      0.013849\n      0.027852\n      9.117356e-15\n      0.008469\n      1.951358e-15\n      -9.059876e-16\n      -4.819776e-17\n      0.023903\n      0.001839\n      -3.755517e-15\n      ...\n      -0.128068\n      0.164470\n      -0.305037\n      -0.297994\n      -6.607001e-15\n      -0.057980\n      -0.102613\n      -0.276027\n      -8.232699e-16\n      0.073095\n    \n    \n      aback\n      0.000202\n      -0.025893\n      -1.432905e-14\n      0.058885\n      8.453735e-15\n      6.200973e-15\n      -8.478915e-15\n      0.121579\n      0.136744\n      1.922402e-14\n      ...\n      0.033902\n      -0.088168\n      -0.139799\n      -0.058193\n      -3.408507e-15\n      -0.036545\n      -0.049069\n      0.025502\n      -1.376502e-16\n      0.055713\n    \n    \n      abaft\n      0.042357\n      -0.043332\n      1.285451e-17\n      -0.060560\n      -1.418736e-14\n      -1.240460e-15\n      -1.728931e-15\n      0.021453\n      0.067742\n      1.922713e-14\n      ...\n      -0.032025\n      -0.024272\n      0.087306\n      -0.050848\n      -4.884714e-16\n      -0.004592\n      0.002073\n      -0.079610\n      -8.034068e-17\n      0.022465\n    \n    \n      abandon\n      0.007568\n      -0.014853\n      2.175405e-14\n      -0.123054\n      -2.934644e-14\n      -3.685461e-15\n      4.454370e-15\n      0.004444\n      -0.007751\n      -2.646420e-14\n      ...\n      0.076529\n      0.142699\n      0.133869\n      0.063106\n      1.322586e-15\n      0.011911\n      -0.121623\n      0.153128\n      -4.990891e-17\n      0.123737\n    \n    \n      abandoned\n      -0.013846\n      0.122148\n      5.677193e-14\n      -0.117773\n      -2.345948e-15\n      -1.354161e-14\n      -7.636808e-15\n      0.032982\n      -0.035400\n      -1.330202e-14\n      ...\n      0.021848\n      0.005364\n      0.008472\n      0.062229\n      -1.477448e-15\n      -0.019922\n      -0.086218\n      0.093767\n      1.197931e-16\n      0.132631\n    \n  \n\n5 rows × 256 columns\n\n\n\n\ndef word_sims(word, n=10):\n    try:\n        sims = SGM.loc[word].sort_values(ascending=False).head(n).reset_index().values\n        return sims\n    except KeyError as e:\n        print('Word \"{}\" not in vocabulary.'.format(word))\n        return None\n\n\nprint(word_sims('happy'))\n\n[['transit' 0.45355928441921106]\n ['anniversary' 0.45355928441921106]\n ['prosperous' 0.413127131611673]\n ['supremely' 0.4090697268137657]\n ['swain' 0.38155540016356587]\n ['prospero' 0.338555446335461]\n ['compleatly' 0.338555446335461]\n ['thankfulness' 0.32866130589539844]\n ['bygone' 0.32866130589539844]\n ['dauntless' 0.3200906116028747]]\n\n\n\ndef word_sim_report(word):\n    sims = word_sims(word)\n    for sim_word, score in sims:\n        context = ' '.join(skipgrams2.loc[sim_word].index.values.tolist()[:5])\n        print(\"{} ({}) {}\".format(sim_word.upper(), score, context))\n        print('-'*80)\n\n\nword_sim_report('woman')\n\nUNMENTIONABLE (0.4188501650732945) presence some stood woman\n--------------------------------------------------------------------------------\nSHOD (0.4188501650732945) a elderly slip woman\n--------------------------------------------------------------------------------\nJACKONET (0.4188501650732945) is or the woman\n--------------------------------------------------------------------------------\nGRABS (0.4188501650732945) at her married woman\n--------------------------------------------------------------------------------\nPROSING (0.4188501650732945) of old this woman\n--------------------------------------------------------------------------------\nREFORMED (0.4188501650732945) a be but woman\n--------------------------------------------------------------------------------\nSILHOUETTED (0.4188501650732945) against the was woman\n--------------------------------------------------------------------------------\nLAUNDRY (0.4111158023726) and as at brought folded\n--------------------------------------------------------------------------------\nCOMPLEXIONED (0.37436060746784916) dark fair i man shabby\n--------------------------------------------------------------------------------\nBLONDE (0.37436060746784916) a lady little stood was\n--------------------------------------------------------------------------------\n\n\n\nword_sim_report('man')\n\nLEGGED (0.3405039403413688) a alone an and are\n--------------------------------------------------------------------------------\nWOUNDING (0.32646954828747865) aim at man the\n--------------------------------------------------------------------------------\nPERSONATING (0.32646954828747865) a man mechanic the\n--------------------------------------------------------------------------------\nINCITES (0.32646954828747865) man other rogue the\n--------------------------------------------------------------------------------\nUNPRACTICAL (0.32646954828747865) an and man myself\n--------------------------------------------------------------------------------\nDANDIFIED (0.32646954828747865) little man quaint this\n--------------------------------------------------------------------------------\nCOARSELY (0.32646954828747865) as clad man sized\n--------------------------------------------------------------------------------\nCLOYS (0.32646954828747865) man only possession which\n--------------------------------------------------------------------------------\nRAKED (0.32646954828747865) fire man old the\n--------------------------------------------------------------------------------\nCELIBACY (0.32646954828747865) man of vows was\n--------------------------------------------------------------------------------\n\n\n\nword_sim_report('young')\n\nADVENTURERS (0.4218204316609113) a after all and as\n--------------------------------------------------------------------------------\nLTD (0.416433825110785) adventurers downwards is responded that\n--------------------------------------------------------------------------------\nCRATCHITS (0.40303898373373426) about again and as at\n--------------------------------------------------------------------------------\nBRIGHAM (0.39704423413499956) great has himself our religion\n--------------------------------------------------------------------------------\nBERESFORDS (0.39704423413499956) ah being in letter to\n--------------------------------------------------------------------------------\nWOMANHOOD (0.3793799118085404) had her i young\n--------------------------------------------------------------------------------\nSMIRKING (0.3793799118085404) at drebber his young\n--------------------------------------------------------------------------------\nOPENSHAWS (0.3793799118085404) of young\n--------------------------------------------------------------------------------\nPARD (0.3793799118085404) it the there young\n--------------------------------------------------------------------------------\nMELCHIOR (0.3793799118085404) accepted bascos offer young\n--------------------------------------------------------------------------------\n\n\n\n\nDefine some semantic functions\nAdded after lecture.\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics.pairwise import euclidean_distances\n\n\ndef get_word_vector(term_str):\n    \"\"\"Get a numpy array from the glove matrix and shape for input into cosine function\"\"\"\n    return SGM.loc[term_str].values.reshape(-1, 1).T\n\ndef get_nearest_vector(wv, method='cosine', n=1):\n    \"\"\"Get the nearest word vectors to a given word vector\"\"\"\n    if method == 'cosine':\n        sims = cosine_similarity(SGM.values, wv)\n    elif method == 'euclidean':\n        eds = euclidean_distances(SGM.values, wv)\n        sims = 1 - (eds/eds.max())\n    else:\n        print('Invalid method {}; defaulting to cosine.'.format(method))\n        sims = cosine_similarity(SGM.values, wv)\n    return pd.DataFrame(sims, index=SGM.index, columns=['score']).sort_values('score',ascending=False).head(n+1).iloc[1:]\n\ndef get_sims(term_str, method='cosine', n=10):\n    \"\"\"Get the top n words for a given word based on cosine similarity\"\"\"\n    wv = get_word_vector(term_str)\n    sims =  get_nearest_vector(wv, method=method, n=n) \n    return sims\n\ndef get_analogy(a, b, c, method='cosine'):\n    \"\"\"Infer missing analogical term\"\"\"\n    print()\n    try:\n        A = get_word_vector(a)\n        B = get_word_vector(b)\n        C = get_word_vector(c)\n        D = np.add(np.subtract(B, A), C)\n        X = get_nearest_vector(C, method=method, n=1)\n        return X.iloc[0].name\n    except ValueError as e:\n        print(e)\n        return None\n\n\nget_nearest_vector(get_word_vector('woman'),  n=10)\n\n\n\n\n\n  \n    \n      \n      score\n    \n    \n      word_str\n      \n    \n  \n  \n    \n      man\n      0.136766\n    \n    \n      gentleman\n      0.122441\n    \n    \n      girl\n      0.122036\n    \n    \n      fellow\n      0.100054\n    \n    \n      lady\n      0.096727\n    \n    \n      enough\n      0.092133\n    \n    \n      women\n      0.090621\n    \n    \n      young\n      0.086759\n    \n    \n      creature\n      0.086439\n    \n    \n      men\n      0.084672\n    \n  \n\n\n\n\n\ndef get_opposite(a, b, method='cosine'):\n    A = get_word_vector(a)\n    B = get_word_vector(b)\n    C = np.subtract(A, B)\n    X = get_nearest_vector(C, n=1, method=method)\n    return X\n#     return X.iloc[0].name\n\n\nget_sims('woman')\n\n\n\n\n\n  \n    \n      \n      score\n    \n    \n      word_str\n      \n    \n  \n  \n    \n      man\n      0.136766\n    \n    \n      gentleman\n      0.122441\n    \n    \n      girl\n      0.122036\n    \n    \n      fellow\n      0.100054\n    \n    \n      lady\n      0.096727\n    \n    \n      enough\n      0.092133\n    \n    \n      women\n      0.090621\n    \n    \n      young\n      0.086759\n    \n    \n      creature\n      0.086439\n    \n    \n      men\n      0.084672\n    \n  \n\n\n\n\n\ntest = get_nearest_vector(get_word_vector('king'), n=10)\n\n\ntest\n\n\n\n\n\n  \n    \n      \n      score\n    \n    \n      word_str\n      \n    \n  \n  \n    \n      versus\n      0.170774\n    \n    \n      wargrave\n      0.167875\n    \n    \n      rents\n      0.140268\n    \n    \n      longitudinal\n      0.128922\n    \n    \n      smollet\n      0.126644\n    \n    \n      felstein\n      0.115455\n    \n    \n      intuitions\n      0.114274\n    \n    \n      litre\n      0.112416\n    \n    \n      hanover\n      0.107851\n    \n    \n      duddings\n      0.101666\n    \n  \n\n\n\n\n\nget_sims('love')\n\n\n\n\n\n  \n    \n      \n      score\n    \n    \n      word_str\n      \n    \n  \n  \n    \n      loved\n      0.104928\n    \n    \n      affection\n      0.103414\n    \n    \n      your\n      0.090075\n    \n    \n      friendship\n      0.083776\n    \n    \n      tenderness\n      0.083345\n    \n    \n      esteem\n      0.082778\n    \n    \n      pity\n      0.080782\n    \n    \n      florentine\n      0.078317\n    \n    \n      transylvanian\n      0.074951\n    \n    \n      murdrer\n      0.074201\n    \n  \n\n\n\n\n\nget_opposite('man','beard')\n\n\n\n\n\n  \n    \n      \n      score\n    \n    \n      word_str\n      \n    \n  \n  \n    \n      woman\n      0.115028\n    \n  \n\n\n\n\n\nget_analogy('man','boy','girl')\n\n\n\n\n'woman'\n\n\n\nget_analogy('male', 'king', 'female')\n\n\n\n\n'garbed'\n\n\n\nSGM\n\n\n\n\n\n  \n    \n      probe\n      a\n      aback\n      abaft\n      abandon\n      abandoned\n      abandoning\n      abandons\n      abasement\n      abashed\n      abate\n      ...\n      zoöphagy\n      zufalle\n      zum\n      zuniga\n      zusammen\n      à\n      æt\n      ætat\n      ça\n      émeutes\n    \n    \n      word_str\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      a\n      -0.213386\n      0.026089\n      0.0\n      -0.032737\n      0.0\n      0.0\n      0.122628\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      aback\n      0.026089\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      abaft\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      abandon\n      -0.032737\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      abandoned\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      à\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      æt\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      ætat\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      ça\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      émeutes\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n27378 rows × 27378 columns"
  },
  {
    "objectID": "lessons/M09_WordEmbedding/M09_03_SkipgramRepresentation.html",
    "href": "lessons/M09_WordEmbedding/M09_03_SkipgramRepresentation.html",
    "title": "Metadata",
    "section": "",
    "text": "Course:    DS 5001\nModule:    09 Lab\nTopic:     Skip Gram Representations\nAuthor:    R.C. Alvarado\nDate:      28 March 2023 (revised)\nPurpose: A demonstration of how to create CBOW vector spaces.\n\nSet Up\n\ndata_in = '../data/novels'\ndata_prefix = 'novels'\n\n\nOHCO = ['book', 'chapter', 'para_num', 'sent_num', 'token_num']\nBAG = OHCO[2:5] # Paragraphs\n\n\n# Word Embedding window\nw = 2\n\n\nimport pandas as pd\nimport numpy as np\nimport scipy as sp\n\n\n\nImport Data\n\nTOKENS = pd.read_csv(f'{data_in}/{data_prefix}-TOKENS.csv')\n\n\n\nCreate DOCS as lists of tokens\n\nDOCS = TOKENS.groupby(BAG).apply(lambda x: x.term_str.tolist()).reset_index(drop=True)\n\n\nDOCS.head()\n\n0    [fourth, in, i, the, i, the, keeping, while, h...\n1    [narrative, the, spoke, question, am, first, m...\n2    [first, of, of, truly, thing, private, was, on...\n3    [part, my, how, sorry, i, sentiments, in, mome...\n4    [of, lady, i, to, did, to, this, i, the, of, t...\ndtype: object\n\n\n\n\nCreate Windows\n\ndef get_windows(x):\n\n    global WINDOWS\n    \n    bag_id  = x.name\n    row = x[0]\n\n    # Move through each word in the bag vector\n    for i in range(len(row)):\n    \n        # Slide the window\n        anchor = row[i]\n        for j in range(-w, w+1):\n            a = i + j\n            if j != 0 and a >= 0 and a < len(row):\n                WINDOWS.append((bag_id, i, anchor, j, row[a])) \n\n\nWINDOWS = []\nDOCS.to_frame(0).apply(get_windows, 1)\nW = pd.DataFrame(WINDOWS, columns=['bag_id', 'window_id', 'anchor', 'dist', 'probe']).set_index(['bag_id', 'window_id', 'anchor', 'dist'])\n\n\nW.head()\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      probe\n    \n    \n      bag_id\n      window_id\n      anchor\n      dist\n      \n    \n  \n  \n    \n      0\n      0\n      fourth\n      1\n      in\n    \n    \n      2\n      i\n    \n    \n      1\n      in\n      -1\n      fourth\n    \n    \n      1\n      i\n    \n    \n      2\n      the\n    \n  \n\n\n\n\n\nN_terms = len(W.probe.unique())\n\n\n\nAs CBOW / Skipgram\nFor predictive modeling, i.e. with a shallow NN.\n\nCBOW = W.probe.unstack().fillna('<s>').reset_index().set_index(['bag_id','window_id'])\n\n\nCBOW.head(20)\n\n\n\n\n\n  \n    \n      \n      dist\n      anchor\n      -2\n      -1\n      1\n      2\n    \n    \n      bag_id\n      window_id\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0\n      fourth\n      <s>\n      <s>\n      in\n      i\n    \n    \n      1\n      in\n      <s>\n      fourth\n      i\n      the\n    \n    \n      2\n      i\n      fourth\n      in\n      the\n      i\n    \n    \n      3\n      the\n      in\n      i\n      i\n      the\n    \n    \n      4\n      i\n      i\n      the\n      the\n      keeping\n    \n    \n      5\n      the\n      the\n      i\n      keeping\n      while\n    \n    \n      6\n      keeping\n      i\n      the\n      while\n      here\n    \n    \n      7\n      while\n      the\n      keeping\n      here\n      june\n    \n    \n      8\n      here\n      keeping\n      while\n      june\n      one\n    \n    \n      9\n      june\n      while\n      here\n      one\n      when\n    \n    \n      10\n      one\n      here\n      june\n      when\n      the\n    \n    \n      11\n      when\n      june\n      one\n      the\n      i\n    \n    \n      12\n      the\n      one\n      when\n      i\n      the\n    \n    \n      13\n      i\n      when\n      the\n      the\n      the\n    \n    \n      14\n      the\n      the\n      i\n      the\n      we\n    \n    \n      15\n      the\n      i\n      the\n      we\n      nothing\n    \n    \n      16\n      we\n      the\n      the\n      nothing\n      going\n    \n    \n      17\n      nothing\n      the\n      we\n      going\n      the\n    \n    \n      18\n      going\n      we\n      nothing\n      the\n      those\n    \n    \n      19\n      the\n      nothing\n      going\n      those\n      the\n    \n  \n\n\n\n\n\n\nAs Matrix\nFor matrix decomposition.\n\nM = W.groupby(['anchor','probe']).probe.count().unstack(fill_value=0)\n\n\nM\n\n\n\n\n\n  \n    \n      probe\n      a\n      aback\n      abaft\n      abandon\n      abandoned\n      abandoning\n      abandons\n      abasement\n      abashed\n      abate\n      ...\n      zoöphagous\n      zoöphagy\n      zufalle\n      zum\n      zusammen\n      à\n      æt\n      ætat\n      ça\n      émeutes\n    \n    \n      anchor\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      a\n      1916\n      0\n      0\n      4\n      4\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n    \n    \n      aback\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      abaft\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      abandon\n      4\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      abandoned\n      4\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      à\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      æt\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ætat\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ça\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      émeutes\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n26973 rows × 26973 columns"
  },
  {
    "objectID": "lessons/M09_WordEmbedding/M09_04_word2vec.html",
    "href": "lessons/M09_WordEmbedding/M09_04_word2vec.html",
    "title": "Metadata",
    "section": "",
    "text": "Set Up\nWe import data from the TOKEN table of the novels corpus, excluding proper nouns."
  },
  {
    "objectID": "lessons/M09_WordEmbedding/M09_04_word2vec.html#get-model-coordinates-to-plot",
    "href": "lessons/M09_WordEmbedding/M09_04_word2vec.html#get-model-coordinates-to-plot",
    "title": "Metadata",
    "section": "Get model coordinates to plot",
    "text": "Get model coordinates to plot\n\ncoords = pd.DataFrame(\n    dict(\n        vector = [model.wv.get_vector(w) for w in model.wv.vocab], \n        term_str = model.wv.vocab.keys()\n    )).set_index('term_str')\n\n\n# coords\n\n\n# TFM = coords.apply(lambda x: pd.Series(x.vector), 1)"
  },
  {
    "objectID": "lessons/M09_WordEmbedding/M09_04_word2vec.html#use-scikitlearns-tsne-library",
    "href": "lessons/M09_WordEmbedding/M09_04_word2vec.html#use-scikitlearns-tsne-library",
    "title": "Metadata",
    "section": "Use ScikitLearn’s TSNE library",
    "text": "Use ScikitLearn’s TSNE library\n\ntsne_engine = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\ntsne_model = tsne_engine.fit_transform(coords.vector.to_list())\n\n/Users/rca2t1/anaconda3/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n  warnings.warn(\n/Users/rca2t1/anaconda3/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py:982: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n  warnings.warn(\n\n\n\ncoords['x'] = tsne_model[:,0]\ncoords['y'] = tsne_model[:,1]"
  },
  {
    "objectID": "lessons/M09_WordEmbedding/M09_04_word2vec.html#add-some-vocab-features",
    "href": "lessons/M09_WordEmbedding/M09_04_word2vec.html#add-some-vocab-features",
    "title": "Metadata",
    "section": "Add some vocab features",
    "text": "Add some vocab features\n\nif coords.shape[1] == 3:\n    coords = coords.merge(VOCAB.reset_index(), on='term_str')\n    coords = coords.set_index('term_str')\n\n\ncoords = coords[coords.stop == 0]\n\n\n# coords.head()"
  },
  {
    "objectID": "lessons/M09_WordEmbedding/M09_04_word2vec.html#plot-the-coordinates",
    "href": "lessons/M09_WordEmbedding/M09_04_word2vec.html#plot-the-coordinates",
    "title": "Metadata",
    "section": "Plot the coordinates",
    "text": "Plot the coordinates\n\npx.scatter(coords.reset_index(), 'x', 'y', \n           text='term_str', \n           color='pos_max', \n           hover_name='term_str',          \n           size='tfidf_max',\n           height=1000).update_traces(\n                mode='markers+text', \n                textfont=dict(color='black', size=14, family='Arial'),\n                textposition='top center')"
  },
  {
    "objectID": "lessons/M09_WordEmbedding/M09_04_word2vec.html#analogies",
    "href": "lessons/M09_WordEmbedding/M09_04_word2vec.html#analogies",
    "title": "Metadata",
    "section": "Analogies",
    "text": "Analogies\n\\(A : B :: C : D? \\rightarrow B - A + C = D\\)\n\ndef complete_analogy(A, B, C, n=2):\n    try:\n        cols = ['term', 'sim']\n        return pd.DataFrame(model.wv.most_similar(positive=[B, C], negative=[A])[0:n], columns=cols)\n    except KeyError as e:\n        print('Error:', e)\n        return None\n    \ndef get_most_similar(positive, negative=None):\n    return pd.DataFrame(model.wv.most_similar(positive, negative), columns=['term', 'sim'])\n\n\ncomplete_analogy('man', 'boy', 'woman', 3)\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      girl\n      0.751171\n    \n    \n      1\n      peasant\n      0.716471\n    \n    \n      2\n      doctor\n      0.713492\n    \n  \n\n\n\n\n\ncomplete_analogy('girl', 'daughter', 'boy', 3)\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      uncle\n      0.792657\n    \n    \n      1\n      master\n      0.770206\n    \n    \n      2\n      instructions\n      0.767312\n    \n  \n\n\n\n\n\ncomplete_analogy('girl', 'sister', 'boy', 3)\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      cousin\n      0.813580\n    \n    \n      1\n      prayers\n      0.761379\n    \n    \n      2\n      uncle\n      0.750564\n    \n  \n\n\n\n\n\ncomplete_analogy('man', 'gentleman', 'woman', 5)\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      girl\n      0.844751\n    \n    \n      1\n      lady\n      0.743035\n    \n    \n      2\n      peasant\n      0.742487\n    \n    \n      3\n      fellow\n      0.734737\n    \n    \n      4\n      excellent\n      0.702886\n    \n  \n\n\n\n\n\ncomplete_analogy('man', 'woman', 'gentleman', 5)\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      girl\n      0.844751\n    \n    \n      1\n      lady\n      0.743035\n    \n    \n      2\n      peasant\n      0.742487\n    \n    \n      3\n      fellow\n      0.734737\n    \n    \n      4\n      excellent\n      0.702886\n    \n  \n\n\n\n\n\ncomplete_analogy('woman', 'man', 'lady', 5)\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      gentleman\n      0.723555\n    \n    \n      1\n      girl\n      0.693647\n    \n    \n      2\n      doctor\n      0.680716\n    \n    \n      3\n      friend\n      0.675953\n    \n    \n      4\n      servant\n      0.666661\n    \n  \n\n\n\n\n\ncomplete_analogy('day', 'night', 'sun', 5)\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      rain\n      0.849055\n    \n    \n      1\n      twilight\n      0.830607\n    \n    \n      2\n      gates\n      0.826733\n    \n    \n      3\n      breeze\n      0.822108\n    \n    \n      4\n      clouds\n      0.818364"
  },
  {
    "objectID": "lessons/M09_WordEmbedding/M09_04_word2vec.html#similarites",
    "href": "lessons/M09_WordEmbedding/M09_04_word2vec.html#similarites",
    "title": "Metadata",
    "section": "Similarites",
    "text": "Similarites\n\nget_most_similar('joy')\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      tenderness\n      0.905040\n    \n    \n      1\n      terror\n      0.901490\n    \n    \n      2\n      grief\n      0.896896\n    \n    \n      3\n      anguish\n      0.887137\n    \n    \n      4\n      horror\n      0.871283\n    \n    \n      5\n      delight\n      0.868740\n    \n    \n      6\n      admiration\n      0.868232\n    \n    \n      7\n      disgust\n      0.864322\n    \n    \n      8\n      emotion\n      0.861447\n    \n    \n      9\n      indignation\n      0.860309\n    \n  \n\n\n\n\n\nget_most_similar('man')\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      gentleman\n      0.861928\n    \n    \n      1\n      woman\n      0.846830\n    \n    \n      2\n      girl\n      0.768002\n    \n    \n      3\n      fellow\n      0.728416\n    \n    \n      4\n      stranger\n      0.694809\n    \n    \n      5\n      creature\n      0.666995\n    \n    \n      6\n      peasant\n      0.665809\n    \n    \n      7\n      person\n      0.662508\n    \n    \n      8\n      lady\n      0.631155\n    \n    \n      9\n      servant\n      0.630209\n    \n  \n\n\n\n\n\nget_most_similar(positive=['man'], negative=['woman'])\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      us\n      0.373235\n    \n    \n      1\n      his\n      0.352213\n    \n    \n      2\n      them\n      0.334045\n    \n    \n      3\n      our\n      0.325573\n    \n    \n      4\n      town\n      0.319389\n    \n    \n      5\n      police\n      0.316197\n    \n    \n      6\n      box\n      0.299743\n    \n    \n      7\n      ourselves\n      0.298184\n    \n    \n      8\n      off\n      0.290156\n    \n    \n      9\n      together\n      0.288549\n    \n  \n\n\n\n\n\nget_most_similar(positive='woman')\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      gentleman\n      0.858438\n    \n    \n      1\n      girl\n      0.847014\n    \n    \n      2\n      man\n      0.846830\n    \n    \n      3\n      creature\n      0.769018\n    \n    \n      4\n      fellow\n      0.765501\n    \n    \n      5\n      stranger\n      0.743524\n    \n    \n      6\n      peasant\n      0.732994\n    \n    \n      7\n      honest\n      0.703872\n    \n    \n      8\n      lady\n      0.698289\n    \n    \n      9\n      youth\n      0.671496\n    \n  \n\n\n\n\n\nget_most_similar(positive=['woman'], negative=['man'])\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      particularly\n      0.506908\n    \n    \n      1\n      extremely\n      0.451128\n    \n    \n      2\n      greatly\n      0.445201\n    \n    \n      3\n      shocked\n      0.425791\n    \n    \n      4\n      utter\n      0.422976\n    \n    \n      5\n      painful\n      0.422493\n    \n    \n      6\n      truly\n      0.419563\n    \n    \n      7\n      affected\n      0.401743\n    \n    \n      8\n      rendered\n      0.398629\n    \n    \n      9\n      strongly\n      0.397592\n    \n  \n\n\n\n\n\nget_most_similar(['man','woman'],['boy','girl'])\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      nor\n      0.485619\n    \n    \n      1\n      respect\n      0.445959\n    \n    \n      2\n      human\n      0.434997\n    \n    \n      3\n      even\n      0.425449\n    \n    \n      4\n      real\n      0.420330\n    \n    \n      5\n      felt\n      0.415230\n    \n    \n      6\n      such\n      0.413426\n    \n    \n      7\n      or\n      0.394491\n    \n    \n      8\n      person\n      0.386583\n    \n    \n      9\n      produce\n      0.378022"
  },
  {
    "objectID": "lessons/M09_WordEmbedding/M09_05_FastText.html",
    "href": "lessons/M09_WordEmbedding/M09_05_FastText.html",
    "title": "Metadata",
    "section": "",
    "text": "Set Up"
  },
  {
    "objectID": "lessons/M09_WordEmbedding/M09_05_FastText.html#generate-coordinates-to-plot",
    "href": "lessons/M09_WordEmbedding/M09_05_FastText.html#generate-coordinates-to-plot",
    "title": "Metadata",
    "section": "Generate coordinates to plot",
    "text": "Generate coordinates to plot\n\ncoords = pd.DataFrame(index=range(1000))\ncoords['label'] = V\ncoords['vector'] = coords['label'].apply(lambda x: model.wv[x])\n\n\ncoords.head()\n\n\n\n\n\n  \n    \n      \n      label\n      vector\n    \n  \n  \n    \n      0\n      guides\n      [-0.0015094569, -0.0014069626, 0.00077895704, ...\n    \n    \n      1\n      donations\n      [0.0007798648, -0.00050445384, -3.0623694e-05,...\n    \n    \n      2\n      works\n      [8.765071e-05, -0.0010986227, 0.0011741201, 0....\n    \n    \n      3\n      cratchits\n      [-0.00061139435, 0.0013432469, -0.0010389704, ...\n    \n    \n      4\n      pips\n      [0.004343744, 0.00029701128, 0.0008883734, -0...."
  },
  {
    "objectID": "lessons/M09_WordEmbedding/M09_05_FastText.html#use-scikitlearns-tsne-library",
    "href": "lessons/M09_WordEmbedding/M09_05_FastText.html#use-scikitlearns-tsne-library",
    "title": "Metadata",
    "section": "Use ScikitLearn’s TSNE library",
    "text": "Use ScikitLearn’s TSNE library\n\ntsne_engine = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\ntsne_model = tsne_engine.fit_transform(coords['vector'].tolist())\n\n/Users/rca2t1/anaconda3/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n  warnings.warn(\n/Users/rca2t1/anaconda3/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py:982: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n  warnings.warn(\n\n\n\ncoords['x'] = tsne_model[:,0]\ncoords['y'] = tsne_model[:,1]\n\n\ncoords.head()\n\n\n\n\n\n  \n    \n      \n      label\n      vector\n      x\n      y\n    \n  \n  \n    \n      0\n      guides\n      [-0.0015094569, -0.0014069626, 0.00077895704, ...\n      15.908065\n      4.329574\n    \n    \n      1\n      donations\n      [0.0007798648, -0.00050445384, -3.0623694e-05,...\n      3.246412\n      -6.150486\n    \n    \n      2\n      works\n      [8.765071e-05, -0.0010986227, 0.0011741201, 0....\n      4.241611\n      -4.747810\n    \n    \n      3\n      cratchits\n      [-0.00061139435, 0.0013432469, -0.0010389704, ...\n      -4.634530\n      -1.217741\n    \n    \n      4\n      pips\n      [0.004343744, 0.00029701128, 0.0008883734, -0....\n      6.874166\n      -16.001637"
  },
  {
    "objectID": "lessons/M09_WordEmbedding/M09_05_FastText.html#plot-the-coordinates",
    "href": "lessons/M09_WordEmbedding/M09_05_FastText.html#plot-the-coordinates",
    "title": "Metadata",
    "section": "Plot the coordinates",
    "text": "Plot the coordinates\n\npx.scatter(coords, 'x', 'y', text='label', height=1000).update_traces(mode='text')"
  },
  {
    "objectID": "lessons/M09_WordEmbedding/M09_05_FastText.html#analogies",
    "href": "lessons/M09_WordEmbedding/M09_05_FastText.html#analogies",
    "title": "Metadata",
    "section": "Analogies",
    "text": "Analogies\n\\(A : B :: C : D? \\rightarrow B - A + C = D\\)\n\ndef complete_analogy(A, B, C, n=2):\n    try:\n        cols = ['term', 'sim']\n        return pd.DataFrame(model.wv.most_similar(positive=[B, C], negative=[A])[:n], columns=cols)\n    except KeyError as e:\n        print('Error:', e)\n        return None\n    \ndef get_most_similar(positive, negative=None):\n    return pd.DataFrame(model.wv.most_similar(positive, negative), columns=['term', 'sim'])\n\n\ncomplete_analogy('man', 'boy', 'woman', 3)\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      æ\n      0.141895\n    \n    \n      1\n      t\n      0.116483\n    \n    \n      2\n      e\n      0.082533\n    \n  \n\n\n\n\n\ncomplete_analogy('girl', 'daughter', 'boy', 3)\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      q\n      0.230626\n    \n    \n      1\n      g\n      0.219620\n    \n    \n      2\n      w\n      0.143295\n    \n  \n\n\n\n\n\ncomplete_analogy('girl', 'sister', 'boy', 3)\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      q\n      0.303455\n    \n    \n      1\n      d\n      0.243905\n    \n    \n      2\n      t\n      0.241948\n    \n  \n\n\n\n\n\ncomplete_analogy('man', 'gentleman', 'woman', 5)\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      æ\n      0.071702\n    \n    \n      1\n      e\n      0.053351\n    \n    \n      2\n      i\n      0.038423\n    \n    \n      3\n      p\n      0.037130\n    \n    \n      4\n      t\n      0.022659\n    \n  \n\n\n\n\n\ncomplete_analogy('woman', 'lady', 'man', 5)\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      v\n      0.179676\n    \n    \n      1\n      m\n      0.160621\n    \n    \n      2\n      â\n      0.159666\n    \n    \n      3\n      z\n      0.157661\n    \n    \n      4\n      ö\n      0.157205\n    \n  \n\n\n\n\n\ncomplete_analogy('day', 'sun', 'night', 5)\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      u\n      0.329168\n    \n    \n      1\n      e\n      0.188017\n    \n    \n      2\n      è\n      0.140207\n    \n    \n      3\n      â\n      0.118661\n    \n    \n      4\n      à\n      0.115619"
  },
  {
    "objectID": "lessons/M09_WordEmbedding/M09_05_FastText.html#similarites",
    "href": "lessons/M09_WordEmbedding/M09_05_FastText.html#similarites",
    "title": "Metadata",
    "section": "Similarites",
    "text": "Similarites\n\nget_most_similar('joy')\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      p\n      0.200325\n    \n    \n      1\n      c\n      0.181891\n    \n    \n      2\n      f\n      0.165556\n    \n    \n      3\n      â\n      0.136493\n    \n    \n      4\n      v\n      0.127911\n    \n    \n      5\n      x\n      0.126062\n    \n    \n      6\n      s\n      0.106196\n    \n    \n      7\n      ô\n      0.103464\n    \n    \n      8\n      d\n      0.102738\n    \n    \n      9\n      w\n      0.095959\n    \n  \n\n\n\n\n\nget_most_similar('man')\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      k\n      0.175545\n    \n    \n      1\n      h\n      0.151002\n    \n    \n      2\n      o\n      0.148407\n    \n    \n      3\n      f\n      0.107983\n    \n    \n      4\n      [\n      0.104052\n    \n    \n      5\n      q\n      0.093817\n    \n    \n      6\n      s\n      0.091813\n    \n    \n      7\n      a\n      0.088121\n    \n    \n      8\n      y\n      0.082049\n    \n    \n      9\n      g\n      0.079399\n    \n  \n\n\n\n\n\nget_most_similar(['woman','girl'], ['man'])\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      e\n      0.184911\n    \n    \n      1\n      æ\n      0.154177\n    \n    \n      2\n      r\n      0.106785\n    \n    \n      3\n      x\n      0.055995\n    \n    \n      4\n      y\n      0.043053\n    \n    \n      5\n      i\n      0.030713\n    \n    \n      6\n      p\n      0.027070\n    \n    \n      7\n      ô\n      0.016304\n    \n    \n      8\n      a\n      0.010156\n    \n    \n      9\n      é\n      0.005075\n    \n  \n\n\n\n\n\nget_most_similar(positive=['man'], negative=['woman'])\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      k\n      0.223951\n    \n    \n      1\n      o\n      0.202994\n    \n    \n      2\n      h\n      0.179946\n    \n    \n      3\n      f\n      0.177833\n    \n    \n      4\n      m\n      0.175260\n    \n    \n      5\n      â\n      0.153746\n    \n    \n      6\n      z\n      0.137688\n    \n    \n      7\n      [\n      0.134596\n    \n    \n      8\n      ,\n      0.132834\n    \n    \n      9\n      ö\n      0.129357\n    \n  \n\n\n\n\n\nget_most_similar(positive=['woman'], negative=['girl'])\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      q\n      0.075670\n    \n    \n      1\n      y\n      0.047197\n    \n    \n      2\n      à\n      0.023397\n    \n    \n      3\n      c\n      0.022546\n    \n    \n      4\n      ô\n      0.019146\n    \n    \n      5\n      j\n      0.006668\n    \n    \n      6\n      s\n      -0.001257\n    \n    \n      7\n      t\n      -0.007901\n    \n    \n      8\n      g\n      -0.009781\n    \n    \n      9\n      d\n      -0.022968\n    \n  \n\n\n\n\n\nget_most_similar(positive='woman')\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      e\n      0.118485\n    \n    \n      1\n      y\n      0.109202\n    \n    \n      2\n      s\n      0.012907\n    \n    \n      3\n      æ\n      0.012595\n    \n    \n      4\n      c\n      0.006432\n    \n    \n      5\n      ô\n      0.003896\n    \n    \n      6\n      j\n      -0.003665\n    \n    \n      7\n      p\n      -0.018191\n    \n    \n      8\n      t\n      -0.018866\n    \n    \n      9\n      n\n      -0.023236\n    \n  \n\n\n\n\n\nget_most_similar('woman')\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      e\n      0.118485\n    \n    \n      1\n      y\n      0.109202\n    \n    \n      2\n      s\n      0.012907\n    \n    \n      3\n      æ\n      0.012595\n    \n    \n      4\n      c\n      0.006432\n    \n    \n      5\n      ô\n      0.003896\n    \n    \n      6\n      j\n      -0.003665\n    \n    \n      7\n      p\n      -0.018191\n    \n    \n      8\n      t\n      -0.018866\n    \n    \n      9\n      n\n      -0.023236\n    \n  \n\n\n\n\n\nget_most_similar(['woman'],['marriage'])\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      t\n      0.129735\n    \n    \n      1\n      e\n      0.119505\n    \n    \n      2\n      s\n      0.118860\n    \n    \n      3\n      k\n      0.105998\n    \n    \n      4\n      n\n      0.063869\n    \n    \n      5\n      [\n      0.061910\n    \n    \n      6\n      p\n      0.061215\n    \n    \n      7\n      j\n      0.058395\n    \n    \n      8\n      c\n      0.049748\n    \n    \n      9\n      \n      0.030749\n    \n  \n\n\n\n\n\nget_most_similar(['woman'],['lady'])\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      y\n      0.134945\n    \n    \n      1\n      e\n      0.122754\n    \n    \n      2\n      i\n      0.114970\n    \n    \n      3\n      c\n      0.091859\n    \n    \n      4\n      s\n      0.033937\n    \n    \n      5\n      u\n      0.026046\n    \n    \n      6\n      p\n      0.024570\n    \n    \n      7\n      [\n      0.014697\n    \n    \n      8\n      t\n      0.003845\n    \n    \n      9\n      x\n      -0.004496\n    \n  \n\n\n\n\n\nget_most_similar(['man'],['gentleman'])\n\n\n\n\n\n  \n    \n      \n      term\n      sim\n    \n  \n  \n    \n      0\n      y\n      0.175613\n    \n    \n      1\n      f\n      0.157579\n    \n    \n      2\n      h\n      0.134571\n    \n    \n      3\n      o\n      0.128863\n    \n    \n      4\n      q\n      0.128854\n    \n    \n      5\n      g\n      0.123197\n    \n    \n      6\n      k\n      0.112518\n    \n    \n      7\n      â\n      0.083391\n    \n    \n      8\n      è\n      0.080266\n    \n    \n      9\n      w\n      0.068027"
  },
  {
    "objectID": "lessons/M09_WordEmbedding/Untitled.html",
    "href": "lessons/M09_WordEmbedding/Untitled.html",
    "title": "{{< var course_title >}}",
    "section": "",
    "text": "import pandas as pd"
  },
  {
    "objectID": "lessons/M09_WordEmbedding/run_fasttext.html#when-to-use-fasttext",
    "href": "lessons/M09_WordEmbedding/run_fasttext.html#when-to-use-fasttext",
    "title": "FastText Model",
    "section": "When to use fastText?",
    "text": "When to use fastText?\nThe main principle behind fastText <https://github.com/facebookresearch/fastText>_ is that the morphological structure of a word carries important information about the meaning of the word. Such structure is not taken into account by traditional word embeddings like Word2Vec, which train a unique word embedding for every individual word. This is especially significant for morphologically rich languages (German, Turkish) in which a single word can have a large number of morphological forms, each of which might occur rarely, thus making it hard to train good word embeddings.\nfastText attempts to solve this by treating each word as the aggregation of its subwords. For the sake of simplicity and language-independence, subwords are taken to be the character ngrams of the word. The vector for a word is simply taken to be the sum of all vectors of its component char-ngrams.\nAccording to a detailed comparison of Word2Vec and fastText in this notebook <https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb>__, fastText does significantly better on syntactic tasks as compared to the original Word2Vec, especially when the size of the training corpus is small. Word2Vec slightly outperforms fastText on semantic tasks though. The differences grow smaller as the size of the training corpus increases.\nfastText can obtain vectors even for out-of-vocabulary (OOV) words, by summing up vectors for its component char-ngrams, provided at least one of the char-ngrams was present in the training data."
  },
  {
    "objectID": "lessons/M09_WordEmbedding/run_fasttext.html#training-models",
    "href": "lessons/M09_WordEmbedding/run_fasttext.html#training-models",
    "title": "FastText Model",
    "section": "Training models",
    "text": "Training models\nFor the following examples, we’ll use the Lee Corpus (which you already have if you’ve installed Gensim) for training our model.\n\nfrom pprint import pprint as print\nfrom gensim.models.fasttext import FastText\nfrom gensim.test.utils import datapath\n\n# Set file names for train and test data\ncorpus_file = datapath('lee_background.cor')\n\nmodel = FastText(vector_size=100)\n\n# build the vocabulary\nmodel.build_vocab(corpus_file=corpus_file)\n\n# train the model\nmodel.train(\n    corpus_file=corpus_file, epochs=model.epochs,\n    total_examples=model.corpus_count, total_words=model.corpus_total_words,\n)\n\nprint(model)\n\nTypeError: __init__() got an unexpected keyword argument 'vector_size'\n\n\nTraining hyperparameters ^^^^^^^^^^^^^^^^^^^^^^^^\nHyperparameters for training the model follow the same pattern as Word2Vec. FastText supports the following parameters from the original word2vec:\n\nmodel: Training architecture. Allowed values: cbow, skipgram (Default cbow)\nvector_size: Dimensionality of vector embeddings to be learnt (Default 100)\nalpha: Initial learning rate (Default 0.025)\nwindow: Context window size (Default 5)\nmin_count: Ignore words with number of occurrences below this (Default 5)\nloss: Training objective. Allowed values: ns, hs, softmax (Default ns)\nsample: Threshold for downsampling higher-frequency words (Default 0.001)\nnegative: Number of negative words to sample, for ns (Default 5)\nepochs: Number of epochs (Default 5)\nsorted_vocab: Sort vocab by descending frequency (Default 1)\nthreads: Number of threads to use (Default 12)\n\nIn addition, fastText has three additional parameters:\n\nmin_n: min length of char ngrams (Default 3)\nmax_n: max length of char ngrams (Default 6)\nbucket: number of buckets used for hashing ngrams (Default 2000000)\n\nParameters min_n and max_n control the lengths of character ngrams that each word is broken down into while training and looking up embeddings. If max_n is set to 0, or to be lesser than min_n , no character ngrams are used, and the model effectively reduces to Word2Vec.\nTo bound the memory requirements of the model being trained, a hashing function is used that maps ngrams to integers in 1 to K. For hashing these character sequences, the Fowler-Noll-Vo hashing function <http://www.isthe.com/chongo/tech/comp/fnv>_ (FNV-1a variant) is employed.\nNote: You can continue to train your model while using Gensim’s native implementation of fastText."
  },
  {
    "objectID": "lessons/M09_WordEmbedding/run_fasttext.html#savingloading-models",
    "href": "lessons/M09_WordEmbedding/run_fasttext.html#savingloading-models",
    "title": "FastText Model",
    "section": "Saving/loading models",
    "text": "Saving/loading models\nModels can be saved and loaded via the load and save methods, just like any other model in Gensim.\n\n# Save a model trained via Gensim's fastText implementation to temp.\nimport tempfile\nimport os\nwith tempfile.NamedTemporaryFile(prefix='saved_model_gensim-', delete=False) as tmp:\n    model.save(tmp.name, separately=[])\n\n# Load back the same model.\nloaded_model = FastText.load(tmp.name)\nprint(loaded_model)\n\nos.unlink(tmp.name)  # demonstration complete, don't need the temp file anymore\n\nThe save_word2vec_format is also available for fastText models, but will cause all vectors for ngrams to be lost. As a result, a model loaded in this way will behave as a regular word2vec model."
  },
  {
    "objectID": "lessons/M09_WordEmbedding/run_fasttext.html#word-vector-lookup",
    "href": "lessons/M09_WordEmbedding/run_fasttext.html#word-vector-lookup",
    "title": "FastText Model",
    "section": "Word vector lookup",
    "text": "Word vector lookup\nAll information necessary for looking up fastText words (incl. OOV words) is contained in its model.wv attribute.\nIf you don’t need to continue training your model, you can export & save this .wv attribute and discard model, to save space and RAM.\n\nwv = model.wv\nprint(wv)\n\n#\n# FastText models support vector lookups for out-of-vocabulary words by summing up character ngrams belonging to the word.\n#\nprint('night' in wv.key_to_index)\n\n\nprint('nights' in wv.key_to_index)\n\n\nprint(wv['night'])\n\n\nprint(wv['nights'])"
  },
  {
    "objectID": "lessons/M09_WordEmbedding/run_fasttext.html#similarity-operations",
    "href": "lessons/M09_WordEmbedding/run_fasttext.html#similarity-operations",
    "title": "FastText Model",
    "section": "Similarity operations",
    "text": "Similarity operations\nSimilarity operations work the same way as word2vec. Out-of-vocabulary words can also be used, provided they have at least one character ngram present in the training data.\n\nprint(\"nights\" in wv.key_to_index)\n\n\nprint(\"night\" in wv.key_to_index)\n\n\nprint(wv.similarity(\"night\", \"nights\"))\n\nSyntactically similar words generally have high similarity in fastText models, since a large number of the component char-ngrams will be the same. As a result, fastText generally does better at syntactic tasks than Word2Vec. A detailed comparison is provided here <Word2Vec_FastText_Comparison.ipynb>_.\nOther similarity operations ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nThe example training corpus is a toy corpus, results are not expected to be good, for proof-of-concept only\n\nprint(wv.most_similar(\"nights\"))\n\n\nprint(wv.n_similarity(['sushi', 'shop'], ['japanese', 'restaurant']))\n\n\nprint(wv.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n\n\nprint(wv.most_similar(positive=['baghdad', 'england'], negative=['london']))\n\n\nprint(wv.evaluate_word_analogies(datapath('questions-words.txt')))\n\nWord Movers distance ^^^^^^^^^^^^^^^^^^^^\nYou’ll need the optional pyemd library for this section, pip install pyemd.\nLet’s start with two sentences:\n\nsentence_obama = 'Obama speaks to the media in Illinois'.lower().split()\nsentence_president = 'The president greets the press in Chicago'.lower().split()\n\nRemove their stopwords.\n\nfrom gensim.parsing.preprocessing import STOPWORDS\nsentence_obama = [w for w in sentence_obama if w not in STOPWORDS]\nsentence_president = [w for w in sentence_president if w not in STOPWORDS]\n\nCompute the Word Movers Distance between the two sentences.\n\ndistance = wv.wmdistance(sentence_obama, sentence_president)\nprint(f\"Word Movers Distance is {distance} (lower means closer)\")\n\nThat’s all! You’ve made it to the end of this tutorial.\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimg = mpimg.imread('fasttext-logo-color-web.png')\nimgplot = plt.imshow(img)\n_ = plt.axis('off')"
  },
  {
    "objectID": "lessons/M10_SentimentAnalysis/M10_01_GeneralInquirer.html",
    "href": "lessons/M10_SentimentAnalysis/M10_01_GeneralInquirer.html",
    "title": "Metadata",
    "section": "",
    "text": "Setting Up\nIt has 182 ontology/sentiment columns\nSplit entries like FASCINATE#1 and FASCINATE#2 with FASCINATE, 1 and FASCINATE, 2 (so our data are atomic)."
  },
  {
    "objectID": "lessons/M10_SentimentAnalysis/M10_01_GeneralInquirer.html#save",
    "href": "lessons/M10_SentimentAnalysis/M10_01_GeneralInquirer.html#save",
    "title": "Metadata",
    "section": "Save",
    "text": "Save\n\nGI.to_csv(data_in + '/lexicons/sources/gi.csv')"
  },
  {
    "objectID": "lessons/M10_SentimentAnalysis/M10_02_CombineLexicons.html",
    "href": "lessons/M10_SentimentAnalysis/M10_02_CombineLexicons.html",
    "title": "Metadata",
    "section": "",
    "text": "Set Up\nFirst, download RAR file from here. Then open in a text editor and convert encoding and line endings in an editor.\nThis is prepared in a separate notebook."
  },
  {
    "objectID": "lessons/M10_SentimentAnalysis/M10_02_CombineLexicons.html#create-single-sentiment-column",
    "href": "lessons/M10_SentimentAnalysis/M10_02_CombineLexicons.html#create-single-sentiment-column",
    "title": "Metadata",
    "section": "Create single sentiment column",
    "text": "Create single sentiment column\n\nbing['bing_sentiment'] = bing['bing_positive'] - bing['bing_negative']\n\n\nbing.sample(10)\n\n\n\n\n\n  \n    \n      polarity\n      bing_negative\n      bing_positive\n      bing_sentiment\n    \n    \n      term_str\n      \n      \n      \n    \n  \n  \n    \n      loud\n      1\n      0\n      -1\n    \n    \n      inept\n      1\n      0\n      -1\n    \n    \n      comical\n      1\n      0\n      -1\n    \n    \n      restricted\n      1\n      0\n      -1\n    \n    \n      improvements\n      0\n      1\n      1\n    \n    \n      weaken\n      1\n      0\n      -1\n    \n    \n      hypocritically\n      1\n      0\n      -1\n    \n    \n      blabber\n      1\n      0\n      -1\n    \n    \n      reputation\n      0\n      1\n      1\n    \n    \n      inescapable\n      1\n      0\n      -1"
  },
  {
    "objectID": "lessons/M10_SentimentAnalysis/M10_03_Novel.html",
    "href": "lessons/M10_SentimentAnalysis/M10_03_Novel.html",
    "title": "Metadata",
    "section": "",
    "text": "Set Up"
  },
  {
    "objectID": "lessons/M10_SentimentAnalysis/M10_03_Novel.html#config",
    "href": "lessons/M10_SentimentAnalysis/M10_03_Novel.html#config",
    "title": "Metadata",
    "section": "Config",
    "text": "Config\n\ndata_in = '../data'\ndata_out = '../data'\ndata_prefix = 'novels'\n\n\nnovels_csv =  f'{data_in}/novels/{data_prefix}-CORPUS.csv'\nOHCO = ['book_id', 'chap_id', 'para_num', 'sent_num'] # We exclude genre\nCHAPS = OHCO[2:3]\nPARAS = OHCO[2:4]\nSENTS = OHCO[2:5]\n\n\ndracula = ('stoker','dracula')\nnorabbey = ('austen','northangerabbey')\n\n\nsalex_csv = f'{data_in}/lexicons/salex_nrc.csv'\nnrc_cols = \"nrc_negative nrc_positive nrc_anger nrc_anticipation nrc_disgust nrc_fear nrc_joy nrc_sadness nrc_surprise nrc_trust\".split()\n# emo = 'polarity'\nemo = 'sentiment'"
  },
  {
    "objectID": "lessons/M10_SentimentAnalysis/M10_03_Novel.html#import",
    "href": "lessons/M10_SentimentAnalysis/M10_03_Novel.html#import",
    "title": "Metadata",
    "section": "Import",
    "text": "Import\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom IPython.display import display, HTML\n\n\nsns.set()\n# sns.set_style('whitegrid')"
  },
  {
    "objectID": "lessons/M10_SentimentAnalysis/M10_03_Novel.html#get-lexicon-columns",
    "href": "lessons/M10_SentimentAnalysis/M10_03_Novel.html#get-lexicon-columns",
    "title": "Metadata",
    "section": "Get lexicon columns",
    "text": "Get lexicon columns\n\nemo_cols = \"anger anticipation disgust fear joy sadness surprise trust sentiment\".split()\n\n\nemo_cols \n\n['anger',\n 'anticipation',\n 'disgust',\n 'fear',\n 'joy',\n 'sadness',\n 'surprise',\n 'trust',\n 'sentiment']"
  },
  {
    "objectID": "lessons/M10_SentimentAnalysis/M10_03_Novel.html#sentiment-by-chapter",
    "href": "lessons/M10_SentimentAnalysis/M10_03_Novel.html#sentiment-by-chapter",
    "title": "Metadata",
    "section": "Sentiment by Chapter",
    "text": "Sentiment by Chapter\n\nDRACULA_chaps = DRACULA.groupby(CHAPS)[emo_cols].mean()\n\n\nNORABBEY_chaps = NORABBEY.groupby(CHAPS)[emo_cols].mean()\n\n\ndef plot_sentiments(df, emo='sentiment'):\n    FIG = dict(figsize=(25, 5), legend=True, fontsize=14, rot=45)\n    df[emo].plot(**FIG)\n\n\nplot_sentiments(DRACULA_chaps, ['trust','fear','joy','sentiment'])\n\n\n\n\n\nplot_sentiments(DRACULA_chaps, ['sentiment'])\n\n\n\n\n\nplot_sentiments(NORABBEY_chaps, ['trust','joy','anticipation','sentiment'])\n\n\n\n\n\nplot_sentiments(NORABBEY_chaps, ['sentiment'])"
  },
  {
    "objectID": "lessons/M10_SentimentAnalysis/M10_03_Novel.html#vader",
    "href": "lessons/M10_SentimentAnalysis/M10_03_Novel.html#vader",
    "title": "Metadata",
    "section": "VADER",
    "text": "VADER\n\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nanalyser = SentimentIntensityAnalyzer()\n\n\nDRACULA_vader_cols = DRACULA_sents.sent_str.apply(analyser.polarity_scores).apply(lambda x: pd.Series(x))\nDRACULA_vader = pd.concat([DRACULA_sents, DRACULA_vader_cols], axis=1)\n\n\nDRACULA_vader\n\n\n\n\n\n  \n    \n      \n      \n      anger\n      anticipation\n      disgust\n      fear\n      joy\n      sadness\n      surprise\n      trust\n      sentiment\n      sent_str\n      html_str\n      neg\n      neu\n      pos\n      compound\n    \n    \n      para_num\n      sent_num\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      jonathan harkers journal i ii v letter from mi...\n      <span class='sent0'>jonathan</span> <span clas...\n      0.076\n      0.924\n      0.000\n      -0.2960\n    \n    \n      1\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      arthur holmwood\n      <span class='sent0'>arthur</span> <span class=...\n      0.000\n      1.000\n      0.000\n      0.0000\n    \n    \n      1\n      0\n      0.010638\n      0.000000\n      0.010638\n      0.010638\n      0.127660\n      0.010638\n      0.000000\n      0.127660\n      0.117021\n      kept in shorthand jonathan harkers journal con...\n      <span class='sent0'>kept</span> <span class='s...\n      0.016\n      0.984\n      0.000\n      -0.1280\n    \n    \n      2\n      0\n      0.013812\n      0.016575\n      0.011050\n      0.013812\n      0.038674\n      0.019337\n      0.019337\n      0.038674\n      0.019337\n      may may i must have been asleep for certainly ...\n      <span class='sent0'>may</span> <span class='se...\n      0.076\n      0.813\n      0.111\n      0.9247\n    \n    \n      1\n      0.002481\n      0.004963\n      0.004963\n      0.009926\n      0.012407\n      0.012407\n      0.000000\n      0.009926\n      0.007444\n      bistritz left munich at p m on may arriving at...\n      <span class='sent0'>bistritz</span> <span clas...\n      0.060\n      0.825\n      0.115\n      0.9838\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      115\n      0\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      i i did not want to hinder him i did not want ...\n      <span class='sent0'>i</span> <span class='sent...\n      0.149\n      0.851\n      0.000\n      -0.1139\n    \n    \n      116\n      0\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      they lay in a sort of or orderly they lay in a...\n      <span class='sent0'>they</span> <span class='s...\n      0.000\n      1.000\n      0.000\n      0.0000\n    \n    \n      117\n      0\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      translyvania transylvania pg\n      <span class='sent0'>translyvania</span> <span ...\n      0.000\n      1.000\n      0.000\n      0.0000\n    \n    \n      118\n      0\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      this mrrning from dardanelles this morning fro...\n      <span class='sent0'>this</span> <span class='s...\n      0.000\n      1.000\n      0.000\n      0.0000\n    \n    \n      119\n      0\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      end of the project gutenberg ebook of dracula ...\n      <span class='sent0'>end</span> <span class='se...\n      0.000\n      1.000\n      0.000\n      0.0000\n    \n  \n\n1694 rows × 15 columns\n\n\n\n\n# DRACULA_vader.head()\n\n\nw = int(DRACULA_vader.shape[0] / 5)\nDRACULA_vader[['pos','neg']].rolling(w).mean().plot(figsize=(25,5))\nDRACULA_vader[['neu']].rolling(w).mean().plot(figsize=(25,5))\nDRACULA_vader[['compound']].rolling(w).mean().plot(figsize=(25,5))\n\n<AxesSubplot: xlabel='para_num,sent_num'>\n\n\n\n\n\n\n\n\n\n\n\n\nNORABBEY_vader_cols = NORABBEY_sents.sent_str.apply(analyser.polarity_scores).apply(lambda x: pd.Series(x))\nNORABBEY_vader = pd.concat([NORABBEY_sents, NORABBEY_vader_cols], axis=1)\n\n\nw = int(NORABBEY_vader.shape[0] / 5)\nNORABBEY_vader[['pos','neg']].rolling(w).mean().plot(figsize=(25,5));\nNORABBEY_vader[['neu']].rolling(w).mean().plot(figsize=(25,5));\nNORABBEY_vader[['compound']].rolling(w).mean().plot(figsize=(25,5));"
  },
  {
    "objectID": "lessons/M10_SentimentAnalysis/M10_04_AustenMelville.html",
    "href": "lessons/M10_SentimentAnalysis/M10_04_AustenMelville.html",
    "title": "Metadata",
    "section": "",
    "text": "Set Up\nWe need to do this to reconstruct the sentences, which are lost in the BOW representation."
  },
  {
    "objectID": "lessons/M10_SentimentAnalysis/M10_04_AustenMelville.html#config",
    "href": "lessons/M10_SentimentAnalysis/M10_04_AustenMelville.html#config",
    "title": "Metadata",
    "section": "Config",
    "text": "Config\n\ndata_in = '../data'\ndata_out = '../data/output'\ndata_prefix = 'austen-melville'\n\n\nnovels_csv = f'{data_in}/output/{data_prefix}-TOKEN2.csv'\nvocab_csv = f'{data_in}/output/{data_prefix}-VOCAB2.csv'\nlib_csv = f'{data_in}/output/{data_prefix}-LIB_FIXED.csv'\nbow_csv = f'{data_in}/output/{data_prefix}-BOW.csv'\n\n\n# For TOKENS\nOHCO = ['book_id', 'chap_num', 'para_num', 'sent_num', 'token_num']\nBOOKS = OHCO[:1]\nCHAPS = OHCO[:2]\nPARAS = OHCO[:3]\nSENTS = OHCO[:4]\n\n\nsalex_csv = f'{data_in}/lexicons/salex_nrc.csv'\nemo_cols = \"anger anticipation disgust fear joy sadness surprise trust sentiment\".split()"
  },
  {
    "objectID": "lessons/M10_SentimentAnalysis/M10_04_AustenMelville.html#import",
    "href": "lessons/M10_SentimentAnalysis/M10_04_AustenMelville.html#import",
    "title": "Metadata",
    "section": "Import",
    "text": "Import\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport plotly_express as px\nfrom IPython.display import display, HTML\n\n\nsns.set()"
  },
  {
    "objectID": "lessons/M10_SentimentAnalysis/M10_05_WordShiftGraphs.html",
    "href": "lessons/M10_SentimentAnalysis/M10_05_WordShiftGraphs.html",
    "title": "Metadata",
    "section": "",
    "text": "Course:   DS 5001\nModule:   10 Lab\nTopic:    Word Shift Graphs\nAuthor:   R.C. Alvarado\nDate:     02 April 2023 (revised)\n\nSee https://ryanjgallagher.github.io/code/word_shift/overview and https://shifterator.readthedocs.io/en/latest/\nSee also SocialSent lexicon https://nlp.stanford.edu/projects/socialsent/\n\n\nSet Up\n\ndata_path = \"../data\"\ncorpus_prefix = 'austen-melville'\n\n\nimport pandas as pd\nimport shifterator as sh\n\n\n\nGet Data\n\nSALEX = pd.read_csv(f\"{data_path}/lexicons/salex_nrc.csv\").set_index('term_str')\n\n\n# SALEX\n\n\nBOW = pd.read_csv(f\"{data_path}/output/{corpus_prefix}-BOW.csv\").set_index(['book_id','chap_num','term_str'])\n\n\n# BOW\n\n\nLIB = pd.read_csv(f\"{data_path}/output/{corpus_prefix}-LIB_FIXED.csv\").set_index(['book_id'])\nLIB['author_id'] = LIB.author.str.split(', ').str[0]\nLIB['book_label'] = LIB.author_id + ' ' + LIB.index.astype('str') + ': ' + LIB.title.str[:20]\n\n\n# LIB\n\n\n\nCombine Tables\n\nAUTH = BOW.join(LIB.author_id, on='book_id').groupby(['author_id','term_str']).agg({'n':'sum', 'tfidf':'mean'}).join(SALEX.nrc_sentiment, on='term_str').dropna()\n\n\nAUTH['sent_weight'] = AUTH['n'] * AUTH['tfidf'] * AUTH['nrc_sentiment']\n\n\n# AUTH['sent_weight'] = AUTH['tfidf'] * AUTH['nrc_sentiment']\n\n\nAUTH\n\n\n\n\n\n  \n    \n      \n      \n      n\n      tfidf\n      nrc_sentiment\n      sent_weight\n    \n    \n      author_id\n      term_str\n      \n      \n      \n      \n    \n  \n  \n    \n      AUSTEN\n      abandoned\n      5\n      0.001951\n      -1.0\n      -0.009756\n    \n    \n      abhor\n      7\n      0.008680\n      -1.0\n      -0.060757\n    \n    \n      abhorrent\n      2\n      0.003221\n      -1.0\n      -0.006443\n    \n    \n      abolish\n      1\n      0.001692\n      -1.0\n      -0.001692\n    \n    \n      abominable\n      18\n      0.002996\n      -1.0\n      -0.053926\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      MELVILLE\n      young\n      676\n      0.001460\n      1.0\n      0.986864\n    \n    \n      youth\n      160\n      0.002918\n      1.0\n      0.466885\n    \n    \n      zeal\n      14\n      0.002857\n      1.0\n      0.039997\n    \n    \n      zealous\n      5\n      0.003010\n      1.0\n      0.015051\n    \n    \n      zest\n      4\n      0.005590\n      1.0\n      0.022361\n    \n  \n\n4866 rows × 4 columns\n\n\n\n\nAUTH.loc['AUSTEN']\n\n\n\n\n\n  \n    \n      \n      n\n      tfidf\n      nrc_sentiment\n      sent_weight\n    \n    \n      term_str\n      \n      \n      \n      \n    \n  \n  \n    \n      abandoned\n      5\n      0.001951\n      -1.0\n      -0.009756\n    \n    \n      abhor\n      7\n      0.008680\n      -1.0\n      -0.060757\n    \n    \n      abhorrent\n      2\n      0.003221\n      -1.0\n      -0.006443\n    \n    \n      abolish\n      1\n      0.001692\n      -1.0\n      -0.001692\n    \n    \n      abominable\n      18\n      0.002996\n      -1.0\n      -0.053926\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      yearning\n      2\n      0.002285\n      0.0\n      0.000000\n    \n    \n      young\n      827\n      0.001573\n      1.0\n      1.300636\n    \n    \n      youth\n      66\n      0.002152\n      1.0\n      0.142024\n    \n    \n      zeal\n      16\n      0.001781\n      1.0\n      0.028503\n    \n    \n      zealous\n      7\n      0.002441\n      1.0\n      0.017088\n    \n  \n\n1866 rows × 4 columns\n\n\n\n\n\nFrequency Shifts\n\nsh.JSDivergenceShift(AUTH.loc['AUSTEN', 'n'].to_dict(), \n                     AUTH.loc['MELVILLE', 'n'].to_dict()).get_shift_graph()\n\n/Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages/shifterator/plotting.py:604: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels(x_ticks, fontsize=plot_params[\"xtick_fontsize\"])\n\n\n\n\n\n<AxesSubplot:xlabel='Score shift $\\\\delta \\\\Phi_{\\\\tau}$ (%)', ylabel='Rank'>\n\n\n\n# sh.ProportionShift(v_austen, v_melville).get_shift_graph()\n\n\nsh.EntropyShift(AUTH.loc['AUSTEN', 'n'].to_dict(), AUTH.loc['MELVILLE', 'n'].to_dict()).get_shift_graph()\n\n/Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages/shifterator/plotting.py:604: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels(x_ticks, fontsize=plot_params[\"xtick_fontsize\"])\n\n\n\n\n\n<AxesSubplot:title={'center':'Text 1: $\\\\Phi_{avg}=$9.03\\nText 2: $\\\\Phi_{avg}=$10.03'}, xlabel='Score shift $\\\\delta \\\\Phi_{\\\\tau}$ (%)', ylabel='Rank'>\n\n\n\n# sh.Shift\n\n\nFREQ = AUTH.n.unstack().T.dropna()\n\n\nFREQ\n\n\n\n\n\n  \n    \n      author_id\n      AUSTEN\n      MELVILLE\n    \n    \n      term_str\n      \n      \n    \n  \n  \n    \n      abandoned\n      5.0\n      68.0\n    \n    \n      abhor\n      7.0\n      4.0\n    \n    \n      abhorrent\n      2.0\n      9.0\n    \n    \n      abolish\n      1.0\n      5.0\n    \n    \n      abominable\n      18.0\n      18.0\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      yearning\n      2.0\n      8.0\n    \n    \n      young\n      827.0\n      676.0\n    \n    \n      youth\n      66.0\n      160.0\n    \n    \n      zeal\n      16.0\n      14.0\n    \n    \n      zealous\n      7.0\n      5.0\n    \n  \n\n1790 rows × 2 columns\n\n\n\n\nSENT = AUTH.nrc_sentiment.unstack().T.dropna()\n\n\nSENT\n\n\n\n\n\n  \n    \n      author_id\n      AUSTEN\n      MELVILLE\n    \n    \n      term_str\n      \n      \n    \n  \n  \n    \n      abandoned\n      -1.0\n      -1.0\n    \n    \n      abhor\n      -1.0\n      -1.0\n    \n    \n      abhorrent\n      -1.0\n      -1.0\n    \n    \n      abolish\n      -1.0\n      -1.0\n    \n    \n      abominable\n      -1.0\n      -1.0\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      yearning\n      0.0\n      0.0\n    \n    \n      young\n      1.0\n      1.0\n    \n    \n      youth\n      1.0\n      1.0\n    \n    \n      zeal\n      1.0\n      1.0\n    \n    \n      zealous\n      1.0\n      1.0\n    \n  \n\n1790 rows × 2 columns\n\n\n\n\nSENTW = AUTH.sent_weight.unstack().T.dropna()\n\n\nSENTW\n\n\n\n\n\n  \n    \n      author_id\n      AUSTEN\n      MELVILLE\n    \n    \n      term_str\n      \n      \n    \n  \n  \n    \n      abandoned\n      -0.009756\n      -0.227707\n    \n    \n      abhor\n      -0.060757\n      -0.009790\n    \n    \n      abhorrent\n      -0.006443\n      -0.030214\n    \n    \n      abolish\n      -0.001692\n      -0.015345\n    \n    \n      abominable\n      -0.053926\n      -0.058639\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      yearning\n      0.000000\n      0.000000\n    \n    \n      young\n      1.300636\n      0.986864\n    \n    \n      youth\n      0.142024\n      0.466885\n    \n    \n      zeal\n      0.028503\n      0.039997\n    \n    \n      zealous\n      0.017088\n      0.015051\n    \n  \n\n1790 rows × 2 columns\n\n\n\n\nsh.Shift(\n    FREQ['AUSTEN'].to_dict(), \n    FREQ['MELVILLE'].to_dict(), \n    SENT['AUSTEN'].to_dict(), \n    SENT['MELVILLE'].to_dict()).get_shift_graph()\n\n/Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages/shifterator/plotting.py:604: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels(x_ticks, fontsize=plot_params[\"xtick_fontsize\"])\n\n\n\n\n\n<AxesSubplot:title={'center':'Text 1: $\\\\Phi_{avg}=$0.27\\nText 2: $\\\\Phi_{avg}=$0.09'}, xlabel='Score shift $\\\\delta \\\\Phi_{\\\\tau}$ (%)', ylabel='Rank'>\n\n\n\nsh.Shift(\n    FREQ['AUSTEN'].to_dict(), \n    FREQ['MELVILLE'].to_dict(), \n    SENTW['AUSTEN'].to_dict(), \n    SENTW['MELVILLE'].to_dict()).get_shift_graph()\n\n/Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages/shifterator/plotting.py:604: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels(x_ticks, fontsize=plot_params[\"xtick_fontsize\"])\n\n\n\n\n\n<AxesSubplot:title={'center':'Text 1: $\\\\Phi_{avg}=$0.29\\nText 2: $\\\\Phi_{avg}=$0.16'}, xlabel='Score shift $\\\\delta \\\\Phi_{\\\\tau}$ (%)', ylabel='Rank'>\n\n\nSee https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-021-00260-3\n\n\n\nimage.png\n\n\n\n\nTopics\n\nPHI = pd.read_csv(f\"{data_path}/output/{corpus_prefix}-LDA_PHI-40.csv\").set_index('topic_id').T\nPHI.index.name = 'term_str'\n\n\nPHI['TSUM'] = PHI.sum(1)\n\n\nPHIP = PHI / PHI.sum()\n\n\nPHIP.TSUM\n\nterm_str\nabbey         0.000104\nabhorrence    0.000104\nabilities     0.000120\naboard        0.000136\nabode         0.000180\n                ...   \nyoungster     0.000066\nyouth         0.000632\nyouths        0.000089\nzeal          0.000098\nzone          0.000060\nName: TSUM, Length: 4000, dtype: float64\n\n\n\ndef plot_shift(v1, v2):\n    sh.EntropyShift(PHIP[v1].to_dict(), PHIP[v2].to_dict()).get_shift_graph()\n\n\nplot_shift('T23', 'T05')\n\n/Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages/shifterator/plotting.py:604: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels(x_ticks, fontsize=plot_params[\"xtick_fontsize\"])"
  },
  {
    "objectID": "lessons/M10_SentimentAnalysis/M10_06_Doc2Vec.html",
    "href": "lessons/M10_SentimentAnalysis/M10_06_Doc2Vec.html",
    "title": "Metadata",
    "section": "",
    "text": "Course:   DS 5001\nModule:   10 Lab\nTopic:    Doc2Vec\nAuthor:   R.C. Alvarado\nDate:     02 April 2023 (revised)\nPurpose: Demonstrate use of Gensim’s doc2vec implementation.\nSee https://www.tutorialspoint.com/gensim/gensim_doc2vec_model.htm#\n\nDoc2Vec model, as opposite to Word2Vec model, is used to create a vectorised representation of a group of words taken collectively as a single unit. It doesn’t only give the simple average of the words in the sentence.\n\n\nSet Up\n\ndata_path = \"../data\"\ncorpus_prefix = 'austen-melville'\nOHCO = ['book_id','chap_id','para_num','sent_num','token_num']\nBAG = OHCO[:1] # BOOKS\n\n\nimport pandas as pd\nimport numpy as np\nimport gensim\nimport plotly_express as px\n\n\n\nGet Data\n\nLIB = pd.read_csv(f\"{data_path}/output/{corpus_prefix}-LIB.csv\").set_index(['book_id'])\nLIB['author_id'] = LIB.author.str.split(', ').str[0]\nLIB['book_label'] = LIB.author_id + ' ' + LIB.index.astype('str') + ': ' + LIB.title.str[:20]\n\n\nCORPUS = pd.read_csv(f\"{data_path}/output/{corpus_prefix}-CORPUS.csv\").set_index(OHCO)[['pos','term_str']]\n\n\nDOCS = CORPUS.groupby(BAG)\n\n\nDOCIDX = DOCS.term_str.count().index\n\n\n\nConvert to Gensim\nWe follow Gensim recipe for converting our data from a dataframe to a TaggedDocument.\n\ndata = DOCS.term_str.apply(lambda x: list(x)).to_list()\n\n\ndef tagged_document(list_of_list_of_words):\n    for i, list_of_words in enumerate(list_of_list_of_words):\n      yield gensim.models.doc2vec.TaggedDocument([str(w) for w in list_of_words], [i])\n\n\ndata_for_training = list(tagged_document(data))\n\n\n# data_for_training[:1]\n\n\n\nGenerate Model\n\nmodel = gensim.models.doc2vec.Doc2Vec(vector_size=40, min_count=2, epochs=30)\n\n\nmodel.build_vocab(data_for_training)\n\n\nmodel.train(data_for_training, total_examples=model.corpus_count, epochs=model.epochs)\n\n\n\nDocument Embedding Matrix\n\n# model.docvecs.vectors_docs\n\n\nX = pd.DataFrame(model.docvecs.vectors_docs, index=LIB.book_label)\n\n\nimport sys\nsys.path.append(\"../lib\")\nfrom hac2 import HAC\n\n\ndv_tree = HAC(X)\ndv_tree.color_thresh = 1\ndv_tree.plot()\n\n<Figure size 640x480 with 0 Axes>\n\n\n\n\n\n\n\nTry Out\n\nr1 = model.infer_vector(\"We went sailing on the Pacific\".split())\nr2 = model.infer_vector(\"I so enjoyed the visit to Bath\".split())\n\n\nR = pd.DataFrame(dict(r1=r1, r2=r2))\n\n\nR.style.background_gradient(cmap='YlGnBu', axis=None)\n\n\n\n\n  \n    \n       \n      r1\n      r2\n    \n  \n  \n    \n      0\n      -0.032868\n      0.076063\n    \n    \n      1\n      0.041601\n      0.213247\n    \n    \n      2\n      0.017655\n      -0.055319\n    \n    \n      3\n      -0.117780\n      0.070053\n    \n    \n      4\n      -0.075479\n      -0.226251\n    \n    \n      5\n      -0.299832\n      -0.171193\n    \n    \n      6\n      -0.013452\n      0.213345\n    \n    \n      7\n      -0.012558\n      -0.374095\n    \n    \n      8\n      0.165766\n      -0.087109\n    \n    \n      9\n      0.105235\n      -0.095683\n    \n    \n      10\n      -0.072005\n      -0.008264\n    \n    \n      11\n      -0.086814\n      -0.054225\n    \n    \n      12\n      -0.156098\n      -0.223550\n    \n    \n      13\n      0.081251\n      -0.139860\n    \n    \n      14\n      -0.127037\n      -0.244419\n    \n    \n      15\n      -0.024148\n      0.116694\n    \n    \n      16\n      -0.066091\n      -0.053389\n    \n    \n      17\n      0.009017\n      -0.224755\n    \n    \n      18\n      0.255091\n      0.148901\n    \n    \n      19\n      0.027833\n      0.160416\n    \n    \n      20\n      -0.043916\n      -0.074221\n    \n    \n      21\n      0.176864\n      0.135345\n    \n    \n      22\n      -0.174742\n      -0.028071\n    \n    \n      23\n      0.126574\n      0.083239\n    \n    \n      24\n      0.143013\n      -0.067028\n    \n    \n      25\n      0.132646\n      0.215656\n    \n    \n      26\n      -0.167932\n      0.151429\n    \n    \n      27\n      -0.053533\n      0.047435\n    \n    \n      28\n      0.150017\n      0.139019\n    \n    \n      29\n      0.199399\n      0.248889\n    \n    \n      30\n      -0.119882\n      -0.046807\n    \n    \n      31\n      -0.183622\n      -0.071927\n    \n    \n      32\n      0.070848\n      -0.033891\n    \n    \n      33\n      0.006979\n      -0.153109\n    \n    \n      34\n      -0.021294\n      0.181731\n    \n    \n      35\n      -0.272228\n      -0.239877\n    \n    \n      36\n      -0.110786\n      -0.145837\n    \n    \n      37\n      -0.010292\n      -0.174637\n    \n    \n      38\n      -0.157765\n      -0.246765\n    \n    \n      39\n      0.080361\n      -0.133748\n    \n  \n\n\n\n\nR['w'] = X.sum().abs()\n\n\nR\n\n\n\n\n\n  \n    \n      \n      r1\n      r2\n      w\n    \n  \n  \n    \n      0\n      -0.032868\n      0.076063\n      55.321709\n    \n    \n      1\n      0.041601\n      0.213247\n      25.288633\n    \n    \n      2\n      0.017655\n      -0.055319\n      15.468231\n    \n    \n      3\n      -0.117780\n      0.070053\n      13.345350\n    \n    \n      4\n      -0.075479\n      -0.226251\n      57.481136\n    \n    \n      5\n      -0.299832\n      -0.171193\n      96.691811\n    \n    \n      6\n      -0.013452\n      0.213345\n      68.565933\n    \n    \n      7\n      -0.012558\n      -0.374095\n      0.304313\n    \n    \n      8\n      0.165766\n      -0.087109\n      62.150070\n    \n    \n      9\n      0.105235\n      -0.095683\n      54.337376\n    \n    \n      10\n      -0.072005\n      -0.008264\n      50.089378\n    \n    \n      11\n      -0.086814\n      -0.054225\n      53.316437\n    \n    \n      12\n      -0.156098\n      -0.223550\n      43.875233\n    \n    \n      13\n      0.081251\n      -0.139860\n      33.254524\n    \n    \n      14\n      -0.127037\n      -0.244419\n      54.821121\n    \n    \n      15\n      -0.024148\n      0.116694\n      90.979797\n    \n    \n      16\n      -0.066091\n      -0.053389\n      61.539963\n    \n    \n      17\n      0.009017\n      -0.224755\n      2.861997\n    \n    \n      18\n      0.255091\n      0.148901\n      131.855865\n    \n    \n      19\n      0.027833\n      0.160416\n      38.517960\n    \n    \n      20\n      -0.043916\n      -0.074221\n      59.931046\n    \n    \n      21\n      0.176864\n      0.135345\n      32.058952\n    \n    \n      22\n      -0.174742\n      -0.028071\n      21.100595\n    \n    \n      23\n      0.126574\n      0.083239\n      81.068512\n    \n    \n      24\n      0.143013\n      -0.067028\n      91.883469\n    \n    \n      25\n      0.132646\n      0.215656\n      129.246490\n    \n    \n      26\n      -0.167932\n      0.151429\n      37.852936\n    \n    \n      27\n      -0.053533\n      0.047435\n      25.073095\n    \n    \n      28\n      0.150017\n      0.139019\n      86.308517\n    \n    \n      29\n      0.199399\n      0.248889\n      110.946449\n    \n    \n      30\n      -0.119882\n      -0.046807\n      36.188591\n    \n    \n      31\n      -0.183622\n      -0.071927\n      63.364124\n    \n    \n      32\n      0.070848\n      -0.033891\n      28.492270\n    \n    \n      33\n      0.006979\n      -0.153109\n      12.222077\n    \n    \n      34\n      -0.021294\n      0.181731\n      17.044861\n    \n    \n      35\n      -0.272228\n      -0.239877\n      82.446907\n    \n    \n      36\n      -0.110786\n      -0.145837\n      8.333665\n    \n    \n      37\n      -0.010292\n      -0.174637\n      34.196110\n    \n    \n      38\n      -0.157765\n      -0.246765\n      64.251175\n    \n    \n      39\n      0.080361\n      -0.133748\n      110.856430\n    \n  \n\n\n\n\n\n# X.sum().to_list()\n\n\npx.scatter(R.reset_index(), 'r1', 'r2', height=600, width=700, text='index', size='w')\n\n\n                                                \n\n\n\n(R.r1 - R.r2).sort_values().plot.barh(figsize=(5,10));\n\n\n\n\n\nmode\n\nNameError: name 'mode' is not defined"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_01_ExportBooksWithSentiment.html",
    "href": "lessons/M11_Narrative/M11_01_ExportBooksWithSentiment.html",
    "title": "Metadata",
    "section": "",
    "text": "Set Up\nSave as sentences for R package."
  },
  {
    "objectID": "lessons/M11_Narrative/M11_01_ExportBooksWithSentiment.html#config",
    "href": "lessons/M11_Narrative/M11_01_ExportBooksWithSentiment.html#config",
    "title": "Metadata",
    "section": "Config",
    "text": "Config\n\ndata_path = '../data'\nout_dir = f\"{data_path}/syuzhet\"\n\n\nconfig = {\n    'novels': {\n        'OHCO': 'book_id chap_id para_num sent_num token_num'.split(),\n        'LIB': 'LIB',\n        'TOKENS': 'CORPUS',\n        'data_dir': f\"{data_path}/novels\"\n    },\n    'austen-melville': {\n        'OHCO': 'book_id chap_num para_num sent_num token_num'.split(),\n        'LIB': 'LIB',\n        'TOKENS': 'TOKEN2',\n        'data_dir': f\"{data_path}/output\"\n    }\n}\n\n\ntoken_cols = ['pos','term_str']\nsalex_csv = f'{data_path}/lexicons/salex_combo.csv'"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_01_ExportBooksWithSentiment.html#import",
    "href": "lessons/M11_Narrative/M11_01_ExportBooksWithSentiment.html#import",
    "title": "Metadata",
    "section": "Import",
    "text": "Import\n\nimport pandas as pd"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_01_ExportBooksWithSentiment.html#get-lexicons",
    "href": "lessons/M11_Narrative/M11_01_ExportBooksWithSentiment.html#get-lexicons",
    "title": "Metadata",
    "section": "Get Lexicons",
    "text": "Get Lexicons\nWe created this last week.\n\nSALEX = pd.read_csv(salex_csv).set_index('term_str')\nSALEX['nrc_polarity'] = SALEX.nrc_positive - SALEX.nrc_negative\n\n\nSALEX.columns.tolist()\n\n['nrc_anger',\n 'nrc_anticipation',\n 'nrc_disgust',\n 'nrc_fear',\n 'nrc_joy',\n 'nrc_negative',\n 'nrc_positive',\n 'nrc_sadness',\n 'nrc_surprise',\n 'nrc_trust',\n 'nrc_sentiment',\n 'bing_negative',\n 'bing_positive',\n 'bing_sentiment',\n 'syu_sentiment',\n 'gi_sentiment',\n 'nrc_polarity']"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_01_ExportBooksWithSentiment.html#get-texts",
    "href": "lessons/M11_Narrative/M11_01_ExportBooksWithSentiment.html#get-texts",
    "title": "Metadata",
    "section": "Get Texts",
    "text": "Get Texts\nWe import two sets of pre-processed novels and combine them.\n\nTOKENS = {} # Dict of dataframes\nLIB = {} # Dict of dataframes\nfor prefix in config:\n    print(prefix)\n    \n    token_file = f\"{config[prefix]['data_dir']}/{prefix}-{config[prefix]['TOKENS']}.csv\"\n    TOKENS[prefix] = pd.read_csv(token_file).set_index(config[prefix]['OHCO'])[token_cols]    \n    \n    lib_file = f\"{config[prefix]['data_dir']}/{prefix}-{config[prefix]['LIB']}.csv\"\n    LIB[prefix] = pd.read_csv(lib_file)\n\nnovels\nausten-melville"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_01_ExportBooksWithSentiment.html#standardize-tokens-tables",
    "href": "lessons/M11_Narrative/M11_01_ExportBooksWithSentiment.html#standardize-tokens-tables",
    "title": "Metadata",
    "section": "Standardize TOKENS tables",
    "text": "Standardize TOKENS tables\nWe make the OHCO columns names the same (since one has chap_num and the other chap_id).\n\nTOKENS['novels'].index.names = config['austen-melville']['OHCO']\n\n\nTOKENS['novels']\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      \n      pos\n      term_str\n    \n    \n      book_id\n      chap_num\n      para_num\n      sent_num\n      token_num\n      \n      \n    \n  \n  \n    \n      secretadversary\n      1\n      0\n      1\n      0\n      DT\n      the\n    \n    \n      1\n      NNP\n      young\n    \n    \n      2\n      NNP\n      adventurers\n    \n    \n      3\n      NNP\n      ltd\n    \n    \n      1\n      0\n      0\n      JJ\n      tommy\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      baskervilles\n      11\n      114\n      1\n      7\n      RBR\n      more\n    \n    \n      8\n      JJ\n      comfortable\n    \n    \n      9\n      IN\n      outside\n    \n    \n      10\n      IN\n      than\n    \n    \n      11\n      NN\n      in\n    \n  \n\n1500417 rows × 2 columns"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_01_ExportBooksWithSentiment.html#standardize-lib-tables",
    "href": "lessons/M11_Narrative/M11_01_ExportBooksWithSentiment.html#standardize-lib-tables",
    "title": "Metadata",
    "section": "Standardize LIB tables",
    "text": "Standardize LIB tables\n\nLIB['austen-melville']['author_id']  = LIB['austen-melville'].author.str.split(', ').str[0].str.lower()\n\n\nLIB['novels']['title'] = LIB['novels']['book_id']\nLIB['novels'] = LIB['novels'].set_index('book_id')\n\n\nLIB['novels'] = LIB['novels'][['author_id', 'title']]\nLIB['novels']['corpus'] = 'novels' \n\n\nLIB['novels']\n\n\n\n\n\n  \n    \n      \n      author_id\n      title\n      corpus\n    \n    \n      book_id\n      \n      \n      \n    \n  \n  \n    \n      secretadversary\n      christie\n      secretadversary\n      novels\n    \n    \n      styles\n      christie\n      styles\n      novels\n    \n    \n      moonstone\n      collins\n      moonstone\n      novels\n    \n    \n      adventures\n      doyle\n      adventures\n      novels\n    \n    \n      baskervilles\n      doyle\n      baskervilles\n      novels\n    \n    \n      scarlet\n      doyle\n      scarlet\n      novels\n    \n    \n      signoffour\n      doyle\n      signoffour\n      novels\n    \n    \n      marieroget\n      poe\n      marieroget\n      novels\n    \n    \n      ruemorgue\n      poe\n      ruemorgue\n      novels\n    \n    \n      northangerabbey\n      austen\n      northangerabbey\n      novels\n    \n    \n      christmascarole\n      dickens\n      christmascarole\n      novels\n    \n    \n      monk\n      lewis\n      monk\n      novels\n    \n    \n      pitandpendulum\n      poe\n      pitandpendulum\n      novels\n    \n    \n      reddeath\n      poe\n      reddeath\n      novels\n    \n    \n      usher\n      poe\n      usher\n      novels\n    \n    \n      udolpho\n      radcliffe\n      udolpho\n      novels\n    \n    \n      oldenglishbaron\n      reeve\n      oldenglishbaron\n      novels\n    \n    \n      frankenstein\n      shelley\n      frankenstein\n      novels\n    \n    \n      dracula\n      stoker\n      dracula\n      novels\n    \n    \n      castleofotranto\n      walpole\n      castleofotranto\n      novels\n    \n  \n\n\n\n\n\nLIB['austen-melville'] = LIB['austen-melville'].set_index('book_id')\nLIB['austen-melville'] = LIB['austen-melville'][['author_id', 'title']]\nLIB['austen-melville']['corpus'] = 'austen-melville'\n\n\nLIB['austen-melville']\n\n\n\n\n\n  \n    \n      \n      author_id\n      title\n      corpus\n    \n    \n      book_id\n      \n      \n      \n    \n  \n  \n    \n      105\n      austen\n      PERSUASION\n      austen-melville\n    \n    \n      121\n      austen\n      NORTHANGER ABBEY\n      austen-melville\n    \n    \n      141\n      austen\n      MANSFIELD PARK\n      austen-melville\n    \n    \n      158\n      austen\n      EMMA\n      austen-melville\n    \n    \n      161\n      austen\n      SENSE AND SENSIBILITY\n      austen-melville\n    \n    \n      946\n      austen\n      LADY SUSAN\n      austen-melville\n    \n    \n      1212\n      austen\n      LOVE AND FREINDSHIP SIC\n      austen-melville\n    \n    \n      1342\n      austen\n      PRIDE AND PREJUDICE\n      austen-melville\n    \n    \n      1900\n      melville\n      TYPEE A ROMANCE OF THE SOUTH SEAS\n      austen-melville\n    \n    \n      2701\n      melville\n      MOBY DICK OR THE WHALE\n      austen-melville\n    \n    \n      4045\n      melville\n      OMOO ADVENTURES IN THE SOUTH SEAS\n      austen-melville\n    \n    \n      8118\n      melville\n      REDBURN HIS FIRST VOYAGE BEING THE SAILOR BOY ...\n      austen-melville\n    \n    \n      10712\n      melville\n      WHITE JACKET OR THE WORLD ON A MAN OF WAR\n      austen-melville\n    \n    \n      13720\n      melville\n      MARDI AND A VOYAGE THITHER VOL I\n      austen-melville\n    \n    \n      13721\n      melville\n      MARDI AND A VOYAGE THITHER VOL II\n      austen-melville\n    \n    \n      15422\n      melville\n      ISRAEL POTTER HIS FIFTY YEARS OF EXILE\n      austen-melville\n    \n    \n      15859\n      melville\n      THE PIAZZA TALES\n      austen-melville\n    \n    \n      21816\n      melville\n      THE CONFIDENCE MAN HIS MASQUERADE\n      austen-melville\n    \n    \n      34970\n      melville\n      PIERRE OR THE AMBIGUITIES\n      austen-melville"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_01_ExportBooksWithSentiment.html#combine-libs",
    "href": "lessons/M11_Narrative/M11_01_ExportBooksWithSentiment.html#combine-libs",
    "title": "Metadata",
    "section": "Combine LIBs",
    "text": "Combine LIBs\n\nLIB_ALL = pd.concat(LIB.values())\n\n\n# LIB_ALL = pd.concat([LIB[prefix] for prefix in config])\n\n\nLIB_ALL.sample(10)\n\n\n\n\n\n  \n    \n      \n      author_id\n      title\n      corpus\n    \n    \n      book_id\n      \n      \n      \n    \n  \n  \n    \n      10712\n      melville\n      WHITE JACKET OR THE WORLD ON A MAN OF WAR\n      austen-melville\n    \n    \n      21816\n      melville\n      THE CONFIDENCE MAN HIS MASQUERADE\n      austen-melville\n    \n    \n      secretadversary\n      christie\n      secretadversary\n      novels\n    \n    \n      121\n      austen\n      NORTHANGER ABBEY\n      austen-melville\n    \n    \n      105\n      austen\n      PERSUASION\n      austen-melville\n    \n    \n      34970\n      melville\n      PIERRE OR THE AMBIGUITIES\n      austen-melville\n    \n    \n      1212\n      austen\n      LOVE AND FREINDSHIP SIC\n      austen-melville\n    \n    \n      8118\n      melville\n      REDBURN HIS FIRST VOYAGE BEING THE SAILOR BOY ...\n      austen-melville\n    \n    \n      reddeath\n      poe\n      reddeath\n      novels\n    \n    \n      udolpho\n      radcliffe\n      udolpho\n      novels"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_01_ExportBooksWithSentiment.html#combine-tokens",
    "href": "lessons/M11_Narrative/M11_01_ExportBooksWithSentiment.html#combine-tokens",
    "title": "Metadata",
    "section": "Combine TOKENS",
    "text": "Combine TOKENS\n\nTOKENS_ALL = pd.concat(TOKENS.values())\n\n\n# TOKENS_ALL = pd.concat([TOKENS[prefix] for prefix in config])\n\n\nTOKENS_ALL\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      \n      pos\n      term_str\n    \n    \n      book_id\n      chap_num\n      para_num\n      sent_num\n      token_num\n      \n      \n    \n  \n  \n    \n      secretadversary\n      1\n      0\n      1\n      0\n      DT\n      the\n    \n    \n      1\n      NNP\n      young\n    \n    \n      2\n      NNP\n      adventurers\n    \n    \n      3\n      NNP\n      ltd\n    \n    \n      1\n      0\n      0\n      JJ\n      tommy\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      10712\n      92\n      23\n      0\n      7\n      DT\n      a\n    \n    \n      8\n      NN\n      voyage\n    \n    \n      9\n      NN\n      thats\n    \n    \n      10\n      SYM\n      homeward\n    \n    \n      12\n      NN\n      bound\n    \n  \n\n3570672 rows × 2 columns"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_02_SyuzhetPlots.html",
    "href": "lessons/M11_Narrative/M11_02_SyuzhetPlots.html",
    "title": "Metadata",
    "section": "",
    "text": "Set Up\nGrab a subset of books to explore.\nJockers’ original FFT function params\nFast Fourier Transform. See https://rdrr.io/cran/syuzhet/man/get_transformed_values.html"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_02_SyuzhetPlots.html#config",
    "href": "lessons/M11_Narrative/M11_02_SyuzhetPlots.html#config",
    "title": "Metadata",
    "section": "Config",
    "text": "Config\n\ndata_dir = \"../data\"\n\n\nlib_file = f'{data_dir}/syuzhet/combo-LIB.csv' # Also created in previous notebook"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_02_SyuzhetPlots.html#import",
    "href": "lessons/M11_Narrative/M11_02_SyuzhetPlots.html#import",
    "title": "Metadata",
    "section": "Import",
    "text": "Import\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp\nimport scipy.fft as fft\n\n\nsns.set()"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_02_SyuzhetPlots.html#persuasion",
    "href": "lessons/M11_Narrative/M11_02_SyuzhetPlots.html#persuasion",
    "title": "Metadata",
    "section": "Persuasion",
    "text": "Persuasion\n\npg105 = SyuzhetBook(105)\n\n105 PERSUASION SENTS nrc\n\n\n\npg105.plot_raw()\n\n\n\n\n\npg105.plot_smooth('FFT', low_pass_size=3)\n\n\n\n\n\npg105.plot_smooth()\n\n\n\n\n\npg105.plot_rolling()"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_02_SyuzhetPlots.html#persuasion-by-paras",
    "href": "lessons/M11_Narrative/M11_02_SyuzhetPlots.html#persuasion-by-paras",
    "title": "Metadata",
    "section": "Persuasion by PARAS",
    "text": "Persuasion by PARAS\n\npg105_paras = SyuzhetBook(105, bag='PARAS')\n\n105 PERSUASION SENTS nrc\n\n\n\npg105_paras.plot_smooth()"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_02_SyuzhetPlots.html#moby-dick",
    "href": "lessons/M11_Narrative/M11_02_SyuzhetPlots.html#moby-dick",
    "title": "Metadata",
    "section": "Moby Dick",
    "text": "Moby Dick\n\npg2701 = SyuzhetBook(2701)\n\n2701 MOBY DICK OR THE WHALE SENTS nrc\n\n\n\npg2701.plot_raw()\n\n\n\n\n\npg2701.plot_smooth('FFT', low_pass_size=3)\n\n\n\n\n\npg2701.plot_smooth()\n\n\n\n\n\npg2701.plot_rolling()"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_02_SyuzhetPlots.html#frankenstein",
    "href": "lessons/M11_Narrative/M11_02_SyuzhetPlots.html#frankenstein",
    "title": "Metadata",
    "section": "Frankenstein",
    "text": "Frankenstein\n\nfrankenstein = SyuzhetBook('frankenstein')\n\nfrankenstein frankenstein SENTS nrc\n\n\n\nfrankenstein.plot_raw()\n\n\n\n\n\nfrankenstein.plot_smooth('FFT', low_pass_size=3)\n\n\n\n\n\nfrankenstein.plot_smooth()\n\n\n\n\n\nfrankenstein.plot_rolling()"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_02_SyuzhetPlots.html#dracula",
    "href": "lessons/M11_Narrative/M11_02_SyuzhetPlots.html#dracula",
    "title": "Metadata",
    "section": "Dracula",
    "text": "Dracula\n\ndracula = SyuzhetBook('dracula')\n\ndracula dracula SENTS nrc\n\n\n\ndracula.plot_raw()\n\n\n\n\n\ndracula.plot_smooth('FFT', low_pass_size=3)\n\n\n\n\n\ndracula.plot_smooth()\n\n\n\n\n\ndracula.plot_rolling()"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_02_SyuzhetPlots.html#pride-and-prejudice",
    "href": "lessons/M11_Narrative/M11_02_SyuzhetPlots.html#pride-and-prejudice",
    "title": "Metadata",
    "section": "Pride and Prejudice",
    "text": "Pride and Prejudice\n\npg1342 = SyuzhetBook(1342)\n\n1342 PRIDE AND PREJUDICE SENTS nrc\n\n\n\npg1342.plot_raw()\n\n\n\n\n\npg1342.plot_rolling()\n\n\n\n\n\npg1342.plot_smooth()\n\n\n\n\n\npg1342.plot_smooth('FFT', low_pass_size=3)"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_02_SyuzhetPlots.html#northanger-abbey",
    "href": "lessons/M11_Narrative/M11_02_SyuzhetPlots.html#northanger-abbey",
    "title": "Metadata",
    "section": "Northanger Abbey",
    "text": "Northanger Abbey\n\nnorthangerabbey = SyuzhetBook('northangerabbey')\n\nnorthangerabbey northangerabbey SENTS nrc\n\n\n\nnorthangerabbey.plot_raw()\n\n\n\n\n\nnorthangerabbey.plot_smooth('FFT', low_pass_size=3)\n\n\n\n\n\nnorthangerabbey.plot_smooth()\n\n\n\n\n\nnorthangerabbey.plot_rolling()\n\n\n\n\nFederalist: > In the end, the two are wed. In the final pages of “Northanger Abbey,” Austen writes that the couple was brought together through mutual affection and that Henry Tinley’s affection for Catherine is rooted in gratitude for her feelings for him. She’s implying that the typically overwrought and dramatic type of romance on display in most fictitious works does not hold a candle to real love that is rooted in a commitment to one’s husband or wife. Happy endings and “perfect happiness” are often brought about through ordinary and conventional means — means that ought not to be dismissed because they are less sexy than the conventions used in a Gothic novel.\nhttps://thefederalist.com/2018/10/09/jane-austen-pulled-off-feat-genius-northanger-abbey/\nSchmoop:\n\nNorthanger Abbey has a very neat and tidy and rather clichéd ending: all the nice main characters get married to other nice characters and live happily ever after. All the mean characters end up alone. It’s like a Disney movie.\n\nAside from being rather predictable, this ending is also a little weird. What’s odd about this ending is that the entire novel spends a lot of time undermining the various clichés of popular Gothic novels. But it ends with a series of giant clichés, including a deus ex machina, which is a fancy Latin term meaning a convenient plot element that is dropped in out of nowhere. The deus ex machina here is that one of the nice and long suffering characters happens to marry a wealthy Viscount who has never before been mentioned. It’s completely random. And the narrator even admits that this is pretty weird and random.\n\nThe novel’s ending isn’t just clichéd, it’s totally over-the-top clichéd. So, rather than being out of step with the rest of the book, the ending actually bumps the satire up a notch and goes for broke. Like the rest of the book, the ending is still highly satirical – it exaggerates and mocks the types of sentimental and even ludicrous endings often found in Gothic novels. The ending just accomplishes the satire in a different way than the rest of the book, which relies more on clever dialogue and humorously disrupted expectations. Instead of letting the characters supply the humor through their words and actions, the ending utilizes outrageous plot devices and ironic narrative commentary. Still, why Jane Austen decided to go with an over-the-top ending instead of a more subtle ending is debatable.\nhttps://www.shmoop.com/study-guides/literature/northanger-abbey/analysis/ending"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_03_Syuzhet_R.html",
    "href": "lessons/M11_Narrative/M11_03_Syuzhet_R.html",
    "title": "Metadata",
    "section": "",
    "text": "Set Up"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_03_Syuzhet_R.html#configuration",
    "href": "lessons/M11_Narrative/M11_03_Syuzhet_R.html#configuration",
    "title": "Metadata",
    "section": "Configuration",
    "text": "Configuration\n\n# methods = c(\"syuzhet\", \"afinn\", \"bing\", \"nrc\", \"stanford\")\n# Try bing or syuzhet\nmethod = 'nrc'\nfft_low_pass = 3\ndct_low_pass = 6"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_03_Syuzhet_R.html#libraries",
    "href": "lessons/M11_Narrative/M11_03_Syuzhet_R.html#libraries",
    "title": "Metadata",
    "section": "Libraries",
    "text": "Libraries\n\nlibrary(syuzhet)\nlibrary(repr)\n\n\noptions(repr.plot.width=15, repr.plot.height=6)"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_03_Syuzhet_R.html#persuasion",
    "href": "lessons/M11_Narrative/M11_03_Syuzhet_R.html#persuasion",
    "title": "Metadata",
    "section": "Persuasion",
    "text": "Persuasion\n\nplotitall('..//data//syuzhet//R//persuasion.txt', 'Persuasion')\n\nWarning message in get_transformed_values(book.sentiment_vector, low_pass_size = fft_low_pass, :\n“This function is maintained for legacy purposes.  Consider using get_dct_transform() instead.”"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_03_Syuzhet_R.html#moby-dick",
    "href": "lessons/M11_Narrative/M11_03_Syuzhet_R.html#moby-dick",
    "title": "Metadata",
    "section": "Moby Dick",
    "text": "Moby Dick\n\nplotitall('..//data//syuzhet//R//moby.txt', 'Moby Dick')\n\nWarning message in get_transformed_values(book.sentiment_vector, low_pass_size = fft_low_pass, :\n“This function is maintained for legacy purposes.  Consider using get_dct_transform() instead.”"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_03_Syuzhet_R.html#frankenstein",
    "href": "lessons/M11_Narrative/M11_03_Syuzhet_R.html#frankenstein",
    "title": "Metadata",
    "section": "Frankenstein",
    "text": "Frankenstein\n\nplotitall('..//data//syuzhet//R//frankenstein.txt', \"Frankenstein\")\n\nWarning message in get_transformed_values(book.sentiment_vector, low_pass_size = fft_low_pass, :\n“This function is maintained for legacy purposes.  Consider using get_dct_transform() instead.”"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_03_Syuzhet_R.html#dracula",
    "href": "lessons/M11_Narrative/M11_03_Syuzhet_R.html#dracula",
    "title": "Metadata",
    "section": "Dracula",
    "text": "Dracula\n\nplotitall('..//data//syuzhet//R//dracula.txt', 'Dracula')\n\nWarning message in get_transformed_values(book.sentiment_vector, low_pass_size = fft_low_pass, :\n“This function is maintained for legacy purposes.  Consider using get_dct_transform() instead.”\n\n\n\n\n\n\n\n\n\n\n\n\nplotitall('..//data//syuzhet//R//greatexpectations.txt', 'Great Expectations')\n\nWarning message in readLines(path_to_file):\n“incomplete final line found on '..//data//syuzhet//R//greatexpectations.txt'”\nWarning message in get_transformed_values(book.sentiment_vector, low_pass_size = fft_low_pass, :\n“This function is maintained for legacy purposes.  Consider using get_dct_transform() instead.”"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_04_Syuzhet_RGeneral.html",
    "href": "lessons/M11_Narrative/M11_04_Syuzhet_RGeneral.html",
    "title": "Metadata",
    "section": "",
    "text": "Set Up\nSee if you can get the sentences for a given text."
  },
  {
    "objectID": "lessons/M11_Narrative/M11_04_Syuzhet_RGeneral.html#configuration",
    "href": "lessons/M11_Narrative/M11_04_Syuzhet_RGeneral.html#configuration",
    "title": "Metadata",
    "section": "Configuration",
    "text": "Configuration\n\nmethod = 'nrc' # methods = \"syuzhet\", \"afinn\", \"bing\", \"nrc\", \"stanford\"\nfft_low_pass = 3\ndct_low_pass = 6"
  },
  {
    "objectID": "lessons/M11_Narrative/M11_04_Syuzhet_RGeneral.html#libraries",
    "href": "lessons/M11_Narrative/M11_04_Syuzhet_RGeneral.html#libraries",
    "title": "Metadata",
    "section": "Libraries",
    "text": "Libraries\n\nlibrary(syuzhet)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.5     ✔ purrr   1.0.1\n✔ tibble  3.2.1     ✔ dplyr   1.1.1\n✔ tidyr   1.2.1     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\noptions(repr.plot.width=15, repr.plot.height=6)"
  },
  {
    "objectID": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html",
    "href": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html",
    "title": "Metadata",
    "section": "",
    "text": "Overview\nThe prediction of the document’s (\\(d\\)) label or class \\(c\\) can be viewed as problem of conditional probability: * \\(p(c|d)\\) ? * \\(p(c|d) = \\large\\frac{p(c)p(d|c)}{p(d)}\\) * \\(p(c|d) = p(c)p(d|c)\\)\nWe use Maximum A Posteriori estimation (MAP) to predict the label: * \\(p(c|d) = \\underset{C}{\\mathrm{argmax}} p(c)p(d|c)\\)\nTo solve this, we need to estimate the values of the priors \\(p(c)\\) and the likelihoods \\(p(d|c)\\). * Prior: \\(p(c)\\) the independent frequency of a given category. * Likelihood: \\(p(d|c)\\) the frequency of the document for a given category.\nThe likelihoods are essentially unigram language models for each label. They are similar to topics in this sense.\nNote that \\(d\\) is a sequence of words \\(w_{1}^{N}\\), which means we apply the independence assumption to avoid the chain rule: * $p(d|c) = p(w_{1}^{N}|c) = p(w_{1}|c)p(w_{2},w_{1}|c) … $ * \\(p(d|c) = \\prod{p(w_{i}|c}) = \\sum{log(p(w_{i}|c))}\\)\nSo, we get: * \\(p(c|d) = \\underset{C}{\\mathrm{argmax}} p(c)\\sum{log(p(w_{i}|c))}\\)\nWe can estimate the priors and the likelihoods using Maximum Likelihood Estimation (MLE) from data that gives\nthe joint distribution of documents and labels, where documents are represented as bags of words.\nThis joint distribution is just the TOKEN table with the label from the LIB table joined to it. * \\((d,w,c)\\)"
  },
  {
    "objectID": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#configure",
    "href": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#configure",
    "title": "Metadata",
    "section": "Configure",
    "text": "Configure\n\ndata_in = '../data'\ndata_out = '../data'\nprefix = 'winereviews'\n\nSet some parameters\nWe create an empty static class to store our parameters.\n\nclass Params:\n    qntile_P = .9\n    qntile_N = .1\n    n_sets = 4 # We want 4 so we can use 3 for training, 1 for testing.\n    smooth_alpha = .1\n    prior_method = 'docs' # 'tokens' or 'docs'"
  },
  {
    "objectID": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#import",
    "href": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#import",
    "title": "Metadata",
    "section": "Import",
    "text": "Import\n\nimport pandas as pd\nimport numpy as np\nfrom numpy import log2 as log\nfrom numpy import exp2 as exp\nfrom numpy.random import randint\nimport lib.textman as tx"
  },
  {
    "objectID": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#import-wine-reviews",
    "href": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#import-wine-reviews",
    "title": "Metadata",
    "section": "Import wine reviews",
    "text": "Import wine reviews\n\ndocs = pd.read_csv(f'{data_in}/{prefix}/winereviews.csv', index_col='doc_id')\n\n\ndocs.head()\n\n\n\n\n\n  \n    \n      \n      doc_content\n      points\n    \n    \n      doc_id\n      \n      \n    \n  \n  \n    \n      0\n      Aromas include tropical fruit, broom, brimston...\n      87\n    \n    \n      1\n      This is ripe and fruity, a wine that is smooth...\n      87\n    \n    \n      2\n      Tart and snappy, the flavors of lime flesh and...\n      87\n    \n    \n      3\n      Pineapple rind, lemon pith and orange blossom ...\n      87\n    \n    \n      4\n      Much like the regular bottling from 2012, this...\n      87"
  },
  {
    "objectID": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#convert-points-to-labels",
    "href": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#convert-points-to-labels",
    "title": "Metadata",
    "section": "Convert points to labels",
    "text": "Convert points to labels\nKeep only reviews with high and low ratings\n\nbound_P = int(docs['points'].quantile(Params.qntile_P))\nbound_N = int(docs['points'].quantile(Params.qntile_N))\ndocs = docs[(docs.points <= bound_N) | (docs.points >= bound_P)]\n\n\nbound_N, bound_P\n\n(84, 93)\n\n\n\ndocs.sample(5)\n\n\n\n\n\n  \n    \n      \n      doc_content\n      points\n    \n    \n      doc_id\n      \n      \n    \n  \n  \n    \n      124893\n      Bold, ripe fruit flavors and full body give oo...\n      93\n    \n    \n      88048\n      White Merlot? Why not? It's actually a deep ro...\n      84\n    \n    \n      127183\n      Simple pear and apple work the largely nondesc...\n      83\n    \n    \n      125293\n      This impressive Brunello Riserva opens with an...\n      94\n    \n    \n      127166\n      This tastes sweet and simple, with flavors of ...\n      82\n    \n  \n\n\n\n\nAssign labels for high and low\n\ndocs.loc[docs['points'] >= 90, 'doc_label'] = 'P'\ndocs.loc[docs['points'] < 90, 'doc_label'] = 'N'\n\n\ndocs.head()\n\n\n\n\n\n  \n    \n      \n      doc_content\n      points\n      doc_label\n    \n    \n      doc_id\n      \n      \n      \n    \n  \n  \n    \n      336\n      Gritty, heavily roasted aromas of peanuts and ...\n      83\n      N\n    \n    \n      337\n      An easy and inviting selection, there's a ment...\n      83\n      N\n    \n    \n      338\n      The wine is earthy and somewhat rustic. There ...\n      82\n      N\n    \n    \n      339\n      Red in color, with berry and apple aromas, thi...\n      82\n      N\n    \n    \n      340\n      The nose is muted, despite the slight spritz o...\n      82\n      N\n    \n  \n\n\n\n\nDrop points columns\n\ntry:\n    docs = docs.drop('points', axis=1)\nexcept KeyError as e:\n    if \"not found in axis\" in str(e):\n        pass\n    else:\n        print(e)\n\n\ndocs.head()\n\n\n\n\n\n  \n    \n      \n      doc_content\n      doc_label\n    \n    \n      doc_id\n      \n      \n    \n  \n  \n    \n      336\n      Gritty, heavily roasted aromas of peanuts and ...\n      N\n    \n    \n      337\n      An easy and inviting selection, there's a ment...\n      N\n    \n    \n      338\n      The wine is earthy and somewhat rustic. There ...\n      N\n    \n    \n      339\n      Red in color, with berry and apple aromas, thi...\n      N\n    \n    \n      340\n      The nose is muted, despite the slight spritz o...\n      N"
  },
  {
    "objectID": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#divide-docs-into-train-and-test-sets",
    "href": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#divide-docs-into-train-and-test-sets",
    "title": "Metadata",
    "section": "Divide docs into train and test sets",
    "text": "Divide docs into train and test sets\nNote: Packages like SciKit learn do this for you. Here we do it from scratch.\nAssign random numbers to docs\n\ndocs['set'] = randint(0, Params.n_sets, len(docs.index))\n\n\ndocs.head()\n\n\n\n\n\n  \n    \n      \n      doc_content\n      doc_label\n      set\n    \n    \n      doc_id\n      \n      \n      \n    \n  \n  \n    \n      336\n      Gritty, heavily roasted aromas of peanuts and ...\n      N\n      1\n    \n    \n      337\n      An easy and inviting selection, there's a ment...\n      N\n      1\n    \n    \n      338\n      The wine is earthy and somewhat rustic. There ...\n      N\n      3\n    \n    \n      339\n      Red in color, with berry and apple aromas, thi...\n      N\n      3\n    \n    \n      340\n      The nose is muted, despite the slight spritz o...\n      N\n      1\n    \n  \n\n\n\n\nSplit docs by assigned number\n\ntraining_docs = docs[docs.set != 0].copy()\ntesting_docs = docs[docs.set == 0].copy()\ndel(docs)\n\nDrop set columns.\n\ntry:\n    training_docs = training_docs.drop('set', axis=1)\n    testing_docs = testing_docs.drop('set', axis=1)\nexcept KeyError as e:\n    if \"not found in axis\" in str(e):\n        pass\n    else:\n        print(e)\n\nSee ratio of set sizes.\n\nround(len(training_docs) / len(testing_docs), 2)\n\n2.98"
  },
  {
    "objectID": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#convert-docs-to-tokens",
    "href": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#convert-docs-to-tokens",
    "title": "Metadata",
    "section": "Convert docs to tokens",
    "text": "Convert docs to tokens\nNote that we only use the vocabulary of the training data.\n\ntraining_tokens, vocab = tx.create_tokens_and_vocab(training_docs, src_col='doc_content')\ntesting_tokens, _ = tx.create_tokens_and_vocab(testing_docs, src_col='doc_content')\n\nClean up results. Remove term_id from tables; just use term_str.\nThis is only need because we are using a legacy library to do our tokenization.\n\nvocab = vocab[vocab.sw == False]\n\n\ntry:\n    vocab = vocab.reset_index(drop=True).set_index('term_str')\nexcept KeyError as e:\n    if \"None of ['term_str'] are in the columns\" in str(e):\n        pass\n    else:\n        print(e)    \n\n\ntry:\n    training_tokens = training_tokens.drop(['term_id','token'], axis=1)\n    testing_tokens = testing_tokens.drop(['term_id','token'], axis=1)\nexcept KeyError as e:\n    if \"not found in axis\" in str(e):\n        pass\n    else:\n        print(e)\n\n\ntraining_tokens.head()\n\n\n\n\n\n  \n    \n      \n      \n      \n      term_str\n    \n    \n      doc_id\n      sent_id\n      token_id\n      \n    \n  \n  \n    \n      336\n      0\n      0\n      gritty\n    \n    \n      1\n      heavily\n    \n    \n      2\n      roasted\n    \n    \n      3\n      aromas\n    \n    \n      5\n      peanuts\n    \n  \n\n\n\n\n\ntesting_tokens.head()\n\n\n\n\n\n  \n    \n      \n      \n      \n      term_str\n    \n    \n      doc_id\n      sent_id\n      token_id\n      \n    \n  \n  \n    \n      342\n      0\n      0\n      funky\n    \n    \n      1\n      yeasty\n    \n    \n      2\n      aromas\n    \n    \n      4\n      cinnamon\n    \n    \n      5\n      spent"
  },
  {
    "objectID": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#convert-tokens-to-bag-of-words",
    "href": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#convert-tokens-to-bag-of-words",
    "title": "Metadata",
    "section": "Convert tokens to bag-of-words",
    "text": "Convert tokens to bag-of-words\n\ntraining_bow = training_tokens.groupby(['doc_id', 'term_str']).term_str.count().to_frame('n')\ntesting_bow = testing_tokens.groupby(['doc_id', 'term_str']).term_str.count().to_frame('n')\n\n\ntraining_bow.head()\n\n\n\n\n\n  \n    \n      \n      \n      n\n    \n    \n      doc_id\n      term_str\n      \n    \n  \n  \n    \n      336\n      aromas\n      1\n    \n    \n      back\n      1\n    \n    \n      berry\n      1\n    \n    \n      better\n      1\n    \n    \n      briny\n      1\n    \n  \n\n\n\n\n\ntesting_bow.head()\n\n\n\n\n\n  \n    \n      \n      \n      n\n    \n    \n      doc_id\n      term_str\n      \n    \n  \n  \n    \n      342\n      apple\n      1\n    \n    \n      aromas\n      1\n    \n    \n      bread\n      1\n    \n    \n      briny\n      1\n    \n    \n      cinnamon\n      1"
  },
  {
    "objectID": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#transfer-doc-labels-and-splits-to-bow-crucial",
    "href": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#transfer-doc-labels-and-splits-to-bow-crucial",
    "title": "Metadata",
    "section": "Transfer DOC labels and splits to BOW — CRUCIAL",
    "text": "Transfer DOC labels and splits to BOW — CRUCIAL\nWe propagate the training doc labels to each of tokens in the docs. Each term_str will then have “votes” for being either P or N.\n\ntry:\n    training_bow = training_bow.join(training_docs[['doc_label']], on='doc_id', how='inner')\n    # training_tokens = training_tokens.join(training_docs[['doc_label']], on='doc_id', how='inner')\nexcept ValueError as e:\n    if 'columns overlap' in str(e):\n        pass\n    else:\n        print(e)\n\n\n# training_tokens.head()\n\n\ntraining_bow.head()\n\n\n\n\n\n  \n    \n      \n      \n      n\n      doc_label\n    \n    \n      doc_id\n      term_str\n      \n      \n    \n  \n  \n    \n      336\n      aromas\n      1\n      N\n    \n    \n      back\n      1\n      N\n    \n    \n      berry\n      1\n      N\n    \n    \n      better\n      1\n      N\n    \n    \n      briny\n      1\n      N"
  },
  {
    "objectID": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#estimate-class-priors-pc",
    "href": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#estimate-class-priors-pc",
    "title": "Metadata",
    "section": "Estimate class priors \\(p(c)\\)",
    "text": "Estimate class priors \\(p(c)\\)\n\\[\n\\hat{P}(c) = \\dfrac{N_{c}}{N_{d}}\n\\]\n\n# training_bow.groupby('term_str')[['n']].sum().join(vocab, rsuffix='r').query(\"n != nr\")\n\n\nif Params.prior_method == 'tokens':\n    class_priors = training_bow['doc_label'].value_counts(normalize=True)\nelif Params.prior_method == 'docs':\n    class_priors = training_docs['doc_label'].value_counts(normalize=True)\n\n\nprint(\"By\", Params.prior_method)\nnp.round(class_priors, 3)\n\nBy docs\n\n\nN    0.5\nP    0.5\nName: doc_label, dtype: float64\n\n\nConvert priors to logs\n\nclass_priors_log = log(class_priors)\n\n\nclass_priors_log\n\nN   -0.999206\nP   -1.000794\nName: doc_label, dtype: float64\n\n\n\n# np.round(class_priors_log)\n\nEstimate likelihoods \\(p(w|c)\\)\n\\[\n\\hat{P}(w_i|c) = \\dfrac{count(w_i,c)}{\\sum_{w \\in V} count(w,c)}\n\\]\n\\[\n\\hat{P}(w_i|c) = \\dfrac{count(w_i,c)+1}{\\sum_{w \\in V} (count(w,c)+1)} = \\dfrac{count(w_i,c)+1}{(\\sum_{w \\in V} count(w,c))+|V|}\n\\]\nNow we compute the probability of a token given the label. This will in effect product two language models, one for each label. Key idea = the likelihoods are language models (see Pearl for interpretation of likelihoods).\n\n# training_tokens\n\n\n# class_likelihoods = training_tokens.groupby(['term_str', 'doc_label']).doc_label.count()\\\n#     .unstack(fill_value=0)\n\nNote that using the bow means we do not count multiple instances of a word in a doc.\n\nclass_counts = training_bow.groupby(['term_str', 'doc_label']).doc_label.count()\\\n    .unstack(fill_value=0)\n\n\nclass_counts.head()\n\n\n\n\n\n  \n    \n      doc_label\n      N\n      P\n    \n    \n      term_str\n      \n      \n    \n  \n  \n    \n      aaron\n      0\n      4\n    \n    \n      abbey\n      0\n      3\n    \n    \n      abbott\n      0\n      3\n    \n    \n      abbreviated\n      3\n      0\n    \n    \n      ability\n      1\n      30\n    \n  \n\n\n\n\n\nclass_counts_smoothed = class_counts + Params.smooth_alpha\nclass_likelihoods = class_counts_smoothed  / class_counts_smoothed.sum()\n\n\nclass_likelihoods.sample(10).style.background_gradient(cmap='GnBu', axis=None)\n\n\n\n\n  \n    \n      doc_label\n      N\n      P\n    \n    \n      term_str\n       \n       \n    \n  \n  \n    \n      enjoyable\n      0.000147\n      0.000196\n    \n    \n      essential\n      0.000001\n      0.000019\n    \n    \n      intriguingly\n      0.000007\n      0.000037\n    \n    \n      pudding\n      0.000013\n      0.000019\n    \n    \n      urban\n      0.000013\n      0.000008\n    \n    \n      standing\n      0.000013\n      0.000023\n    \n    \n      demonstrates\n      0.000001\n      0.000023\n    \n    \n      satisfaction\n      0.000007\n      0.000011\n    \n    \n      fresh\n      0.004197\n      0.003310\n    \n    \n      accessibility\n      0.000001\n      0.000011"
  },
  {
    "objectID": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#convert-likelihoods-to-logs",
    "href": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#convert-likelihoods-to-logs",
    "title": "Metadata",
    "section": "Convert likelihoods to logs",
    "text": "Convert likelihoods to logs\n\nclass_likelihoods_log = log(class_likelihoods)\n\n\nclass_likelihoods_log.sample(10).style.background_gradient(cmap='GnBu')\n\n\n\n\n  \n    \n      doc_label\n      N\n      P\n    \n    \n      term_str\n       \n       \n    \n  \n  \n    \n      grillo\n      -15.291581\n      -21.366933\n    \n    \n      toughness\n      -15.694937\n      -15.694507\n    \n    \n      veneer\n      -15.694937\n      -15.027083\n    \n    \n      undrinkable\n      -15.694937\n      -15.027083\n    \n    \n      volcanic\n      -17.189702\n      -14.036016\n    \n    \n      catty\n      -15.694937\n      -21.366933\n    \n    \n      feral\n      -13.990922\n      -13.086162\n    \n    \n      wonder\n      -15.291581\n      -15.027083\n    \n    \n      newer\n      -17.189702\n      -15.436195\n    \n    \n      giuseppe\n      -20.649133\n      -16.412736"
  },
  {
    "objectID": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#add-likelihood-columns-to-test-tokens-table",
    "href": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#add-likelihood-columns-to-test-tokens-table",
    "title": "Metadata",
    "section": "Add likelihood columns to test tokens table",
    "text": "Add likelihood columns to test tokens table\nThis is effectively how we apply our model to the test set.\n\ntesting_tokens.head()\n\n\n\n\n\n  \n    \n      \n      \n      \n      term_str\n    \n    \n      doc_id\n      sent_id\n      token_id\n      \n    \n  \n  \n    \n      342\n      0\n      0\n      funky\n    \n    \n      1\n      yeasty\n    \n    \n      2\n      aromas\n    \n    \n      4\n      cinnamon\n    \n    \n      5\n      spent\n    \n  \n\n\n\n\n\ntry:\n    testing_tokens = testing_tokens\\\n        .join(class_likelihoods_log[['P','N']], on='term_str', how='inner')\nexcept ValueError as e:\n    if \"columns overlap\" in str(e):\n        pass\n    else:\n        print(e)\n\n\ntesting_tokens.sample(5)\n\n\n\n\n\n  \n    \n      \n      \n      \n      term_str\n      P\n      N\n    \n    \n      doc_id\n      sent_id\n      token_id\n      \n      \n      \n    \n  \n  \n    \n      126957\n      0\n      1\n      dry\n      -8.095032\n      -7.040532\n    \n    \n      95531\n      0\n      27\n      leather\n      -8.823176\n      -9.986465\n    \n    \n      75951\n      1\n      7\n      concentrated\n      -7.996381\n      -11.966139\n    \n    \n      9795\n      0\n      19\n      texture\n      -7.923341\n      -8.321300\n    \n    \n      38716\n      1\n      2\n      tastes\n      -11.385365\n      -7.888206"
  },
  {
    "objectID": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#compute-posteriors-pcw",
    "href": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#compute-posteriors-pcw",
    "title": "Metadata",
    "section": "Compute posteriors \\(p(c|w)\\)",
    "text": "Compute posteriors \\(p(c|w)\\)\n\\[\nc_{NB} = \\arg\\max \\log{P(c)} + \\sum_{id=1}^{id_{max}} \\log{P(token_{id}|c)}\n\\]\n\n# testing_docs['prediction'] = testing_tokens.groupby('doc_id')\\\n#     .apply(lambda x: x[['P','N']].sum())\\\n#     .apply(lambda x: x + class_priors_log, axis=1)\\\n#     .idxmax(1)\n\n\ntesting_docs['prediction'] = testing_tokens.groupby('doc_id')\\\n    .apply(lambda x: x[['P','N']].sum() + class_priors_log).idxmax(1)\n\n\ntesting_docs.sample(10)\n\n\n\n\n\n  \n    \n      \n      doc_content\n      doc_label\n      prediction\n    \n    \n      doc_id\n      \n      \n      \n    \n  \n  \n    \n      52795\n      From Passadouro's old vines, this is serious, ...\n      P\n      P\n    \n    \n      111251\n      Juicy red fruits are the veneer on a wine that...\n      P\n      P\n    \n    \n      74337\n      Light lemon and melon aromas lead the nose, bu...\n      N\n      N\n    \n    \n      107056\n      Candied blueberry and syrupy aromas jump from ...\n      N\n      N\n    \n    \n      7088\n      Smells raisiny-Porty, and turns dry and auster...\n      N\n      N\n    \n    \n      7084\n      This is light and seems a little sparkly, as i...\n      N\n      N\n    \n    \n      127157\n      Green, unripe vegetal notes se compromise this...\n      N\n      N\n    \n    \n      23981\n      This was the final vintage Patrick Campbell fu...\n      P\n      P\n    \n    \n      37365\n      Robust and full-bodied, as you'd expect from t...\n      P\n      P\n    \n    \n      64298\n      An unadulterated Chardonnay with a light-to-me...\n      N\n      N"
  },
  {
    "objectID": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#show-raw-t-f-counts",
    "href": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#show-raw-t-f-counts",
    "title": "Metadata",
    "section": "Show raw T & F counts",
    "text": "Show raw T & F counts\n\nraw = testing_docs.result.value_counts()\n\n\nraw\n\nTrue     5896\nFalse     206\nName: result, dtype: int64\n\n\n\nraw[True] / raw[False]\n\n28.62135922330097"
  },
  {
    "objectID": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#show-raw-tp-tn-fp-fn-counts",
    "href": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#show-raw-tp-tn-fp-fn-counts",
    "title": "Metadata",
    "section": "Show raw TP, TN, FP, FN counts",
    "text": "Show raw TP, TN, FP, FN counts\n\nraw1 = testing_docs.result_label.value_counts()\n\n\nraw1\n\nTP    3048\nTN    2848\nFP     149\nFN      57\nName: result_label, dtype: int64\n\n\n\n# Accuracy\n(raw1.TP + raw1.TN) / raw1.sum()\n\n0.9662405768600459"
  },
  {
    "objectID": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#create-confusion-matrix",
    "href": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#create-confusion-matrix",
    "title": "Metadata",
    "section": "Create confusion matrix",
    "text": "Create confusion matrix\n\nCM = testing_docs.reset_index().groupby(['prediction','doc_label'])\\\n    .doc_id.count().unstack().fillna(0)\n\n\nCM.columns.name = 'actual'\n\n\nCM.style.background_gradient(axis=None)\n\n\n\n\n  \n    \n      actual\n      N\n      P\n    \n    \n      prediction\n       \n       \n    \n  \n  \n    \n      N\n      2848\n      57\n    \n    \n      P\n      149\n      3048\n    \n  \n\n\n\n\ndef get_results(CM):\n\n    class Results():\n        \n        TP = CM.iloc[0,0] # hits\n        FP = CM.iloc[0,1] # Type I errors; false alarms\n        TN = CM.iloc[1,1] # correct rejections\n        FN = CM.iloc[1,0] # Type  II errors; misses\n        \n        T = TP + TN\n        F = FP + FN\n        ALL =  T + F\n        \n        ACC = T / ALL                       # Accuracy\n        TPR = TP / (TP + FN)                # Recall, Sensitivity\n        TNR = TN / (TN + FP)                # Specificity\n        PPV = TP / (TP + FP)                # Precision; Positive predictive value \n        BA = (TNR + TPR) / 2                # Balanced Accuracy\n        F1 = (2 *  TP) / (2 * TP + FP + FN) # F-score where F =  1\n\n        assert ALL == CM.sum().sum()\n                \n        def show_results(self):\n            print('TPR:', round(self.TPR, 2), '(sensitivity)')\n            print('TNR:', round(self.TNR, 2), '(specificity)')\n            print('F1: ', round(self.F1, 2), '<-- GRADE')\n            print('-'*9)\n            print('PPV:', round(self.PPV, 2),  '(precision)')\n            print('ACC:', round(self.ACC, 2), '(accuracy)')\n            \n    return Results()\n\n\nR = get_results(CM)\n\n\nR.show_results()\n\nTPR: 0.95 (sensitivity)\nTNR: 0.98 (specificity)\nF1:  0.97 <-- GRADE\n---------\nPPV: 0.98 (precision)\nACC: 0.97 (accuracy)"
  },
  {
    "objectID": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#likelihoods-as-sentiment-lexicon",
    "href": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#likelihoods-as-sentiment-lexicon",
    "title": "Metadata",
    "section": "Likelihoods as Sentiment Lexicon",
    "text": "Likelihoods as Sentiment Lexicon\nWe make up a way to compute sentiment valence and polarity\n\nsentilex = (class_likelihoods_log.P - class_likelihoods_log.N).to_frame('valence')\n\n\nsentilex['polarity'] = np.sign(sentilex['valence'])\n\nRatio of positive to negative terms\n\nsentilex.polarity.value_counts().plot(kind='pie');"
  },
  {
    "objectID": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#top-positive-words",
    "href": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#top-positive-words",
    "title": "Metadata",
    "section": "Top positive words",
    "text": "Top positive words\n\nsentilex.sort_values('valence', ascending=False).head(10)\n\n\n\n\n\n  \n    \n      \n      valence\n      polarity\n    \n    \n      term_str\n      \n      \n    \n  \n  \n    \n      superb\n      9.487994\n      1.0\n    \n    \n      pipe\n      9.360352\n      1.0\n    \n    \n      exceptional\n      9.205528\n      1.0\n    \n    \n      dramatic\n      8.776056\n      1.0\n    \n    \n      seamless\n      8.735471\n      1.0\n    \n    \n      premier\n      8.693712\n      1.0\n    \n    \n      parcel\n      8.693712\n      1.0\n    \n    \n      sites\n      8.650707\n      1.0\n    \n    \n      sagebrush\n      8.537229\n      1.0\n    \n    \n      exquisite\n      8.489215\n      1.0"
  },
  {
    "objectID": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#top-negative-words",
    "href": "lessons/M12_Classification/M12_01_NaiveBayesWineReviews.html#top-negative-words",
    "title": "Metadata",
    "section": "Top negative words",
    "text": "Top negative words\n\nsentilex.sort_values('valence', ascending=True).head(10)\n\n\n\n\n\n  \n    \n      \n      valence\n      polarity\n    \n    \n      term_str\n      \n      \n    \n  \n  \n    \n      everyday\n      -11.362557\n      -1.0\n    \n    \n      bland\n      -11.085214\n      -1.0\n    \n    \n      watery\n      -10.971647\n      -1.0\n    \n    \n      mealy\n      -10.611101\n      -1.0\n    \n    \n      dilute\n      -10.564857\n      -1.0\n    \n    \n      weedy\n      -10.564857\n      -1.0\n    \n    \n      pickled\n      -10.533183\n      -1.0\n    \n    \n      strange\n      -10.308386\n      -1.0\n    \n    \n      melony\n      -9.949020\n      -1.0\n    \n    \n      murky\n      -9.875146\n      -1.0\n    \n  \n\n\n\n\n\nwpos = sentilex.valence.sort_values().tail(10)\nwneg = sentilex.valence.sort_values().head(10)\npd.concat([wneg,wpos]).plot.barh(figsize=(5,7));"
  },
  {
    "objectID": "lessons/M12_Classification/M12_01b_SKLearnMNB.html",
    "href": "lessons/M12_Classification/M12_01b_SKLearnMNB.html",
    "title": "Metadata",
    "section": "",
    "text": "Course:    DS 5001\nModule:    M12 Lab\nTopic:     Implementing Naive Bayes in SciKit Learn\nAuthor:    R.C. Alvarado\nDate:      19 April 2023\n\nConfig\n\nclass Params:\n    min_df = 5\n    test_size = 0.25\n    a_cut = 93\n    b_cut = 84\n\n\n\nImport\n\nimport pandas as pd\nimport numpy as np\nimport plotly_express as px\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB, CategoricalNB\n\n\n\nGet raw data\n\nDOC = pd.read_csv(\"../data/winereviews/winereviews.csv\").set_index('doc_id')\n\n\nV = pd.read_csv(\"../data/winereviews/winereviews-VOCAB.csv\").term_str.values\n\n\nV\n\narray(['aaron', 'abbey', 'abbott', ..., 'zweigelt', 'émilion', 'über'],\n      dtype=object)\n\n\n\nDOC\n\n\n\n\n\n  \n    \n      \n      doc_content\n      points\n    \n    \n      doc_id\n      \n      \n    \n  \n  \n    \n      0\n      Aromas include tropical fruit, broom, brimston...\n      87\n    \n    \n      1\n      This is ripe and fruity, a wine that is smooth...\n      87\n    \n    \n      2\n      Tart and snappy, the flavors of lime flesh and...\n      87\n    \n    \n      3\n      Pineapple rind, lemon pith and orange blossom ...\n      87\n    \n    \n      4\n      Much like the regular bottling from 2012, this...\n      87\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      129966\n      Notes of honeysuckle and cantaloupe sweeten th...\n      90\n    \n    \n      129967\n      Citation is given as much as a decade of bottl...\n      90\n    \n    \n      129968\n      Well-drained gravel soil gives this wine its c...\n      90\n    \n    \n      129969\n      A dry style of Pinot Gris, this is crisp with ...\n      90\n    \n    \n      129970\n      Big, rich and off-dry, this is powered by inte...\n      90\n    \n  \n\n119978 rows × 2 columns\n\n\n\n\n\nFilter out middle\n\nDOC = DOC.loc[(DOC.points >= Params.a_cut) | (DOC.points <= Params.b_cut)].copy()\n\n\n\nThunk ratings to labels\n\nDOC.loc[DOC.points >=90, 'label'] = 'P'\nDOC.loc[DOC.points <90, 'label'] = 'N'\n\n\nDOC\n\n\n\n\n\n  \n    \n      \n      doc_content\n      points\n      label\n    \n    \n      doc_id\n      \n      \n      \n    \n  \n  \n    \n      336\n      Gritty, heavily roasted aromas of peanuts and ...\n      83\n      N\n    \n    \n      337\n      An easy and inviting selection, there's a ment...\n      83\n      N\n    \n    \n      338\n      The wine is earthy and somewhat rustic. There ...\n      82\n      N\n    \n    \n      339\n      Red in color, with berry and apple aromas, thi...\n      82\n      N\n    \n    \n      340\n      The nose is muted, despite the slight spritz o...\n      82\n      N\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      129678\n      There's a stunning amount of action on the nos...\n      93\n      P\n    \n    \n      129679\n      From a ranch planted originally in 1926 this i...\n      93\n      P\n    \n    \n      129680\n      Old vines in a field blend give a rich structu...\n      93\n      P\n    \n    \n      129681\n      Ripe black-skinned berry, violet, leather and ...\n      93\n      P\n    \n    \n      129682\n      After helping Liquid Farm launch to Chardonnay...\n      93\n      P\n    \n  \n\n24270 rows × 3 columns\n\n\n\n\n\nConvert docs to count matrix\n\ncount_engine = CountVectorizer(stop_words='english', min_df=Params.min_df, vocabulary=V)\n\n\ncount_model = count_engine.fit_transform(DOC.doc_content)\n\n\nDTM = pd.DataFrame(count_model.toarray(), columns=count_engine.get_feature_names_out(), index=DOC.index)\n\n\nDTM\n\n\n\n\n\n  \n    \n      \n      aaron\n      abbey\n      abbott\n      abbreviated\n      ability\n      able\n      abound\n      abounds\n      abrasive\n      abrupt\n      ...\n      zing\n      zingy\n      zinny\n      zins\n      zip\n      zippy\n      zone\n      zweigelt\n      émilion\n      über\n    \n    \n      doc_id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      336\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      337\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      338\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      339\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      340\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      129678\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      129679\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      129680\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      129681\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      129682\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n24270 rows × 7254 columns\n\n\n\n\n\nSplit data\n\nX_train, X_test, y_train, y_test = train_test_split(DTM, DOC.label, test_size=Params.test_size, random_state=0)\n\n\n\nFit model\n\nmnb_engine = MultinomialNB(force_alpha=True)\n\n\nmnb_model = mnb_engine.fit(X_train, y_train)\n\n\n\nTest model\n\ny_predict = pd.DataFrame(mnb_engine.predict(X_test), index=X_test.index)\n\n\n\nEvaluate model\n\nRESULT = pd.concat([y_test, y_predict], axis=1)\nRESULT.columns = ['actual', 'predicted']\nRESULT['validity'] = (RESULT.actual == RESULT.predicted).astype('str').str[0]\n\n\nRESULT\n\n\n\n\n\n  \n    \n      \n      actual\n      predicted\n      validity\n    \n    \n      doc_id\n      \n      \n      \n    \n  \n  \n    \n      33313\n      N\n      N\n      T\n    \n    \n      124580\n      N\n      N\n      T\n    \n    \n      127027\n      N\n      N\n      T\n    \n    \n      100482\n      N\n      N\n      T\n    \n    \n      61771\n      N\n      N\n      T\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      112450\n      P\n      P\n      T\n    \n    \n      40342\n      P\n      P\n      T\n    \n    \n      112684\n      P\n      P\n      T\n    \n    \n      57579\n      P\n      P\n      T\n    \n    \n      93789\n      N\n      N\n      T\n    \n  \n\n6068 rows × 3 columns\n\n\n\n\nRESULT.value_counts('validity')\n\nvalidity\nT    5866\nF     202\ndtype: int64\n\n\n\nTALLY = RESULT.value_counts(['validity', 'predicted'])\n\n\nTALLY\n\nvalidity  predicted\nT         P            3049\n          N            2817\nF         P             139\n          N              63\ndtype: int64\n\n\n\nCONFUSION = RESULT.value_counts(['predicted','actual']).unstack()\n\n\nCONFUSION\n\n\n\n\n\n  \n    \n      actual\n      N\n      P\n    \n    \n      predicted\n      \n      \n    \n  \n  \n    \n      N\n      2817\n      63\n    \n    \n      P\n      139\n      3049\n    \n  \n\n\n\n\n\nR = pd.DataFrame(\n    dict(\n        test_size = Params.test_size,\n        a_cut = Params.a_cut,\n        b_cut = Params.b_cut,\n        TP = TALLY.loc[('T','P')],\n        TN = TALLY.loc[('T','N')],\n        FP = TALLY.loc[('F','P')],\n        FN = TALLY.loc[('F','N')]\n    ),\n    index=['mnb']\n)\n\n\nR\n\n\n\n\n\n  \n    \n      \n      test_size\n      a_cut\n      b_cut\n      TP\n      TN\n      FP\n      FN\n    \n  \n  \n    \n      mnb\n      0.25\n      93\n      84\n      3049\n      2817\n      139\n      63\n    \n  \n\n\n\n\n\nR['P'] = R.TP + R.FN\nR['N'] = R.TN + R.FP\n\naccuracy\n\n\n\nimage.png\n\n\n\nR['ACC'] = (R.TP + R.TN) / (R.P + R.N)\n\nsensitivity, recall, hit rate, or true positive rate (TPR)\n\n\n\nimage.png\n\n\n\nR['TPR'] = R.TP / R.P\n\nspecificity, selectivity or true negative rate (TNR)\n\n\n\nimage.png\n\n\n\nR['TNR'] = R.TN / R.N\n\nprecision or positive predictive value (PPV)\n\n\n\nimage.png\n\n\n\nR['PPV'] = R.TP / (R.TP + R.FP)\n\nF1 score\nThe harmonic mean of precision and sensitivity.\n\n\n\nimage.png\n\n\nbalanced accuracy (BA)\n\n\n\nimage.png\n\n\n\nR['BA'] = (R.TPR + R.TNR) / 2\n\n\nR['F1'] = 2 * ((R.PPV * R.TPR) / (R.PPV + R.TPR))\n\n\nR\n\n\n\n\n\n  \n    \n      \n      test_size\n      a_cut\n      b_cut\n      TP\n      TN\n      FP\n      FN\n      P\n      N\n      ACC\n      TPR\n      TNR\n      PPV\n      BA\n      F1\n    \n  \n  \n    \n      mnb\n      0.25\n      93\n      84\n      3049\n      2817\n      139\n      63\n      3112\n      2956\n      0.966711\n      0.979756\n      0.952977\n      0.956399\n      0.966366\n      0.967937\n    \n  \n\n\n\n\n\n\nExplore Model\n\nVOCAB = DTM.loc[X_train.index].sum().to_frame('n').reset_index().rename(columns={'index':'term_str'})\nVOCAB[['P','N']] = pd.DataFrame(mnb_engine.feature_count_).T\nVOCAB = VOCAB.set_index('term_str')\n\n\nVOCAB\n\n\n\n\n\n  \n    \n      \n      n\n      P\n      N\n    \n    \n      term_str\n      \n      \n      \n    \n  \n  \n    \n      aaron\n      3\n      0.0\n      3.0\n    \n    \n      abbey\n      3\n      0.0\n      3.0\n    \n    \n      abbott\n      3\n      0.0\n      3.0\n    \n    \n      abbreviated\n      3\n      3.0\n      0.0\n    \n    \n      ability\n      27\n      0.0\n      27.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      zippy\n      28\n      10.0\n      18.0\n    \n    \n      zone\n      4\n      1.0\n      3.0\n    \n    \n      zweigelt\n      7\n      5.0\n      2.0\n    \n    \n      émilion\n      3\n      1.0\n      2.0\n    \n    \n      über\n      6\n      3.0\n      3.0\n    \n  \n\n7254 rows × 3 columns\n\n\n\n\nVOCAB[['P_ll','N_ll']] = np.log2((VOCAB[['P','N']] + .01) / (VOCAB[['P','N']] + .01).sum())\nVOCAB['valence'] = VOCAB.N_ll - VOCAB.P_ll\nVOCAB['polarity'] = np.sign(VOCAB.valence)\n\n\nPOS = VOCAB.sort_values('valence', ascending=False).head(20)\n\n\nPOS\n\n\n\n\n\n  \n    \n      \n      n\n      P\n      N\n      P_ll\n      N_ll\n      valence\n      polarity\n    \n    \n      term_str\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      wonderfully\n      129\n      0.0\n      129.0\n      -23.932313\n      -10.985540\n      12.946773\n      1.0\n    \n    \n      superb\n      112\n      0.0\n      112.0\n      -23.932313\n      -11.189395\n      12.742917\n      1.0\n    \n    \n      doles\n      98\n      0.0\n      98.0\n      -23.932313\n      -11.382022\n      12.550291\n      1.0\n    \n    \n      exceptional\n      90\n      0.0\n      90.0\n      -23.932313\n      -11.504866\n      12.427447\n      1.0\n    \n    \n      premier\n      78\n      0.0\n      78.0\n      -23.932313\n      -11.711292\n      12.221021\n      1.0\n    \n    \n      framework\n      71\n      0.0\n      71.0\n      -23.932313\n      -11.846929\n      12.085384\n      1.0\n    \n    \n      dramatic\n      70\n      0.0\n      70.0\n      -23.932313\n      -11.867390\n      12.064923\n      1.0\n    \n    \n      sites\n      67\n      0.0\n      67.0\n      -23.932313\n      -11.930575\n      12.001738\n      1.0\n    \n    \n      parcel\n      60\n      0.0\n      60.0\n      -23.932313\n      -12.089748\n      11.842565\n      1.0\n    \n    \n      exquisite\n      59\n      0.0\n      59.0\n      -23.932313\n      -12.113992\n      11.818321\n      1.0\n    \n    \n      douro\n      57\n      0.0\n      57.0\n      -23.932313\n      -12.163736\n      11.768577\n      1.0\n    \n    \n      sumptuous\n      55\n      0.0\n      55.0\n      -23.932313\n      -12.215257\n      11.717055\n      1.0\n    \n    \n      detailed\n      52\n      0.0\n      52.0\n      -23.932313\n      -12.296162\n      11.636151\n      1.0\n    \n    \n      iris\n      52\n      0.0\n      52.0\n      -23.932313\n      -12.296162\n      11.636151\n      1.0\n    \n    \n      oldest\n      49\n      0.0\n      49.0\n      -23.932313\n      -12.381875\n      11.550438\n      1.0\n    \n    \n      explodes\n      49\n      0.0\n      49.0\n      -23.932313\n      -12.381875\n      11.550438\n      1.0\n    \n    \n      woodland\n      47\n      0.0\n      47.0\n      -23.932313\n      -12.441983\n      11.490329\n      1.0\n    \n    \n      impeccable\n      46\n      0.0\n      46.0\n      -23.932313\n      -12.473004\n      11.459309\n      1.0\n    \n    \n      dazzling\n      46\n      0.0\n      46.0\n      -23.932313\n      -12.473004\n      11.459309\n      1.0\n    \n    \n      crus\n      45\n      0.0\n      45.0\n      -23.932313\n      -12.504705\n      11.427607\n      1.0\n    \n  \n\n\n\n\n\nNEG = VOCAB.sort_values('valence', ascending=True).head(20)\n\n\nNEG\n\n\n\n\n\n  \n    \n      \n      n\n      P\n      N\n      P_ll\n      N_ll\n      valence\n      polarity\n    \n    \n      term_str\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      everyday\n      161\n      161.0\n      0.0\n      -9.957450\n      -24.640735\n      -14.683285\n      -1.0\n    \n    \n      bland\n      136\n      136.0\n      0.0\n      -10.200888\n      -24.640735\n      -14.439848\n      -1.0\n    \n    \n      watery\n      128\n      128.0\n      0.0\n      -10.288344\n      -24.640735\n      -14.352392\n      -1.0\n    \n    \n      dilute\n      107\n      107.0\n      0.0\n      -10.546855\n      -24.640735\n      -14.093881\n      -1.0\n    \n    \n      pickled\n      106\n      106.0\n      0.0\n      -10.560400\n      -24.640735\n      -14.080335\n      -1.0\n    \n    \n      weedy\n      88\n      88.0\n      0.0\n      -10.828861\n      -24.640735\n      -13.811874\n      -1.0\n    \n    \n      scratchy\n      86\n      86.0\n      0.0\n      -10.862024\n      -24.640735\n      -13.778711\n      -1.0\n    \n    \n      mealy\n      84\n      84.0\n      0.0\n      -10.895967\n      -24.640735\n      -13.744768\n      -1.0\n    \n    \n      strange\n      76\n      76.0\n      0.0\n      -11.040339\n      -24.640735\n      -13.600396\n      -1.0\n    \n    \n      murky\n      66\n      66.0\n      0.0\n      -11.243844\n      -24.640735\n      -13.396892\n      -1.0\n    \n    \n      diluted\n      65\n      65.0\n      0.0\n      -11.265867\n      -24.640735\n      -13.374869\n      -1.0\n    \n    \n      stewy\n      61\n      61.0\n      0.0\n      -11.357483\n      -24.640735\n      -13.283253\n      -1.0\n    \n    \n      easygoing\n      61\n      61.0\n      0.0\n      -11.357483\n      -24.640735\n      -13.283253\n      -1.0\n    \n    \n      clumsy\n      59\n      59.0\n      0.0\n      -11.405569\n      -24.640735\n      -13.235166\n      -1.0\n    \n    \n      melony\n      54\n      54.0\n      0.0\n      -11.533302\n      -24.640735\n      -13.107434\n      -1.0\n    \n    \n      confected\n      53\n      53.0\n      0.0\n      -11.560264\n      -24.640735\n      -13.080472\n      -1.0\n    \n    \n      informal\n      52\n      52.0\n      0.0\n      -11.587739\n      -24.640735\n      -13.052996\n      -1.0\n    \n    \n      plastic\n      50\n      50.0\n      0.0\n      -11.644312\n      -24.640735\n      -12.996424\n      -1.0\n    \n    \n      okay\n      48\n      48.0\n      0.0\n      -11.703193\n      -24.640735\n      -12.937542\n      -1.0\n    \n    \n      tired\n      46\n      46.0\n      0.0\n      -11.764581\n      -24.640735\n      -12.876154\n      -1.0\n    \n  \n\n\n\n\n\n# VOCAB['feature_names'] = mnb_engine.feature_names_in_\n\n\nVOCAB\n\n\n\n\n\n  \n    \n      \n      n\n      P\n      N\n      P_ll\n      N_ll\n      valence\n      polarity\n    \n    \n      term_str\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      aaron\n      3\n      0.0\n      3.0\n      -23.932313\n      -16.407116\n      7.525197\n      1.0\n    \n    \n      abbey\n      3\n      0.0\n      3.0\n      -23.932313\n      -16.407116\n      7.525197\n      1.0\n    \n    \n      abbott\n      3\n      0.0\n      3.0\n      -23.932313\n      -16.407116\n      7.525197\n      1.0\n    \n    \n      abbreviated\n      3\n      3.0\n      0.0\n      -15.698693\n      -24.640735\n      -8.942042\n      -1.0\n    \n    \n      ability\n      27\n      0.0\n      27.0\n      -23.932313\n      -13.241457\n      10.690855\n      1.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      zippy\n      28\n      10.0\n      18.0\n      -13.965086\n      -13.826153\n      0.138933\n      1.0\n    \n    \n      zone\n      4\n      1.0\n      3.0\n      -17.274101\n      -16.407116\n      0.866985\n      1.0\n    \n    \n      zweigelt\n      7\n      5.0\n      2.0\n      -14.963646\n      -16.989684\n      -2.026038\n      -1.0\n    \n    \n      émilion\n      3\n      1.0\n      2.0\n      -17.274101\n      -16.989684\n      0.284418\n      1.0\n    \n    \n      über\n      6\n      3.0\n      3.0\n      -15.698693\n      -16.407116\n      -0.708423\n      -1.0\n    \n  \n\n7254 rows × 7 columns\n\n\n\n\n\nSave\n\nVOCAB.to_csv(\"../data/winereviews/winereviews-SKMNB_SALEX.csv\")"
  },
  {
    "objectID": "lessons/M12_Classification/M12_02_PerceptronWine.html",
    "href": "lessons/M12_Classification/M12_02_PerceptronWine.html",
    "title": "Metadata",
    "section": "",
    "text": "Overview"
  },
  {
    "objectID": "lessons/M12_Classification/M12_02_PerceptronWine.html#config",
    "href": "lessons/M12_Classification/M12_02_PerceptronWine.html#config",
    "title": "Metadata",
    "section": "Config",
    "text": "Config\n\ndata_in = '../data'\ndata_out = '../data'\ndata_prefix = 'winereviews'\n\n\nOHCO = ['doc_id','sent_id','token_id']"
  },
  {
    "objectID": "lessons/M12_Classification/M12_02_PerceptronWine.html#import",
    "href": "lessons/M12_Classification/M12_02_PerceptronWine.html#import",
    "title": "Metadata",
    "section": "Import",
    "text": "Import\n\nimport pandas as pd\nimport numpy as np\nimport lib.textman as tx\nimport plotly_express as px\nfrom sklearn.linear_model import Perceptron"
  },
  {
    "objectID": "lessons/M12_Classification/M12_02_PerceptronWine.html#get-data",
    "href": "lessons/M12_Classification/M12_02_PerceptronWine.html#get-data",
    "title": "Metadata",
    "section": "Get Data",
    "text": "Get Data\n\nVOCAB = pd.read_csv(f\"{data_in}/{data_prefix}/{data_prefix}-VOCAB.csv\").set_index('term_str')\nTOKEN = pd.read_csv(f\"{data_in}/{data_prefix}/{data_prefix}-TOKENS.csv\")\n\n\nDOC_training = pd.read_csv(f\"{data_in}/{data_prefix}/{data_prefix}-DOCS_training.csv\").set_index('doc_id')\nDOC_testing = pd.read_csv(f\"{data_in}/{data_prefix}/{data_prefix}-DOCS_testing.csv\").set_index('doc_id')[['doc_content','doc_label']]\n\n\nBOW_training = pd.read_csv(f\"{data_in}/{data_prefix}/{data_prefix}-BOW_training.csv\").set_index(['doc_id','term_str'])\nBOW_testing = pd.read_csv(f\"{data_in}/{data_prefix}/{data_prefix}-BOW_testing.csv\").set_index(['doc_id','term_str'])\n\n\nDTM = TOKEN.groupby(['doc_id','term_str']).term_str.count().unstack(fill_value=0)"
  },
  {
    "objectID": "lessons/M12_Classification/M12_02_PerceptronWine.html#create-confusion-matrix",
    "href": "lessons/M12_Classification/M12_02_PerceptronWine.html#create-confusion-matrix",
    "title": "Metadata",
    "section": "Create confusion matrix",
    "text": "Create confusion matrix\n\nCM = T.reset_index().groupby(['prediction','doc_label']).doc_id.count().unstack().fillna(0)\nCM.columns.name = 'actual'\n\n\nCM\n\n\n\n\n\n  \n    \n      actual\n      N\n      P\n    \n    \n      prediction\n      \n      \n    \n  \n  \n    \n      N\n      2987\n      61\n    \n    \n      P\n      65\n      2917\n    \n  \n\n\n\n\n\ndef get_results(CM):\n\n    class Results():\n        \n        TP = CM.iloc[0,0] # hits\n        FP = CM.iloc[0,1] # Type I errors; false alarms\n        TN = CM.iloc[1,1] # correct rejections\n        FN = CM.iloc[1,0] # Type  II errors; misses\n        \n        T = TP + TN\n        F = FP + FN\n        ALL =  T + F\n        \n        ACC = T / ALL # Accuracy\n        TPR = TP / (TP + FN) # Recall, Sensitivity, True Positive Rate\n        TNR = TN / (TN + FP) # Specificity, True Negative Rate\n        PPV = TP / (TP + FP)  # Precision; Positive predictive value \n        BA = (TNR + TPR) / 2 # Balanced Accuracy\n        F1 = (2 *  TP) / (2 * TP + FP + FN) # F-score where F =  1\n\n        assert ALL == CM.sum().sum()\n                \n        def show_results(self):\n            print('TPR:', round(self.TPR, 2), '(sensitivity)')\n            print('TNR:', round(self.TNR, 2), '(specificity)')\n            print('F1: ', round(self.F1, 2), '<-- GRADE')\n            print('-'*9)\n            print('PPV:', round(self.PPV, 2),  '(precision)')\n            print('ACC:', round(self.ACC, 2), '(accuracy)')\n            print('BA: ', round(self.BA, 2), '(balanced accuracy)')\n            \n    return Results()\n\n\nCMR = get_results(CM)\n\n\nCMR.show_results()\n\nTPR: 0.98 (sensitivity)\nTNR: 0.98 (specificity)\nF1:  0.98 <-- GRADE\n---------\nPPV: 0.98 (precision)\nACC: 0.98 (accuracy)\nBA:  0.98 (balanced accuracy)"
  },
  {
    "objectID": "lessons/M12_Classification/M12_02_PerceptronWine.html#distribution-of-weights",
    "href": "lessons/M12_Classification/M12_02_PerceptronWine.html#distribution-of-weights",
    "title": "Metadata",
    "section": "Distribution of Weights",
    "text": "Distribution of Weights\n\nWEIGHTS.weight.describe()\n\ncount    8327.000000\nmean        0.603459\nstd         2.427192\nmin       -15.000000\n25%         0.000000\n50%         0.000000\n75%         1.000000\nmax        21.000000\nName: weight, dtype: float64\n\n\n\npx.histogram(WEIGHTS, 'weight')\n\n\n                                                \n\n\n\nWEIGHTS.sort_values('weight', ascending=False).head(20)\n\n\n\n\n\n  \n    \n      \n      weight\n    \n    \n      term_str\n      \n    \n  \n  \n    \n      sample\n      21.0\n    \n    \n      opulent\n      17.0\n    \n    \n      impressive\n      16.0\n    \n    \n      powerful\n      16.0\n    \n    \n      exceptional\n      15.0\n    \n    \n      delicious\n      15.0\n    \n    \n      complex\n      14.0\n    \n    \n      concentrated\n      14.0\n    \n    \n      velvety\n      14.0\n    \n    \n      long\n      14.0\n    \n    \n      wonderfully\n      14.0\n    \n    \n      poised\n      13.0\n    \n    \n      power\n      13.0\n    \n    \n      density\n      13.0\n    \n    \n      compelling\n      13.0\n    \n    \n      luscious\n      13.0\n    \n    \n      memorable\n      12.0\n    \n    \n      excellent\n      12.0\n    \n    \n      elegant\n      12.0\n    \n    \n      beautifully\n      12.0"
  },
  {
    "objectID": "lessons/M12_Classification/M12_02_PerceptronWine.html#top-negative-words",
    "href": "lessons/M12_Classification/M12_02_PerceptronWine.html#top-negative-words",
    "title": "Metadata",
    "section": "Top negative words",
    "text": "Top negative words\n\nWEIGHTS.sort_values('weight', ascending=True).head(20)\n\n\n\n\n\n  \n    \n      \n      weight\n    \n    \n      term_str\n      \n    \n  \n  \n    \n      lacks\n      -15.0\n    \n    \n      simple\n      -14.0\n    \n    \n      vegetal\n      -13.0\n    \n    \n      thin\n      -13.0\n    \n    \n      bitterness\n      -12.0\n    \n    \n      rough\n      -11.0\n    \n    \n      short\n      -11.0\n    \n    \n      awkward\n      -10.0\n    \n    \n      everyday\n      -10.0\n    \n    \n      overall\n      -9.0\n    \n    \n      generic\n      -9.0\n    \n    \n      misses\n      -9.0\n    \n    \n      virginia\n      -9.0\n    \n    \n      adequate\n      -9.0\n    \n    \n      odd\n      -9.0\n    \n    \n      stalky\n      -9.0\n    \n    \n      mild\n      -9.0\n    \n    \n      lacking\n      -9.0\n    \n    \n      flat\n      -8.0\n    \n    \n      decent\n      -8.0\n    \n  \n\n\n\n\n\nwpos = WEIGHTS.weight.sort_values(ascending=False).head(10)\nwneg = WEIGHTS.weight.sort_values(ascending=False).tail(10)\npd.concat([wpos,wneg]).sort_values().plot.barh(figsize=(5, 10));"
  },
  {
    "objectID": "lessons/M12_Classification/M12_03.ExpectedMutualInformation.html",
    "href": "lessons/M12_Classification/M12_03.ExpectedMutualInformation.html",
    "title": "Metadata",
    "section": "",
    "text": "Set Up"
  },
  {
    "objectID": "lessons/M12_Classification/M12_03.ExpectedMutualInformation.html#configure",
    "href": "lessons/M12_Classification/M12_03.ExpectedMutualInformation.html#configure",
    "title": "Metadata",
    "section": "Configure",
    "text": "Configure\n\ndata_in = '../data'\ndata_out = '../data'\ndata_prefix = 'winereviews'"
  },
  {
    "objectID": "lessons/M12_Classification/M12_03.ExpectedMutualInformation.html#import",
    "href": "lessons/M12_Classification/M12_03.ExpectedMutualInformation.html#import",
    "title": "Metadata",
    "section": "Import",
    "text": "Import\n\nimport pandas as pd\nimport numpy as np\nfrom numpy import log2 as log\nfrom numpy import exp2 as exp\nfrom numpy.random import randint\nimport lib.textman as tx"
  },
  {
    "objectID": "lessons/M12_Classification/M12_04_Viz.html",
    "href": "lessons/M12_Classification/M12_04_Viz.html",
    "title": "Metadata",
    "section": "",
    "text": "Set Up"
  },
  {
    "objectID": "lessons/M12_Classification/M12_04_Viz.html#config",
    "href": "lessons/M12_Classification/M12_04_Viz.html#config",
    "title": "Metadata",
    "section": "Config",
    "text": "Config\n\ndata_in = \"../data/winereviews\""
  },
  {
    "objectID": "lessons/M12_Classification/M12_04_Viz.html#import",
    "href": "lessons/M12_Classification/M12_04_Viz.html#import",
    "title": "Metadata",
    "section": "Import",
    "text": "Import\n\nimport pandas as pd\nimport numpy as np\nimport plotly_express as px"
  },
  {
    "objectID": "lessons/M12_Classification/M12_04_Viz.html#add-lexicons-to-vocab",
    "href": "lessons/M12_Classification/M12_04_Viz.html#add-lexicons-to-vocab",
    "title": "Metadata",
    "section": "Add Lexicons to VOCAB",
    "text": "Add Lexicons to VOCAB\n\nsalex_nb = pd.read_csv(f'{data_in}/winereviews-NB_SALEX.csv').set_index('term_str')\nsalex_pt = pd.read_csv(f'{data_in}/winereviews-PERCEPTRON_SALEX.csv').set_index('term_str')\nsalex_mi = pd.read_csv(f'{data_in}/winereviews-MI_SALEX.csv').set_index('term_str')\nsalex_skmnb = pd.read_csv(f'{data_in}/winereviews-SKMNB_SALEX.csv').set_index('term_str')[['valence','polarity']]\nvocab = pd.read_csv(f'{data_in}/winereviews-VOCAB.csv').set_index('term_str')\n\n\nsalex_skmnb\n\n\n\n\n\n  \n    \n      \n      valence\n      polarity\n    \n    \n      term_str\n      \n      \n    \n  \n  \n    \n      aaron\n      7.525197\n      1.0\n    \n    \n      abbey\n      7.525197\n      1.0\n    \n    \n      abbott\n      7.525197\n      1.0\n    \n    \n      abbreviated\n      -8.942042\n      -1.0\n    \n    \n      ability\n      10.690855\n      1.0\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      zippy\n      0.138933\n      1.0\n    \n    \n      zone\n      0.866985\n      1.0\n    \n    \n      zweigelt\n      -2.026038\n      -1.0\n    \n    \n      émilion\n      0.284418\n      1.0\n    \n    \n      über\n      -0.708423\n      -1.0\n    \n  \n\n7254 rows × 2 columns\n\n\n\n\n# salex_nb = salex_nb.drop('n', axis=1)\n# salex_nb.columns = ['nb_polarity','nb_valence','emi']\nsalex_nb.columns = ['nb_valence','nb_polarity']\nsalex_skmnb.columns = ['skmnb_valence','skmnb_polarity']\nsalex_pt.columns = ['perceptron_weight']\nsalex_mi.columns = [f\"mi_{col}\" for col in salex_mi.columns.tolist()]\n\n\nVOCAB = vocab.merge(salex_nb, on='term_str')\\\n    .merge(salex_pt, on='term_str')\\\n    .merge(salex_mi, on='term_str')\\\n    .merge(salex_skmnb, on='term_str')\n\n\nVOCAB\n\n\n\n\n\n  \n    \n      \n      n\n      f\n      stem\n      sw\n      go\n      nb_valence\n      nb_polarity\n      perceptron_weight\n      mi_N\n      mi_P\n      mi_valence\n      mi_polarity\n      skmnb_valence\n      skmnb_polarity\n    \n    \n      term_str\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      aaron\n      4\n      0.000005\n      aaron\n      False\n      True\n      4.639753\n      1.0\n      0.0\n      0.000001\n      0.000343\n      0.000342\n      1.0\n      7.525197\n      1.0\n    \n    \n      abbott\n      3\n      0.000004\n      abbott\n      False\n      True\n      4.236397\n      1.0\n      0.0\n      0.000001\n      0.000217\n      0.000215\n      1.0\n      7.525197\n      1.0\n    \n    \n      abbreviated\n      3\n      0.000004\n      abbrevi\n      False\n      True\n      -5.671996\n      -1.0\n      0.0\n      0.000220\n      0.000001\n      -0.000218\n      -1.0\n      -8.942042\n      -1.0\n    \n    \n      ability\n      32\n      0.000042\n      abil\n      False\n      True\n      4.056389\n      1.0\n      6.0\n      0.000001\n      0.001504\n      0.001502\n      1.0\n      10.690855\n      1.0\n    \n    \n      able\n      12\n      0.000016\n      abl\n      False\n      True\n      2.617185\n      1.0\n      -1.0\n      0.000082\n      0.000405\n      0.000323\n      1.0\n      1.985931\n      1.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      zippy\n      30\n      0.000039\n      zippi\n      False\n      True\n      0.065214\n      1.0\n      4.0\n      0.000765\n      0.001191\n      0.000426\n      1.0\n      0.138933\n      1.0\n    \n    \n      zone\n      4\n      0.000005\n      zone\n      False\n      True\n      0.776965\n      1.0\n      1.0\n      0.000001\n      0.000217\n      0.000215\n      1.0\n      0.866985\n      1.0\n    \n    \n      zweigelt\n      5\n      0.000007\n      zweigelt\n      False\n      True\n      -0.155920\n      -1.0\n      2.0\n      0.000348\n      0.000150\n      -0.000198\n      -1.0\n      -2.026038\n      -1.0\n    \n    \n      émilion\n      4\n      0.000005\n      émilion\n      False\n      True\n      -0.717799\n      -1.0\n      -2.0\n      0.000153\n      0.000081\n      -0.000072\n      -1.0\n      0.284418\n      1.0\n    \n    \n      über\n      7\n      0.000009\n      über\n      False\n      True\n      -1.997907\n      -1.0\n      2.0\n      0.000285\n      0.000217\n      -0.000068\n      -1.0\n      -0.708423\n      -1.0\n    \n  \n\n6674 rows × 14 columns"
  },
  {
    "objectID": "lessons/M12_Classification/M12_04_Viz.html#naive-bayes",
    "href": "lessons/M12_Classification/M12_04_Viz.html#naive-bayes",
    "title": "Metadata",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\ntext('nb_valence')"
  },
  {
    "objectID": "lessons/M12_Classification/M12_04_Viz.html#sk-naive-bayes",
    "href": "lessons/M12_Classification/M12_04_Viz.html#sk-naive-bayes",
    "title": "Metadata",
    "section": "SK Naive Bayes",
    "text": "SK Naive Bayes\n\ntext('skmnb_valence')"
  },
  {
    "objectID": "lessons/M12_Classification/M12_04_Viz.html#perceptron",
    "href": "lessons/M12_Classification/M12_04_Viz.html#perceptron",
    "title": "Metadata",
    "section": "Perceptron",
    "text": "Perceptron\n\ntext('perceptron_weight')"
  },
  {
    "objectID": "lessons/M12_Classification/M12_04_Viz.html#emi",
    "href": "lessons/M12_Classification/M12_04_Viz.html#emi",
    "title": "Metadata",
    "section": "EMI",
    "text": "EMI\n\ntext('mi_valence')"
  },
  {
    "objectID": "lessons/M12_Classification/Untitled.html",
    "href": "lessons/M12_Classification/Untitled.html",
    "title": "Metadata",
    "section": "",
    "text": "Plan"
  },
  {
    "objectID": "lessons/M12_Classification/Untitled.html#get-corpus",
    "href": "lessons/M12_Classification/Untitled.html#get-corpus",
    "title": "Metadata",
    "section": "Get Corpus",
    "text": "Get Corpus\n\nDOC = pd.read_csv(f\"{Params.data_in}/{Params.collection}/{Params.collection}.csv\").set_index('doc_id')"
  },
  {
    "objectID": "lessons/M12_Classification/Untitled.html#filter-out-middle",
    "href": "lessons/M12_Classification/Untitled.html#filter-out-middle",
    "title": "Metadata",
    "section": "Filter out middle",
    "text": "Filter out middle\n\nDOC = DOC.loc[(DOC.points >= Params.a_cut) | (DOC.points <= Params.b_cut)].copy()"
  },
  {
    "objectID": "lessons/M12_Classification/Untitled.html#thunk-ratings-to-labels",
    "href": "lessons/M12_Classification/Untitled.html#thunk-ratings-to-labels",
    "title": "Metadata",
    "section": "Thunk ratings to labels",
    "text": "Thunk ratings to labels\n\nDOC.loc[DOC.points <90, 'label'] = Params.cat_cols[0]\nDOC.loc[DOC.points >=90, 'label'] = Params.cat_cols[1]\n\n\nDOC\n\n\n\n\n\n  \n    \n      \n      doc_content\n      points\n      label\n    \n    \n      doc_id\n      \n      \n      \n    \n  \n  \n    \n      336\n      Gritty, heavily roasted aromas of peanuts and ...\n      83\n      N\n    \n    \n      337\n      An easy and inviting selection, there's a ment...\n      83\n      N\n    \n    \n      338\n      The wine is earthy and somewhat rustic. There ...\n      82\n      N\n    \n    \n      339\n      Red in color, with berry and apple aromas, thi...\n      82\n      N\n    \n    \n      340\n      The nose is muted, despite the slight spritz o...\n      82\n      N\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      129678\n      There's a stunning amount of action on the nos...\n      93\n      P\n    \n    \n      129679\n      From a ranch planted originally in 1926 this i...\n      93\n      P\n    \n    \n      129680\n      Old vines in a field blend give a rich structu...\n      93\n      P\n    \n    \n      129681\n      Ripe black-skinned berry, violet, leather and ...\n      93\n      P\n    \n    \n      129682\n      After helping Liquid Farm launch to Chardonnay...\n      93\n      P\n    \n  \n\n24270 rows × 3 columns"
  },
  {
    "objectID": "lessons/M12_Classification/Untitled.html#convert-docs-to-count-matrix",
    "href": "lessons/M12_Classification/Untitled.html#convert-docs-to-count-matrix",
    "title": "Metadata",
    "section": "Convert docs to count matrix",
    "text": "Convert docs to count matrix\n\n# CountVectorizer?\n\n\ncount_engine = CountVectorizer(stop_words='english', ngram_range=Params.ngram_range, token_pattern=Params.token_pattern, min_df=Params.min_df)\n\n\ncount_model = count_engine.fit_transform(DOC.doc_content)\n\n\nDTM = pd.DataFrame(count_model.toarray(), columns=count_engine.get_feature_names_out(), index=DOC.index)\n\n\nDTM.sample(10)\n\n\n\n\n\n  \n    \n      \n      aaron\n      ability\n      ability age\n      able\n      abound\n      abound nose\n      abounds\n      abrasive\n      abrupt\n      abrupt finish\n      ...\n      zing\n      zingy\n      zingy acidity\n      zinny\n      zins\n      zip\n      zippy\n      zippy acidity\n      zone\n      zweigelt\n    \n    \n      doc_id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      97831\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      102041\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      87622\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      82725\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      24754\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      37369\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      6743\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      6299\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      47848\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      34970\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n10 rows × 23072 columns\n\n\n\n\nVOCAB = DTM.sum().to_frame('n')\nVOCAB.index.name = 'term_str'\n\n\nVOCAB\n\n\n\n\n\n  \n    \n      \n      n\n    \n    \n      term_str\n      \n    \n  \n  \n    \n      aaron\n      5\n    \n    \n      ability\n      39\n    \n    \n      ability age\n      10\n    \n    \n      able\n      16\n    \n    \n      abound\n      34\n    \n    \n      ...\n      ...\n    \n    \n      zip\n      15\n    \n    \n      zippy\n      38\n    \n    \n      zippy acidity\n      11\n    \n    \n      zone\n      5\n    \n    \n      zweigelt\n      9\n    \n  \n\n23072 rows × 1 columns"
  },
  {
    "objectID": "lessons/M12_Classification/Untitled.html#split-data",
    "href": "lessons/M12_Classification/Untitled.html#split-data",
    "title": "Metadata",
    "section": "Split data",
    "text": "Split data\n\nX_train, X_test, y_train, y_test = train_test_split(DTM, DOC.label, test_size=Params.test_size, random_state=0)\n\n\nRESULTS = pd.DataFrame({'actual': y_test}, index=X_test.index)\n\n\nRESULTS\n\n\n\n\n\n  \n    \n      \n      actual\n    \n    \n      doc_id\n      \n    \n  \n  \n    \n      33313\n      N\n    \n    \n      124580\n      N\n    \n    \n      127027\n      N\n    \n    \n      100482\n      N\n    \n    \n      61771\n      N\n    \n    \n      ...\n      ...\n    \n    \n      112450\n      P\n    \n    \n      40342\n      P\n    \n    \n      112684\n      P\n    \n    \n      57579\n      P\n    \n    \n      93789\n      N\n    \n  \n\n6068 rows × 1 columns"
  },
  {
    "objectID": "lessons/M12_Classification/Untitled.html#naive-bayes",
    "href": "lessons/M12_Classification/Untitled.html#naive-bayes",
    "title": "Metadata",
    "section": "Naive Bayes",
    "text": "Naive Bayes\nCreate BOW\n\nBOW = DTM.stack().replace(0, np.nan).dropna().to_frame('n')\nBOW.index.names = ['doc_id', 'term_str']\n\n\nBOW_training = BOW.loc[X_train.index]\\\n    .join(DOC.loc[X_train.index, ['label']], on='doc_id', how='inner')\nBOW_test = BOW.loc[X_test.index]\n\nEstimate Model\n\nPRIORS = np.log2(DOC.loc[X_test.index, 'label'].value_counts(normalize=True))\n\n\n# PRIORS\n\n\nDOC_CLASS = BOW_training.groupby(['term_str', 'label']).label.count()\\\n    .unstack(fill_value=0) + Params.smooth_alpha\nLLS = np.log2(DOC_CLASS / DOC_CLASS.sum())\n\n\n# LLS.sample(10)\n\n\nRESULTS['nb'] = (BOW_test.join(LLS)[Params.cat_cols].groupby('doc_id').sum() + PRIORS).idxmax(axis=1)\n\n\nRESULTS\n\n\n\n\n\n  \n    \n      \n      actual\n      nb\n    \n    \n      doc_id\n      \n      \n    \n  \n  \n    \n      33313\n      N\n      N\n    \n    \n      124580\n      N\n      N\n    \n    \n      127027\n      N\n      N\n    \n    \n      100482\n      N\n      N\n    \n    \n      61771\n      N\n      N\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      112450\n      P\n      P\n    \n    \n      40342\n      P\n      P\n    \n    \n      112684\n      P\n      P\n    \n    \n      57579\n      P\n      P\n    \n    \n      93789\n      N\n      N\n    \n  \n\n6068 rows × 2 columns"
  },
  {
    "objectID": "lessons/M12_Classification/Untitled.html#sk-naive-bayes",
    "href": "lessons/M12_Classification/Untitled.html#sk-naive-bayes",
    "title": "Metadata",
    "section": "SK Naive Bayes",
    "text": "SK Naive Bayes\n\nmnb_engine = MultinomialNB(force_alpha=True)\n\n\nmnb_model = mnb_engine.fit(X_train, y_train)\n\n\nRESULTS['mnb'] = pd.DataFrame(mnb_engine.predict(X_test), index=X_test.index)\n\n\nRESULTS\n\n\n\n\n\n  \n    \n      \n      actual\n      nb\n      mnb\n    \n    \n      doc_id\n      \n      \n      \n    \n  \n  \n    \n      33313\n      N\n      N\n      N\n    \n    \n      124580\n      N\n      N\n      N\n    \n    \n      127027\n      N\n      N\n      N\n    \n    \n      100482\n      N\n      N\n      N\n    \n    \n      61771\n      N\n      N\n      N\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      112450\n      P\n      P\n      P\n    \n    \n      40342\n      P\n      P\n      P\n    \n    \n      112684\n      P\n      P\n      P\n    \n    \n      57579\n      P\n      P\n      P\n    \n    \n      93789\n      N\n      N\n      N\n    \n  \n\n6068 rows × 3 columns"
  },
  {
    "objectID": "lessons/M12_Classification/Untitled.html#sk-perceptron",
    "href": "lessons/M12_Classification/Untitled.html#sk-perceptron",
    "title": "Metadata",
    "section": "SK Perceptron",
    "text": "SK Perceptron\n\npercep_engine = Perceptron(tol=1e-3, random_state=0)\npercep_model = percep_engine.fit(X_train, y_train)\n\n\nRESULTS['percep'] = pd.DataFrame(percep_engine.predict(X_test), index=X_test.index)\n\n\nRESULTS.sample(10)\n\n\n\n\n\n  \n    \n      \n      actual\n      nb\n      mnb\n      percep\n    \n    \n      doc_id\n      \n      \n      \n      \n    \n  \n  \n    \n      51386\n      P\n      P\n      P\n      P\n    \n    \n      58353\n      P\n      P\n      P\n      P\n    \n    \n      22742\n      N\n      N\n      N\n      P\n    \n    \n      13147\n      N\n      N\n      N\n      N\n    \n    \n      57605\n      P\n      P\n      P\n      P\n    \n    \n      69562\n      N\n      N\n      N\n      N\n    \n    \n      127598\n      N\n      N\n      N\n      N\n    \n    \n      109423\n      N\n      N\n      N\n      N\n    \n    \n      3842\n      P\n      P\n      P\n      P\n    \n    \n      88477\n      N\n      N\n      N\n      N"
  },
  {
    "objectID": "lessons/M13_LLMs/M13_01_ChatGPTDemo.html",
    "href": "lessons/M13_LLMs/M13_01_ChatGPTDemo.html",
    "title": "Metadata",
    "section": "",
    "text": "Course:    DS 5001\nModule:    13 LLMs\nTopic:     Demonstrate ChatGPT\nAuthor:    R.C. Alvarado\nDate:      27 April 2023\n\nPrompt 1\nWrite a Python script to scrape Jane’s Austen’s Persuasion from Project Gutenberg.\n\nSure, here’s a Python script using the requests and BeautifulSoup libraries to scrape Jane Austen’s “Persuasion” from Project Gutenberg:\n\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n# The URL for Jane Austen's Persuasion on Project Gutenberg\nurl = \"https://www.gutenberg.org/files/105/105-h/105-h.htm\"\n\n# Send an HTTP GET request to the URL and get the response\nresponse = requests.get(url)\n\n# Use BeautifulSoup to parse the HTML content of the response\nsoup = BeautifulSoup(response.content, \"html.parser\")\n\n# Find the main content of the book, which is contained in the \"pre\" tag\nbook_content = soup.find(\"pre\").get_text()\n\n# Print the book content\nprint(book_content)\n\nAttributeError: 'NoneType' object has no attribute 'get_text'\n\n\n\nThis script will print the entire text of “Persuasion” in the console when run. You can also modify it to save the text to a file or perform further text processing.\n\n\n\nPrompt 2\nCan you fix that code? When I run it, it throws an error that says “AttributeError: ‘NoneType’ object has no attribute ‘get_text’”\n\nI apologize for the mistake. Here’s an updated code that should work:\n\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n# The URL for Jane Austen's Persuasion on Project Gutenberg\nurl = \"https://www.gutenberg.org/files/105/105-h/105-h.htm\"\n\n# Send an HTTP GET request to the URL and get the response\nresponse = requests.get(url)\n\n# Use BeautifulSoup to parse the HTML content of the response\nsoup = BeautifulSoup(response.content, \"html.parser\")\n\n# Find the main content of the book, which is contained in the \"body\" tag\nbook_content = soup.find(\"body\").get_text()\n\n# Print the book content\n# print(book_content)\n\n\nThis code finds the main content of the book in the body tag instead of the pre tag, which was causing the error. I hope this helps!\n\n\n\nPrompt 3\nNow I would like you to parse the text into chapter segments. In other words, convert the raw text into a list of chapter strings.\n\nSure, here’s an updated code that parses the text into chapter segments:\n\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n# The URL for Jane Austen's Persuasion on Project Gutenberg\nurl = \"https://www.gutenberg.org/files/105/105-h/105-h.htm\"\n\n# Send an HTTP GET request to the URL and get the response\nresponse = requests.get(url)\n\n# Use BeautifulSoup to parse the HTML content of the response\nsoup = BeautifulSoup(response.content, \"html.parser\")\n\n# Find the main content of the book, which is contained in the \"body\" tag\nbody = soup.find(\"body\")\n\n# Find all the <h2> tags in the body, which mark the beginning of a chapter\nchapter_tags = body.find_all(\"h2\")\n\n# Initialize an empty list to store the chapter strings\nchapters = []\n\n# Loop through each <h2> tag and extract the text of the chapter\nfor i in range(len(chapter_tags)):\n    # If this is not the last chapter, get the text of the chapter by slicing the body content\n    if i != len(chapter_tags) - 1:\n        chapter_text = body[chapter_tags[i].name:chapter_tags[i+1].name].get_text()\n    # If this is the last chapter, get the text of the chapter from the last <h2> tag to the end of the content\n    else:\n        chapter_text = body[chapter_tags[i].name:].get_text()\n    \n    # Add the chapter text to the list of chapters\n    chapters.append(chapter_text)\n\n# Print the list of chapters\nprint(chapters)\n\nTypeError: unhashable type: 'slice'\n\n\n\nThis code finds all the <h2> tags in the text and uses them as markers for the beginning of each chapter. It then loops through each <h2> tag and extracts the text of the chapter by slicing the content between that tag and the next one (or the end of the content for the last chapter). The chapter strings are then added to a list, which is printed at the end of the script.\n\n\n\nPrompt 4\nCan you please fix that code? I get an error: “TypeError: unhashable type: ‘slice’”\n\nI apologize for that mistake. The issue is that the slice object cannot be used as a dictionary key. Here’s an updated code that should work:\n\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n# The URL for Jane Austen's Persuasion on Project Gutenberg\nurl = \"https://www.gutenberg.org/files/105/105-h/105-h.htm\"\n\n# Send an HTTP GET request to the URL and get the response\nresponse = requests.get(url)\n\n# Use BeautifulSoup to parse the HTML content of the response\nsoup = BeautifulSoup(response.content, \"html.parser\")\n\n# Find the main content of the book, which is contained in the \"body\" tag\nbody = soup.find(\"body\")\n\n# Find all the <h2> tags in the body, which mark the beginning of a chapter\nchapter_tags = body.find_all(\"h2\")\n\n# Initialize an empty list to store the chapter strings\nchapters = []\n\n# Loop through each <h2> tag and extract the text of the chapter\nfor i in range(len(chapter_tags)):\n    # If this is not the last chapter, get the text of the chapter by slicing the body content\n    if i != len(chapter_tags) - 1:\n        chapter_text = body.find_all()[body.find_all().index(chapter_tags[i])+1:body.find_all().index(chapter_tags[i+1])]\n    # If this is the last chapter, get the text of the chapter from the last <h2> tag to the end of the content\n    else:\n        chapter_text = body.find_all()[body.find_all().index(chapter_tags[i])+1:]\n        \n    # Convert the chapter text list to a string\n    chapter_string = \"\\n\".join([str(tag) for tag in chapter_text])\n    \n    # Add the chapter text to the list of chapters\n    chapters.append(chapter_string)\n\n\n# Print the list of chapters\n# print(chapters)\n\n\nIn this updated code, instead of slicing the body content using slice objects, I’m using the find_all() method to get a list of all tags in the body content and then finding the index of the current chapter tag and the next chapter tag (or the end of the content for the last chapter). I’m then slicing the list of tags using these indices to get the tags between the current and next chapter tags (or the end of the content for the last chapter). Finally, I’m converting the list of tags to a string using a list comprehension and the join() method before appending it to the list of chapters.\n\n\n\nPrompt 5\nThat’s great work! Can you make it so that it also parses the paragraphs in each chapter, so that the result is a Pandas dataframe with a multiindex that contains the chapter number and the paragraph number for each row?\n\nSure, here’s an updated code that should parse the text into paragraphs and return a Pandas dataframe with a multiindex:\n\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# The URL for Jane Austen's Persuasion on Project Gutenberg\nurl = \"https://www.gutenberg.org/files/105/105-h/105-h.htm\"\n\n# Send an HTTP GET request to the URL and get the response\nresponse = requests.get(url)\n\n# Use BeautifulSoup to parse the HTML content of the response\nsoup = BeautifulSoup(response.content, \"html.parser\")\n\n# Find the main content of the book, which is contained in the \"body\" tag\nbody = soup.find(\"body\")\n\n# Find all the <h2> tags in the body, which mark the beginning of a chapter\nchapter_tags = body.find_all(\"h2\")\n\n# Initialize empty lists to store the chapter and paragraph numbers and the text of each paragraph\nchapter_numbers = []\nparagraph_numbers = []\nparagraphs = []\n\n# Loop through each <h2> tag and extract the text of the chapter and its paragraphs\nfor i in range(len(chapter_tags)):\n    # If this is not the last chapter, get the text of the chapter and its paragraphs by slicing the body content\n    if i != len(chapter_tags) - 1:\n        chapter_text = body.find_all()[body.find_all().index(chapter_tags[i])+1:body.find_all().index(chapter_tags[i+1])]\n    # If this is the last chapter, get the text of the chapter and its paragraphs from the last <h2> tag to the end of the content\n    else:\n        chapter_text = body.find_all()[body.find_all().index(chapter_tags[i])+1:]\n    \n    # Loop through each tag in the chapter text and extract its text as a paragraph\n    for j, tag in enumerate(chapter_text):\n        # Skip any non-<p> tags\n        if tag.name != \"p\":\n            continue\n        # Add the chapter and paragraph numbers and the text of the paragraph to the respective lists\n        chapter_numbers.append(i+1)\n        paragraph_numbers.append(j+1)\n        paragraphs.append(tag.get_text())\n        \n# Create a Pandas dataframe with a multiindex containing the chapter and paragraph numbers\ndf = pd.DataFrame(paragraphs, index=pd.MultiIndex.from_arrays([chapter_numbers, paragraph_numbers], names=[\"Chapter\", \"Paragraph\"]), columns=[\"Text\"])\n\n# Print the dataframe\nprint(df)\n\n                                                                Text\nChapter Paragraph                                                   \n3       2          \\r\\nSir Walter Elliot, of Kellynch Hall, in So...\n        3                          \\r\\n“ELLIOT OF KELLYNCH HALL.\\r\\n\n        4          \\r\\n“Walter Elliot, born March 1, 1760, marrie...\n        5          \\r\\nPrecisely such had the paragraph originall...\n        6          \\r\\nThen followed the history and rise of the ...\n...                                                              ...\n26      10         \\r\\nIt cannot be doubted that Sir Walter and E...\n        11         \\r\\nAnne, satisfied at a very early period of ...\n        12         \\r\\nHer recent good offices by Anne had been e...\n        13         \\r\\nMrs Smith’s enjoyments were not spoiled by...\n        14                                             \\r\\nFinis\\r\\n\n\n[1011 rows x 1 columns]\n\n\n\nThis code extracts the text of each paragraph in each chapter by looping through the tags in the chapter text and skipping any non-<p> tags. It then appends the chapter and paragraph numbers and the text of each paragraph to their respective lists. Finally, it creates a Pandas dataframe with a multiindex containing the chapter and paragraph numbers, and prints the dataframe.\n\n\n# df\n\n\n\nPrompt 6\nThat is so awesome! You are the best. Can you make it so that it not only gets paragraphs, but also sentences and tokens? This would mean that that index would have columns for chapter, paragraph, sentence, and token.\n\nSure thing! Here’s an updated code that should parse the text into sentences and tokens as well, and return a Pandas dataframe with a multiindex containing the chapter, paragraph, sentence, and token numbers:\n\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport nltk\n\n# The URL for Jane Austen's Persuasion on Project Gutenberg\nurl = \"https://www.gutenberg.org/files/105/105-h/105-h.htm\"\n\n# Send an HTTP GET request to the URL and get the response\nresponse = requests.get(url)\n\n# Use BeautifulSoup to parse the HTML content of the response\nsoup = BeautifulSoup(response.content, \"html.parser\")\n\n# Find the main content of the book, which is contained in the \"body\" tag\nbody = soup.find(\"body\")\n\n# Find all the <h2> tags in the body, which mark the beginning of a chapter\nchapter_tags = body.find_all(\"h2\")\n\n# Initialize empty lists to store the chapter, paragraph, sentence, and token numbers, and the text of each token\nchapter_numbers = []\nparagraph_numbers = []\nsentence_numbers = []\ntoken_numbers = []\ntokens = []\n\n# Loop through each <h2> tag and extract the text of the chapter, paragraph, sentence, and tokens\nfor i in range(len(chapter_tags)):\n    # If this is not the last chapter, get the text of the chapter and its paragraphs by slicing the body content\n    if i != len(chapter_tags) - 1:\n        chapter_text = body.find_all()[body.find_all().index(chapter_tags[i])+1:body.find_all().index(chapter_tags[i+1])]\n    # If this is the last chapter, get the text of the chapter and its paragraphs from the last <h2> tag to the end of the content\n    else:\n        chapter_text = body.find_all()[body.find_all().index(chapter_tags[i])+1:]\n    \n    # Loop through each tag in the chapter text and extract its text as a token\n    for j, tag in enumerate(chapter_text):\n        # Skip any non-<p> tags\n        if tag.name != \"p\":\n            continue\n        # Tokenize the text of the paragraph into sentences\n        sentences = nltk.sent_tokenize(tag.get_text())\n        # Loop through each sentence and tokenize it into words\n        for k, sentence in enumerate(sentences):\n            words = nltk.word_tokenize(sentence)\n            # Loop through each word and add its chapter, paragraph, sentence, and token numbers and its text to the respective lists\n            for l, word in enumerate(words):\n                chapter_numbers.append(i+1)\n                paragraph_numbers.append(j+1)\n                sentence_numbers.append(k+1)\n                token_numbers.append(l+1)\n                tokens.append(word)\n        \n# Create a Pandas dataframe with a multiindex containing the chapter, paragraph, sentence, and token numbers\ndf = pd.DataFrame(tokens, index=pd.MultiIndex.from_arrays([chapter_numbers, paragraph_numbers, sentence_numbers, token_numbers], names=[\"Chapter\", \"Paragraph\", \"Sentence\", \"Token\"]), columns=[\"Text\"])\n\n# Print the dataframe\nprint(df)\n\n                                        Text\nChapter Paragraph Sentence Token            \n3       2         1        1             Sir\n                           2          Walter\n                           3          Elliot\n                           4               ,\n                           5              of\n...                                      ...\n26      13        6        39            its\n                           40       national\n                           41     importance\n                           42              .\n        14        1        1           Finis\n\n[98205 rows x 1 columns]\n\n\n\nThis code tokenizes each paragraph into sentences using the nltk.sent_tokenize() function, and then tokenizes each sentence into words using the nltk.word_tokenize() function. It then loops through each word and adds its chapter, paragraph, sentence, and token numbers and its text to their respective lists. Finally, it creates a Pandas dataframe with a multiindex containing the chapter, paragraph, sentence, and token numbers, and prints the dataframe.\n\n\n\nPrompt 7\nExcellent! Now generalize this into a class that can work with pretty much any Project Gutenberg novel.\n\nSure! Here’s a class called ProjectGutenbergNovel that can be used to parse the text of any Project Gutenberg novel into a Pandas dataframe with a multiindex containing the chapter, paragraph, sentence, and token numbers:\n\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport nltk\n\nclass ProjectGutenbergNovel:\n    def __init__(self, url):\n        self.url = url\n        self.response = requests.get(url)\n        self.soup = BeautifulSoup(self.response.content, \"html.parser\")\n        self.body = self.soup.find(\"body\")\n        self.chapter_tags = self.body.find_all(\"h2\")\n        \n    def parse_text(self):\n        chapter_numbers = []\n        paragraph_numbers = []\n        sentence_numbers = []\n        token_numbers = []\n        tokens = []\n        \n        for i in range(len(self.chapter_tags)):\n            if i != len(self.chapter_tags) - 1:\n                chapter_text = self.body.find_all()[self.body.find_all().index(self.chapter_tags[i])+1:self.body.find_all().index(self.chapter_tags[i+1])]\n            else:\n                chapter_text = self.body.find_all()[self.body.find_all().index(self.chapter_tags[i])+1:]\n            \n            for j, tag in enumerate(chapter_text):\n                if tag.name != \"p\":\n                    continue\n                sentences = nltk.sent_tokenize(tag.get_text())\n                for k, sentence in enumerate(sentences):\n                    words = nltk.word_tokenize(sentence)\n                    for l, word in enumerate(words):\n                        chapter_numbers.append(i+1)\n                        paragraph_numbers.append(j+1)\n                        sentence_numbers.append(k+1)\n                        token_numbers.append(l+1)\n                        tokens.append(word)\n        \n        df = pd.DataFrame(tokens, index=pd.MultiIndex.from_arrays([chapter_numbers, paragraph_numbers, sentence_numbers, token_numbers], names=[\"Chapter\", \"Paragraph\", \"Sentence\", \"Token\"]), columns=[\"Text\"])\n        return df\n\n\nThis class has an init method that takes a URL for a Project Gutenberg novel and stores the response and soup objects for the HTML content of the novel, as well as the chapter tags. It also has a parse_text method that tokenizes the text of the novel into paragraphs, sentences, and tokens, and returns a Pandas dataframe with a multiindex containing the chapter, paragraph, sentence, and token numbers.\n\n\nTo use this class, simply create an instance of it with the URL of the Project Gutenberg novel you want to parse, and then call the parse_text method on that instance. For example:\n\n\n# URL for Jane Austen's Persuasion on Project Gutenberg\nurl = \"https://www.gutenberg.org/files/105/105-h/105-h.htm\"\n\n# Create an instance of the ProjectGutenbergNovel class with the URL for Persuasion\npersuasion = ProjectGutenbergNovel(url)\n\n# Parse the text of Persuasion into a Pandas dataframe with a multiindex containing the chapter, paragraph, sentence, and token numbers\ndf = persuasion.parse_text()\n\n# Print the dataframe\nprint(df)\n\n                                        Text\nChapter Paragraph Sentence Token            \n3       2         1        1             Sir\n                           2          Walter\n                           3          Elliot\n                           4               ,\n                           5              of\n...                                      ...\n26      13        6        39            its\n                           40       national\n                           41     importance\n                           42              .\n        14        1        1           Finis\n\n[98205 rows x 1 columns]\n\n\n\n\nPrompt 8\nThis is amazing. But I forgot to ask for two more things. Can you make it so that I only need to pass the Project Gutenberg ID for each text? And, also, can you detect the part of speech for each token and add that as a feature to the resulting dataframe?\n\nSure, I can modify the class to take only the Project Gutenberg ID and to detect the part of speech for each token. Here’s the updated class:\n\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport nltk\n\nclass ProjectGutenbergNovel:\n    def __init__(self, pg_id):\n        self.url = f\"https://www.gutenberg.org/files/{pg_id}/{pg_id}-h/{pg_id}-h.htm\"\n        self.response = requests.get(self.url)\n        self.soup = BeautifulSoup(self.response.content, \"html.parser\")\n        self.body = self.soup.find(\"body\")\n        self.chapter_tags = self.body.find_all(\"h2\")\n        self.nltk_download()\n        \n    def nltk_download(self):\n        try:\n            nltk.data.find(\"tokenizers/punkt\")\n        except LookupError:\n            nltk.download(\"punkt\")\n        try:\n            nltk.data.find(\"taggers/averaged_perceptron_tagger\")\n        except LookupError:\n            nltk.download(\"averaged_perceptron_tagger\")\n        \n    def parse_text(self):\n        chapter_numbers = []\n        paragraph_numbers = []\n        sentence_numbers = []\n        token_numbers = []\n        tokens = []\n        pos_tags = []\n        \n        for i in range(len(self.chapter_tags)):\n            if i != len(self.chapter_tags) - 1:\n                chapter_text = self.body.find_all()[self.body.find_all().index(self.chapter_tags[i])+1:self.body.find_all().index(self.chapter_tags[i+1])]\n            else:\n                chapter_text = self.body.find_all()[self.body.find_all().index(self.chapter_tags[i])+1:]\n            \n            for j, tag in enumerate(chapter_text):\n                if tag.name != \"p\":\n                    continue\n                sentences = nltk.sent_tokenize(tag.get_text())\n                for k, sentence in enumerate(sentences):\n                    words = nltk.word_tokenize(sentence)\n                    tagged_words = nltk.pos_tag(words)\n                    for l, (word, pos) in enumerate(tagged_words):\n                        chapter_numbers.append(i+1)\n                        paragraph_numbers.append(j+1)\n                        sentence_numbers.append(k+1)\n                        token_numbers.append(l+1)\n                        tokens.append(word)\n                        pos_tags.append(pos)\n        \n        df = pd.DataFrame({\"Text\": tokens, \"POS\": pos_tags}, index=pd.MultiIndex.from_arrays([chapter_numbers, paragraph_numbers, sentence_numbers, token_numbers], names=[\"Chapter\", \"Paragraph\", \"Sentence\", \"Token\"]))\n        return df\n\n\nThis class has an __init__ method that takes the Project Gutenberg ID and constructs the URL for the novel using the ID. It also downloads the necessary NLTK resources and stores the chapter tags for the novel. The parse_text method tokenizes the text into paragraphs, sentences, and tokens, detects the part of speech for each token, and returns a Pandas dataframe with a multiindex containing the chapter, paragraph, sentence, and token numbers, as well as the text and part of speech of each token.\n\n\nTo use this updated class, simply create an instance of it with the Project Gutenberg ID for the novel you want to parse, and then call the parse_text method on that instance. For example:\n\n\n# Project Gutenberg ID for Jane Austen's Persuasion\npg_id = 105\n\n# Create an instance of the ProjectGutenbergNovel class with the Project Gutenberg ID for Persuasion\npersuasion = ProjectGutenbergNovel(pg_id)\n\n# Parse the text of Persuasion into a Pandas dataframe with a multiindex containing the chapter, paragraph, sentence, and token numbers, as well as the part of speech of each token\ndf = persuasion.parse_text()\n\n# Print the dataframe\nprint(df)\n\n                                        Text   POS\nChapter Paragraph Sentence Token                  \n3       2         1        1             Sir   NNP\n                           2          Walter   NNP\n                           3          Elliot   NNP\n                           4               ,     ,\n                           5              of    IN\n...                                      ...   ...\n26      13        6        39            its  PRP$\n                           40       national    JJ\n                           41     importance    NN\n                           42              .     .\n        14        1        1           Finis    NN\n\n[98205 rows x 2 columns]\n\n\n\nthis\n\n\n\nPrompt 9\nSuperb! One more thing: Create a new feature that normalizes the token string into a term string with no capitalization and no non-alphanumeric characters. Also, extract the unique list of terms into an new dataframe called VOCAB that also contains the counts of each term string, as well as the part of speech the term is most frequently associated with. Can you do that? I know it’s a lot to ask.\n\nSure, I can do that! Here’s an updated code with the additional features:\n\n\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nimport re\n\n\nclass ProjectGutenbergText:\n\n    def __init__(self, pg_id):\n        self.pg_id = pg_id\n        self.url = f'http://www.gutenberg.org/files/{pg_id}/{pg_id}-0.txt'\n\n    def __repr__(self):\n        return f'ProjectGutenbergText(pg_id={self.pg_id})'\n\n    def download(self):\n        response = requests.get(self.url)\n        response.encoding = 'utf-8'\n        return response.text\n\n    def get_chapters(self):\n        text = self.download()\n        soup = BeautifulSoup(text, 'html.parser')\n        chapter_tags = soup.find_all('h2')\n        chapters = []\n        for i, tag in enumerate(chapter_tags):\n            chapter = tag.get_text().strip()\n            start = tag\n            end = chapter_tags[i+1] if i+1 < len(chapter_tags) else None\n            chapter_text = ''\n            for element in start.next_elements:\n                if end and element == end:\n                    break\n                if element.name == 'h2':\n                    break\n                if element.name in ['p', 'br']:\n                    chapter_text += str(element)\n            chapters.append(chapter_text)\n        return chapters\n\n    def get_dataframe(self):\n        chapters = self.get_chapters()\n        data = []\n        stop_words = set(stopwords.words('english'))\n        for i, chapter in enumerate(chapters):\n            chapter_num = i+1\n            paragraphs = chapter.split('<p>')\n            for j, paragraph in enumerate(paragraphs):\n                if len(paragraph) == 0:\n                    continue\n                sentences = sent_tokenize(paragraph)\n                for k, sentence in enumerate(sentences):\n                    tokens = word_tokenize(sentence.lower())\n                    tokens = [re.sub(r'[^a-z0-9]', '', token) for token in tokens]\n                    tokens = [token for token in tokens if token not in stop_words and len(token) > 0]\n                    pos_tags = nltk.pos_tag(tokens)\n                    term_pos_tags = [(re.sub(r'[^a-z0-9]', '', token), pos) for token, pos in pos_tags]\n                    for term, pos in term_pos_tags:\n                        data.append((chapter_num, j+1, k+1, term, pos))\n        df = pd.DataFrame(data, columns=['Chapter', 'Paragraph', 'Sentence', 'Token', 'POS'])\n        df['Term'] = df['Token'].apply(lambda x: re.sub(r'[^a-z0-9]', '', x.lower()))\n        vocab_df = df.groupby(['Term', 'POS']).size().reset_index(name='Count')\n        vocab_df = vocab_df.sort_values(by='Count', ascending=False).reset_index(drop=True)\n        return df, vocab_df\n\n\nThis code creates a ProjectGutenbergText class that takes a Project Gutenberg ID as input and returns a Pandas dataframe with the following columns: ‘Chapter’, ‘Paragraph’, ‘Sentence’, ‘Token’, ‘POS’, and ‘Term’. The ‘Term’ column contains the normalized version of the ‘Token’ column with no capitalization and no non-alphanumeric characters.\n\n\nAdditionally, this code also creates a vocab_df dataframe that contains the unique list of terms with their counts and the part of speech they are most frequently associated with.\n\n\nTo use the code, simply instantiate the ProjectGutenbergText class with a Project Gutenberg ID and call the get_dataframe() method. Here\n\n\n\nPrompt 10\nCan you give me an example script using the class you just created?\n\nSure, here’s an example script that uses the ProjectGutenberg class to parse Jane Austen’s “Pride and Prejudice”:\n\n\nfrom gutenberg import ProjectGutenberg\n\npg = ProjectGutenberg()\n\n# Get the text of \"Pride and Prejudice\" (ID 1342)\ntext = pg.get_text(1342)\n\n# Parse the text into a Pandas dataframe with chapter, paragraph, sentence, and token columns\ndf = pg.parse_text(text)\n\n# Add a normalized term column to the dataframe\ndf['term'] = df['token'].apply(lambda x: pg.normalize_term(x))\n\n# Create a vocabulary dataframe with term counts and most common part of speech\nvocab_df = pg.create_vocab(df)\n\nprint(df.head())\nprint(vocab_df.head())\n\nModuleNotFoundError: No module named 'gutenberg'\n\n\n\nThis script will print the first few rows of the parsed dataframe and the first few rows of the vocabulary dataframe.\n\n\n\nPrompt 11\nI tried running that and I get the error “ModuleNotFoundError: No module named ‘gutenberg’”. And when I tried to install gutenberg with pip, I got an error “Can’t find a local Berkeley DB installation.” I I tried installing that with brew (since I have a Mac) but that did not work. Do you have any advice on how I can fix this problem?\n\nIt sounds like the gutenberg module is not installed on your machine. To install it, you can try running the following command:\n\n\npip install gutenbergpy\n\nRequirement already satisfied: gutenbergpy in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (0.3.5)\nRequirement already satisfied: lxml>=3.2.0 in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from gutenbergpy) (4.9.2)\nRequirement already satisfied: httpsproxy-urllib2 in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from gutenbergpy) (1.0)\nRequirement already satisfied: pymongo in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from gutenbergpy) (3.11.4)\nRequirement already satisfied: future>=0.15.2 in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from gutenbergpy) (0.18.2)\nRequirement already satisfied: chardet in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from gutenbergpy) (4.0.0)\nRequirement already satisfied: setuptools>=18.5 in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from gutenbergpy) (65.6.3)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nThis should install gutenbergpy, which is a Python wrapper for the Project Gutenberg corpus that doesn’t require a Berkeley DB installation.\n\n\nOnce you’ve installed gutenbergpy, you can modify the previous script like this:\n\n\nfrom gutenberg.acquire import load_etext\nfrom gutenberg.cleanup import strip_headers\nfrom gutenberg.query import get_metadata\n\nfrom project_gutenberg import ProjectGutenberg\n\npg = ProjectGutenberg()\n\n# Get the text of \"Pride and Prejudice\" (ID 1342)\ntext = strip_headers(load_etext(1342)).strip()\n\n# Parse the text into a Pandas dataframe with chapter, paragraph, sentence, and token columns\ndf = pg.parse_text(text)\n\n# Add a normalized term column to the dataframe\ndf['term'] = df['token'].apply(lambda x: pg.normalize_term(x))\n\n# Create a vocabulary dataframe with term counts and most common part of speech\nvocab_df = pg.create_vocab(df)\n\nprint(df.head())\nprint(vocab_df.head())\n\nModuleNotFoundError: No module named 'gutenberg'\n\n\n\nThis should allow you to run the script without any issues.\n\n\n\nPrompt 12\nThis is weird. You are referring to a class called ProjectGutenberg, but you actually created one called ProjectGutenbergText. Also, are you assuming that I have saved the class to a .py file and am importing it?\n\nYou’re right, I apologize for the confusion. In my previous messages, I referred to the class as ProjectGutenberg when it should be ProjectGutenbergText. Thank you for pointing that out.\n\n\nRegarding your second question, yes, I assumed that you saved the ProjectGutenbergText class in a separate Python file and are importing it in your script. For example, if you saved the class in a file called project_gutenberg.py, you would import it like this:\n\n\nfrom project_gutenberg import ProjectGutenbergText\n\npg = ProjectGutenbergText()\n\n# Get the text of \"Pride and Prejudice\" (ID 1342)\ntext = pg.get_text(1342)\n\n# Parse the text into a Pandas dataframe with chapter, paragraph, sentence, and token columns\ndf = pg.parse_text(text)\n\n# Add a normalized term column to the dataframe\ndf['term'] = df['token'].apply(lambda x: pg.normalize_term(x))\n\n\n# Create a vocabulary dataframe with term counts and most common part of speech\nvocab_df = pg.create_vocab(df)\n\nprint(df.head())\nprint(vocab_df.head())\n\nModuleNotFoundError: No module named 'gutenberg'\n\n\n\nAgain, I apologize for any confusion my previous messages may have caused.\n\n\nfrom project_gutenberg import ProjectGutenbergText\n\n# pg = ProjectGutenbergText(default_id=1342)\npg = ProjectGutenbergText(1342) # ADDED\n\n# Get the text of the default book (ID 1342)\ntext = pg.get_text()\n\n# Parse the text into a Pandas dataframe with chapter, paragraph, sentence, and token columns\ndf = pg.parse_text(text)\n\n# Add a normalized term column to the dataframe\ndf['term'] = df['token'].apply(lambda x: pg.normalize_term(x))\n\n# Create a vocabulary dataframe with term counts and most common part of speech\nvocab_df = pg.create_vocab(df)\n\nprint(df.head())\nprint(vocab_df.head())\n\nModuleNotFoundError: No module named 'gutenberg'\n\n\n\nfrom project_gutenberg import ProjectGutenbergText\n\npg = ProjectGutenbergText(book_id=1342)\n\n# Get the text of the default book (ID 1342)\ntext = pg.get_text()\n\n# Parse the text into a Pandas dataframe with chapter, paragraph, sentence, and token columns\ndf = pg.parse_text(text)\n\n# Add a normalized term column to the dataframe\ndf['term'] = df['token'].apply(lambda x: pg.normalize_term(x))\n\n# Create a vocabulary dataframe with term counts and most common part of speech\nvocab_df = pg.create_vocab(df)\n\nprint(df.head())\nprint(vocab_df.head())\n\nModuleNotFoundError: No module named 'gutenberg'"
  },
  {
    "objectID": "lessons/M13_LLMs/M13_02_ChatGPTDemo2.html",
    "href": "lessons/M13_LLMs/M13_02_ChatGPTDemo2.html",
    "title": "Metadata",
    "section": "",
    "text": "Course:    DS 5001\nModule:    13 LLMs\nTopic:     Demonstrate ChatGPT Plus with GPT-3.5\nAuthor:    R.C. Alvarado\nDate:      27 April 2023\n\nPrompt 1\nWrite a Python class that will scrape an arbitrary text from Project Gutenberg and convert the text into a Pandas dataframe of tokens with a multi-index that maps onto the text’s structure, i.e. it’s chapter, paragraph, sentence, and token numbers. Demonstrate with Jane Austen’s Persuasion.\n\nAs an AI language model, I don’t have the capability to access the internet or write files to your computer. However, I can give you the code for the Python class that can scrape an arbitrary text from Project Gutenberg and convert it into a Pandas dataframe of tokens with a multi-index that maps onto the text’s structure, i.e., its chapter, paragraph, sentence, and token numbers. You can then use this class to scrape and process Jane Austen’s Persuasion.\n\n\nHere’s the code for the class:\n\n\nimport requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\nclass GutenbergScraper:\n    def __init__(self, url):\n        self.url = url\n        self.text = ''\n        self.structure = []\n        \n    def get_text(self):\n        response = requests.get(self.url)\n        soup = BeautifulSoup(response.content, 'html.parser')\n        self.text = soup.get_text()\n        \n    def get_structure(self):\n        if not self.text:\n            self.get_text()\n        lines = self.text.split('\\n')\n        for i, line in enumerate(lines):\n            if 'CHAPTER' in line:\n                self.structure.append(('Chapter', int(line.split(' ')[-1])))\n            elif line.startswith('  '):\n                if lines[i-1].startswith('   '):\n                    self.structure.append(('Sentence', None))\n                else:\n                    self.structure.append(('Paragraph', None))\n            else:\n                pass\n                \n    def tokenize(self):\n        if not self.structure:\n            self.get_structure()\n        tokens = self.text.split()\n        tokens_df = pd.DataFrame(tokens, columns=['Token'])\n        tokens_df['Chapter'] = 0\n        tokens_df['Paragraph'] = 0\n        tokens_df['Sentence'] = 0\n        tokens_df['Token Number'] = tokens_df.index\n        for i, (section, number) in enumerate(self.structure):\n            if section == 'Chapter':\n                tokens_df.loc[tokens_df.index >= i, 'Chapter'] = number\n            elif section == 'Paragraph':\n                tokens_df.loc[tokens_df.index >= i, 'Paragraph'] = i\n            elif section == 'Sentence':\n                tokens_df.loc[tokens_df.index >= i, 'Sentence'] = i\n        tokens_df.set_index(['Chapter', 'Paragraph', 'Sentence', 'Token Number'], inplace=True)\n        return tokens_df\n\n\nTo demonstrate how to use this class to scrape and process Jane Austen’s Persuasion, you can do the following:\n\n\nurl = 'http://www.gutenberg.org/files/105/105-0.txt'\nscraper = GutenbergScraper(url)\ntokens_df = scraper.tokenize()\n\nValueError: invalid literal for int() with base 10: 'I.\\r'\n\n\n\nThis will create a Pandas dataframe with the tokens of the text, mapped onto its chapter, paragraph, sentence, and token numbers. You can then use this dataframe for further analysis or processing of the text.\n\n\n\nPrompt 2\nThis code produces the error “ValueError: invalid literal for int() with base 10: ’I.”. Can you fix that?\n\nCertainly! The error is occurring because the int() function is trying to convert the string ’I. into an integer, which is not possible. To fix this error, we need to modify the get_structure() method to handle the case where the chapter number is not a valid integer.\n\n\nHere’s the updated code for the get_structure() method:\n\n\ndef get_structure(self):\n    if not self.text:\n        self.get_text()\n    lines = self.text.split('\\n')\n    for i, line in enumerate(lines):\n        if 'CHAPTER' in line:\n            try:\n                chapter_number = int(line.split(' ')[-1])\n            except ValueError:\n                chapter_number = None\n            self.structure.append(('Chapter', chapter_number))\n        elif line.startswith('  '):\n            if lines[i-1].startswith('   '):\n                self.structure.append(('Sentence', None))\n            else:\n                self.structure.append(('Paragraph', None))\n        else:\n            pass\n\n\nThis code will now handle the case where the chapter number is not a valid integer by setting it to None. This should prevent the int() function from trying to convert invalid values into integers, and should eliminate the ValueError that you were seeing.\n\n\n\nPrompt 3\nCan you rewrite the class with the new method please?\n\nSure, here’s the updated code for the GutenbergScraper class that includes the modified get_structure() method:\n\n\nimport requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\nclass GutenbergScraper:\n    def __init__(self, url):\n        self.url = url\n        self.text = ''\n        self.structure = []\n        \n    def get_text(self):\n        response = requests.get(self.url)\n        soup = BeautifulSoup(response.content, 'html.parser')\n        self.text = soup.get_text()\n        \n    def get_structure(self):\n        if not self.text:\n            self.get_text()\n        lines = self.text.split('\\n')\n        for i, line in enumerate(lines):\n            if 'CHAPTER' in line:\n                try:\n                    chapter_number = int(line.split(' ')[-1])\n                except ValueError:\n                    chapter_number = None\n                self.structure.append(('Chapter', chapter_number))\n            elif line.startswith('  '):\n                if lines[i-1].startswith('   '):\n                    self.structure.append(('Sentence', None))\n                else:\n                    self.structure.append(('Paragraph', None))\n            else:\n                pass\n                \n    def tokenize(self):\n        if not self.structure:\n            self.get_structure()\n        tokens = self.text.split()\n        tokens_df = pd.DataFrame(tokens, columns=['Token'])\n        tokens_df['Chapter'] = 0\n        tokens_df['Paragraph'] = 0\n        tokens_df['Sentence'] = 0\n        tokens_df['Token Number'] = tokens_df.index\n        for i, (section, number) in enumerate(self.structure):\n            if section == 'Chapter':\n                tokens_df.loc[tokens_df.index >= i, 'Chapter'] = number\n            elif section == 'Paragraph':\n                tokens_df.loc[tokens_df.index >= i, 'Paragraph'] = i\n            elif section == 'Sentence':\n                tokens_df.loc[tokens_df.index >= i, 'Sentence'] = i\n        tokens_df.set_index(['Chapter', 'Paragraph', 'Sentence', 'Token Number'], inplace=True)\n        return tokens_df\n\n\nThis should eliminate the ValueError that was occurring and allow the class to correctly tokenize the text with its multi-index mapping.\n\nTrying previously generated code block …\n\nurl = 'http://www.gutenberg.org/files/105/105-0.txt'\nscraper = GutenbergScraper(url)\ntokens_df = scraper.tokenize()\n\n\ntokens_df\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      Token\n    \n    \n      Chapter\n      Paragraph\n      Sentence\n      Token Number\n      \n    \n  \n  \n    \n      NaN\n      0\n      0\n      0\n      The\n    \n    \n      1\n      Project\n    \n    \n      2\n      Gutenberg\n    \n    \n      3\n      eBook\n    \n    \n      4\n      of\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      75\n      0\n      86360\n      to\n    \n    \n      86361\n      hear\n    \n    \n      86362\n      about\n    \n    \n      86363\n      new\n    \n    \n      86364\n      eBooks.\n    \n  \n\n86365 rows × 1 columns\n\n\n\n\n\nPrompt 4\nThank you! I ran the code and it produces a dataframe with null values in the chapter index column. Can you fix that?\n\nCertainly! The issue you’re seeing is most likely because the get_structure() method is not correctly identifying the chapter numbers. This can happen if the formatting of the text is not consistent throughout, which is often the case with Project Gutenberg texts.\n\n\nTo fix this issue, we can modify the get_structure() method to use a regular expression to identify the chapter headings. This should make it more robust and able to handle variations in formatting.\n\n\nHere’s the updated code for the get_structure() method:\n\n\nimport re\n\ndef get_structure(self):\n    if not self.text:\n        self.get_text()\n    lines = self.text.split('\\n')\n    chapter_pattern = re.compile(r'^\\s*chapter\\s+\\d+\\s*$', re.IGNORECASE)\n    for i, line in enumerate(lines):\n        if chapter_pattern.match(line):\n            chapter_number = int(re.search(r'\\d+', line).group())\n            self.structure.append(('Chapter', chapter_number))\n        elif line.startswith('  '):\n            if lines[i-1].startswith('   '):\n                self.structure.append(('Sentence', None))\n            else:\n                self.structure.append(('Paragraph', None))\n        else:\n            pass\n\n\nThis code uses the re module to create a regular expression pattern that matches chapter headings. It then searches each line of the text for a match, and if it finds one, extracts the chapter number and adds it to the structure list.\n\n\nWith this modification, the get_structure() method should be able to correctly identify the chapter headings and create the multi-index mapping as expected.\n\n\nLet me know if this solves the issue you were seeing!\n\n\n\nPrompt 5\nPlease refactor the class with the new method.\n\nSure, here’s the refactored GutenbergScraper class that includes the updated get_structure() method:\n\n\nimport requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport re\n\nclass GutenbergScraper:\n    def __init__(self, url):\n        self.url = url\n        self.text = ''\n        self.structure = []\n        \n    def get_text(self):\n        response = requests.get(self.url)\n        soup = BeautifulSoup(response.content, 'html.parser')\n        self.text = soup.get_text()\n        \n    def get_structure(self):\n        if not self.text:\n            self.get_text()\n        lines = self.text.split('\\n')\n        chapter_pattern = re.compile(r'^\\s*chapter\\s+\\d+\\s*$', re.IGNORECASE)\n        for i, line in enumerate(lines):\n            if chapter_pattern.match(line):\n                chapter_number = int(re.search(r'\\d+', line).group())\n                self.structure.append(('Chapter', chapter_number))\n            elif line.startswith('  '):\n                if lines[i-1].startswith('   '):\n                    self.structure.append(('Sentence', None))\n                else:\n                    self.structure.append(('Paragraph', None))\n            else:\n                pass\n                \n    def tokenize(self):\n        if not self.structure:\n            self.get_structure()\n        tokens = self.text.split()\n        tokens_df = pd.DataFrame(tokens, columns=['Token'])\n        tokens_df['Chapter'] = None\n        tokens_df['Paragraph'] = 0\n        tokens_df['Sentence'] = 0\n        tokens_df['Token Number'] = tokens_df.index\n        for i, (section, number) in enumerate(self.structure):\n            if section == 'Chapter':\n                tokens_df.loc[tokens_df.index >= i, 'Chapter'] = number\n            elif section == 'Paragraph':\n                tokens_df.loc[tokens_df.index >= i, 'Paragraph'] = i\n            elif section == 'Sentence':\n                tokens_df.loc[tokens_df.index >= i, 'Sentence'] = i\n        tokens_df.set_index(['Chapter', 'Paragraph', 'Sentence', 'Token Number'], inplace=True)\n        return tokens_df\n\n\nNote that I also modified the tokenize() method to set the chapter index values to None instead of 0. This is because 0 is a valid integer value, and we need to use None to indicate missing values.\n\n\nWith these modifications, the class should be able to correctly tokenize Jane Austen’s Persuasion with the multi-index mapping and without null values in the chapter index column.\n\nTrying previously generated code block …\n\nurl = 'http://www.gutenberg.org/files/105/105-0.txt'\nscraper = GutenbergScraper(url)\ntokens_df = scraper.tokenize()"
  },
  {
    "objectID": "lessons/M13_LLMs/M13_03_ChatGPT4Demo.html",
    "href": "lessons/M13_LLMs/M13_03_ChatGPT4Demo.html",
    "title": "Metadata",
    "section": "",
    "text": "Course:    DS 5001\nModule:    13 LLMs\nTopic:     Demonstrate ChatGPT Plus with GPT-4\nAuthor:    R.C. Alvarado\nDate:      27 April 2023\n\nPrompt 1\nWrite a Python class that will scrape an arbitrary text from Project Gutenberg and convert the text into a Pandas dataframe of tokens with a multi-index that maps onto the text’s structure, i.e. it’s chapter, paragraph, sentence, and token numbers. Use NLTK to parse sentences and tokens, and get part-of-speech for each token. Also, use regular expressions to detect chapter breaks. Demonstrate with Jane Austen’s Persuasion.\n\nHere’s a Python class that scrapes an arbitrary text from Project Gutenberg, converts the text into a Pandas DataFrame of tokens with a multi-index that maps onto the text’s structure (chapter, paragraph, sentence, and token numbers), and also provides part-of-speech for each token. We will use the nltk library to parse sentences and tokens and get the part-of-speech tags, and use regular expressions to detect chapter breaks.\n\n\nFirst, make sure you have the required libraries installed:\n\n\npip install pandas nltk requests beautifulsoup4\n\nRequirement already satisfied: pandas in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (1.5.3)\nRequirement already satisfied: nltk in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (3.7)\nRequirement already satisfied: requests in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (2.28.1)\nRequirement already satisfied: beautifulsoup4 in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (4.11.1)\nRequirement already satisfied: python-dateutil>=2.8.1 in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from pandas) (2022.7)\nRequirement already satisfied: numpy>=1.20.3 in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from pandas) (1.23.5)\nRequirement already satisfied: joblib in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from nltk) (1.1.1)\nRequirement already satisfied: click in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from nltk) (8.0.4)\nRequirement already satisfied: tqdm in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from nltk) (4.56.0)\nRequirement already satisfied: regex>=2021.8.3 in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from nltk) (2022.7.9)\nRequirement already satisfied: charset-normalizer<3,>=2 in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from requests) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from requests) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from requests) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from requests) (1.26.14)\nRequirement already satisfied: soupsieve>1.2 in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from beautifulsoup4) (2.3.2.post1)\nRequirement already satisfied: six>=1.5 in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nThen, you can use the following class:\n\n\nimport requests\nimport re\nimport pandas as pd\nimport nltk\nfrom bs4 import BeautifulSoup\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\nnltk.download(\"punkt\")\nnltk.download(\"averaged_perceptron_tagger\")\n\n\nclass GutenbergScraper:\n    def __init__(self, url):\n        self.url = url\n        self.text = self.get_text()\n        self.chapters = self.split_chapters()\n        self.data = self.parse_text()\n\n    def get_text(self):\n        response = requests.get(self.url)\n        soup = BeautifulSoup(response.content, \"html.parser\")\n        text = soup.get_text()\n        return text\n\n    def split_chapters(self):\n        return re.split(r\"\\bChapter\\b\", self.text, flags=re.IGNORECASE)\n\n    def parse_text(self):\n        parsed_data = []\n        for chap_no, chapter in enumerate(self.chapters):\n            paragraphs = re.split(r\"\\n\\n+\", chapter)\n            for para_no, paragraph in enumerate(paragraphs):\n                sentences = sent_tokenize(paragraph)\n                for sent_no, sentence in enumerate(sentences):\n                    tokens = word_tokenize(sentence)\n                    pos_tags = nltk.pos_tag(tokens)\n                    for token_no, (token, pos) in enumerate(pos_tags):\n                        parsed_data.append(\n                            (\n                                (chap_no, para_no, sent_no, token_no),\n                                {\"token\": token, \"pos\": pos},\n                            )\n                        )\n        return pd.DataFrame.from_dict(\n            dict(parsed_data), orient=\"index\", columns=[\"token\", \"pos\"]\n        )\n\n    def get_dataframe(self):\n        return self.data\n\n\nif __name__ == \"__main__\":\n    url = \"https://www.gutenberg.org/files/105/105-h/105-h.htm\"\n    scraper = GutenbergScraper(url)\n    df = scraper.get_dataframe()\n    print(df.head())\n\n[nltk_data] Downloading package punkt to /Users/rca2t1/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/rca2t1/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n\n\n             token  pos\n0 1 0 0        The   DT\n      1    Project  NNP\n      2  Gutenberg  NNP\n      3      eBook   NN\n      4         of   IN\n\n\n\nReplace the url variable with the URL of the specific Project Gutenberg page you want to scrape. In this case, we use Jane Austen’s Persuasion HTML file. Running this script will print the first few rows of the parsed Pandas DataFrame.\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      token\n      pos\n    \n  \n  \n    \n      0\n      1\n      0\n      0\n      The\n      DT\n    \n    \n      1\n      Project\n      NNP\n    \n    \n      2\n      Gutenberg\n      NNP\n    \n    \n      3\n      eBook\n      NN\n    \n    \n      4\n      of\n      IN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      48\n      58\n      0\n      38\n      hear\n      VB\n    \n    \n      39\n      about\n      IN\n    \n    \n      40\n      new\n      JJ\n    \n    \n      41\n      eBooks\n      NNS\n    \n    \n      42\n      .\n      .\n    \n  \n\n101722 rows × 2 columns\n\n\n\n\n\nPrompt 2\nRewrite this so that it only takes the Project Gutenburg ID in the initializer, and then figures out the regular expressions for chapter breaks in each case. This is necessary because sometimes chapters are indicated without the word “Chapter”, and also we want to include things like letters as chapters.\n\nTo make the class more flexible, we can modify the init method to take a Project Gutenberg ID instead of the URL, and then create a method that automatically generates the URL based on the ID. We can also update the split_chapters method to handle different chapter break patterns, including cases where the word “Chapter” is not used, and include things like letters as chapters.\n\n\nHere’s the updated class:\n\n\nimport requests\nimport re\nimport pandas as pd\nimport nltk\nfrom bs4 import BeautifulSoup\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\nnltk.download(\"punkt\")\nnltk.download(\"averaged_perceptron_tagger\")\n\n\nclass GutenbergScraper:\n    def __init__(self, book_id):\n        self.book_id = book_id\n        self.url = self.get_url()\n        self.text = self.get_text()\n        self.chapters = self.split_chapters()\n        self.data = self.parse_text()\n\n    def get_url(self):\n        return f\"https://www.gutenberg.org/files/{self.book_id}/{self.book_id}-h/{self.book_id}-h.htm\"\n\n    def get_text(self):\n        response = requests.get(self.url)\n        soup = BeautifulSoup(response.content, \"html.parser\")\n        text = soup.get_text()\n        return text\n\n    def split_chapters(self):\n        patterns = [\n            r\"\\bChapter\\b\",\n            r\"\\bLetter\\b\",\n            r\"\\bSection\\b\",\n            r\"\\bPart\\b\",\n            r\"\\bBook\\b\",\n        ]\n        combined_pattern = r\"|\".join(patterns)\n        return re.split(combined_pattern, self.text, flags=re.IGNORECASE)\n\n    def parse_text(self):\n        parsed_data = []\n        for chap_no, chapter in enumerate(self.chapters):\n            paragraphs = re.split(r\"\\n\\n+\", chapter)\n            for para_no, paragraph in enumerate(paragraphs):\n                sentences = sent_tokenize(paragraph)\n                for sent_no, sentence in enumerate(sentences):\n                    tokens = word_tokenize(sentence)\n                    pos_tags = nltk.pos_tag(tokens)\n                    for token_no, (token, pos) in enumerate(pos_tags):\n                        parsed_data.append(\n                            (\n                                (chap_no, para_no, sent_no, token_no),\n                                {\"token\": token, \"pos\": pos},\n                            )\n                        )\n        return pd.DataFrame.from_dict(\n            dict(parsed_data), orient=\"index\", columns=[\"token\", \"pos\"]\n        )\n\n    def get_dataframe(self):\n        return self.data\n\n\nif __name__ == \"__main__\":\n    book_id = 105  # Jane Austen's Persuasion\n    scraper = GutenbergScraper(book_id)\n    df = scraper.get_dataframe()\n    print(df.head())\n\n[nltk_data] Downloading package punkt to /Users/rca2t1/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/rca2t1/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n\n\n             token  pos\n0 1 0 0        The   DT\n      1    Project  NNP\n      2  Gutenberg  NNP\n      3      eBook   NN\n      4         of   IN\n\n\n\nYou can now provide the Project Gutenberg ID for the book you want to scrape, and the class will automatically generate the URL and handle different chapter break patterns. In the example above, we use the ID 105 for Jane Austen’s Persuasion. Running the script will print the first few rows of the parsed Pandas DataFrame."
  },
  {
    "objectID": "lessons/M13_LLMs/M13_play.html",
    "href": "lessons/M13_LLMs/M13_play.html",
    "title": "DAE.CORPUS.term_str.map(DAE.W2V)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\n\n! ls ../data/novels/novels-*\n\n../data/novels/novels-CORPUS.csv      ../data/novels/novels-TOKENS.csv\n../data/novels/novels-FastText.csv    ../data/novels/novels-VOCAB.csv\n../data/novels/novels-GENSIM_DOCS.csv ../data/novels/novels-VOCAB2.csv\n../data/novels/novels-LIB.csv         ../data/novels/novels-W2V.csv\n\n\n\nclass DAE:\n    tables = dict(\n        CORPUS = ['book_id', 'chap_id', 'para_num', 'sent_num', 'token_num'],\n        W2V  = ['term_str'],\n        VOCAB  = ['term_str'],\n        VOCAB2 = ['term_str'],\n        LIB = ['book_id']\n    )\n\n\nfor table, idx in DAE.tables.items():\n    print(table, idx)\n    setattr(DAE, table, pd.read_csv(f\"../data/novels/novels-{table}.csv\").set_index(idx))\n\nCORPUS ['book_id', 'chap_id', 'para_num', 'sent_num', 'token_num']\nW2V ['term_str']\nVOCAB ['term_str']\nVOCAB2 ['term_str']\nLIB ['book_id']\n\n\n\nlen(DAE.VOCAB), len(DAE.VOCAB2), len(DAE.W2V)\n\n(27397, 27396, 2435)\n\n\n\n\n\n\nCORPUS_EMB = DAE.W2V.merge(DAE.CORPUS.reset_index(), on='term_str').set_index(DAE.CORPUS.index.names).sort_index()\n\n\nS = CORPUS_EMB.groupby(DAE.tables['CORPUS'][:3])\n\n\nS.\n\nAttributeError: 'DataFrameGroupBy' object has no attribute 'get_groups'"
  },
  {
    "objectID": "lessons/M13_LLMs/tokenizer_training.html",
    "href": "lessons/M13_LLMs/tokenizer_training.html",
    "title": "!pip install datasets transformers[sentencepiece]",
    "section": "",
    "text": "Training your own tokenizer from scratch\nIn this notebook, we will see several ways to train your own tokenizer from scratch on a given corpus, so you can then use it to train a language model from scratch.\nWhy would you need to train a tokenizer? That’s because Transformer models very often use subword tokenization algorithms, and they need to be trained to identify the parts of words that are often present in the corpus you are using. We recommend you take a look at the tokenization chapter of the Hugging Face course for a general introduction on tokenizers, and at the tokenizers summary for a look at the differences between the subword tokenization algorithms."
  },
  {
    "objectID": "lessons/M13_LLMs/tokenizer_training.html#loading-the-dataset",
    "href": "lessons/M13_LLMs/tokenizer_training.html#loading-the-dataset",
    "title": "!pip install datasets transformers[sentencepiece]",
    "section": "Loading the dataset",
    "text": "Loading the dataset"
  },
  {
    "objectID": "lessons/M13_LLMs/tokenizer_training.html#getting-a-corpus",
    "href": "lessons/M13_LLMs/tokenizer_training.html#getting-a-corpus",
    "title": "!pip install datasets transformers[sentencepiece]",
    "section": "Getting a corpus",
    "text": "Getting a corpus\nWe will need texts to train our tokenizer. We will use the 🤗 Datasets library to download our text data, which can be easily done with the load_dataset function:\n\nfrom datasets import load_dataset\n\nFor this example, we will use Wikitext-2 (which contains 4.5MB of texts so training goes fast for our example) but you can use any dataset you want (and in any language, just not English).\n\ndataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")\n\nFound cached dataset wikitext (/Users/rca2t1/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n\n\nWe can have a look at the dataset, which as 36,718 texts:\n\ndataset\n\nDataset({\n    features: ['text'],\n    num_rows: 36718\n})\n\n\nTo access an element, we just have to provide its index:\n\ndataset[1]\n\n{'text': ' = Valkyria Chronicles III = \\n'}\n\n\nWe can also access a slice directly, in which case we get a dictionary with the key \"text\" and a list of texts as value:\n\ndataset[:5]\n\n{'text': ['',\n  ' = Valkyria Chronicles III = \\n',\n  '',\n  ' Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n',\n  \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\"]}\n\n\nThe API to train our tokenizer will require an iterator of batch of texts, for instance a list of list of texts:\n\nbatch_size = 1000\nall_texts = [dataset[i : i + batch_size][\"text\"] for i in range(0, len(dataset), batch_size)]\n\nTo avoid loading everything into memory (since the Datasets library keeps the element on disk and only load them in memory when requested), we define a Python iterator. This is particularly useful if you have a huge dataset:\n\ndef batch_iterator():\n    for i in range(0, len(dataset), batch_size):\n        yield dataset[i : i + batch_size][\"text\"]\n\nNow let’s see how we can use this corpus to train a new tokenizer! There are two APIs to do this: the first one uses an existing tokenizer and will train a new version of it on your corpus in one line of code, the second is to actually build your tokenizer block by block, so lets you customize every step!"
  },
  {
    "objectID": "lessons/M13_LLMs/tokenizer_training.html#using-an-existing-tokenizer",
    "href": "lessons/M13_LLMs/tokenizer_training.html#using-an-existing-tokenizer",
    "title": "!pip install datasets transformers[sentencepiece]",
    "section": "Using an existing tokenizer",
    "text": "Using an existing tokenizer\nIf you want to train a tokenizer with the exact same algorithms and parameters as an existing one, you can just use the train_new_from_iterator API. For instance, let’s train a new version of the GPT-2 tokenzier on Wikitext-2 using the same tokenization algorithm.\nFirst we need to load the tokenizer we want to use as a model:\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\nMake sure that the tokenizer you picked as a fast version (backed by the 🤗 Tokenizers library) otherwise the rest of the notebook will not run:\n\ntokenizer.is_fast\n\nTrue\n\n\nThen we feed the training corpus (either the list of list or the iterator we defined earlier) to the train_new_from_iterator method. We also have to specify the vocabulary size we want to use:\n\nnew_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=25000)\n\n\n\n\n\n\nAnd that’s all there is to it! The training goes very fast thanks to the 🤗 Tokenizers library, backed by Rust.\nYou now have a new tokenizer ready to preprocess your data and train a language model. You can feed it input texts as usual:\n\nnew_tokenizer(dataset[:5][\"text\"])\n\n{'input_ids': [[], [301, 8639, 9504, 3050, 301, 315], [], [4720, 74, 4825, 889, 8639, 491, 529, 672, 6944, 475, 267, 9504, 374, 2809, 529, 10879, 231, 100, 162, 255, 113, 14391, 4046, 113, 4509, 95, 18351, 4509, 256, 4046, 99, 4046, 22234, 96, 19, 264, 6437, 272, 8639, 281, 261, 3518, 2035, 491, 373, 264, 5162, 3305, 290, 344, 8639, 9504, 3050, 2616, 1822, 264, 364, 259, 14059, 1559, 340, 2393, 1527, 737, 1961, 370, 805, 3604, 288, 7577, 14, 54, 782, 337, 261, 4840, 15585, 272, 19958, 284, 1404, 1696, 284, 1822, 264, 385, 364, 261, 1431, 737, 284, 261, 8639, 906, 272, 2531, 1858, 286, 261, 1112, 9658, 281, 14059, 288, 1626, 340, 645, 6556, 344, 520, 14434, 264, 261, 1485, 3436, 7515, 290, 261, 518, 737, 288, 4750, 261, 302, 22039, 302, 264, 259, 21720, 1743, 3836, 5654, 261, 4259, 281, 4742, 490, 724, 261, 3581, 1351, 283, 1114, 579, 952, 4010, 1985, 2563, 288, 453, 2128, 807, 935, 261, 7655, 3836, 302, 2038, 314, 271, 89, 22414, 302, 272, 315], [324, 737, 1022, 1984, 284, 1525, 264, 7663, 610, 259, 1241, 4816, 281, 261, 693, 3654, 326, 8639, 9504, 1243, 272, 1894, 385, 7631, 261, 3684, 2303, 281, 261, 906, 264, 385, 534, 9638, 5354, 16654, 1030, 264, 844, 344, 1878, 261, 737, 667, 10407, 1315, 337, 906, 727, 3210, 383, 272, 13353, 8814, 8187, 2591, 6086, 74, 298, 288, 7508, 10103, 17447, 304, 11550, 9013, 920, 1898, 403, 1445, 22645, 264, 1071, 359, 8639, 9504, 1243, 2499, 21197, 5400, 19526, 5224, 272, 303, 1241, 990, 281, 3839, 8713, 261, 3418, 272, 324, 737, 331, 83, 2574, 3535, 321, 8351, 370, 1073, 331, 78, 272, 315]], 'attention_mask': [[], [1, 1, 1, 1, 1, 1], [], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n\n\nYou can save it locally with the save_pretrained method:\n\nnew_tokenizer.save_pretrained(\"my-new-tokenizer\")\n\n('my-new-tokenizer/tokenizer_config.json',\n 'my-new-tokenizer/special_tokens_map.json',\n 'my-new-tokenizer/vocab.json',\n 'my-new-tokenizer/merges.txt',\n 'my-new-tokenizer/added_tokens.json',\n 'my-new-tokenizer/tokenizer.json')\n\n\nOr even push it to the Hugging Face Hub to use that new tokenzier from anywhere. Just make sure you have your authentication token stored by executing huggingface-cli login in a terminal or executing the following cell:\n\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n\n\n\n\nWe are almost there, it is also necessary that you have git lfs installed. You can do it directly from this notebook by uncommenting the following cells:\n\n# !apt install git-lfs\n\n\nnew_tokenizer.push_to_hub(\"my-new-shiny-tokenizer\")\n\nValueError: Token is required (write-access action) but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `notebook_login`. See https://huggingface.co/settings/tokens.\n\n\nThe tokenizer can now be reloaded on this machine with:\n\ntok = new_tokenizer.from_pretrained(\"my-new-tokenizer\")\n\nOr from anywhere using the repo ID, which is your namespace followed by a slash an the name you gave in the push_to_hub method, so for instance:\ntok = new_tokenizer.from_pretrained(\"sgugger/my-new-shiny-tokenizer\")\nNow if you want to create and a train a new tokenizer that doesn’t look like anything in existence, you will need to build it from scratch using the 🤗 Tokenizers library."
  },
  {
    "objectID": "lessons/M13_LLMs/tokenizer_training.html#building-your-tokenizer-from-scratch",
    "href": "lessons/M13_LLMs/tokenizer_training.html#building-your-tokenizer-from-scratch",
    "title": "!pip install datasets transformers[sentencepiece]",
    "section": "Building your tokenizer from scratch",
    "text": "Building your tokenizer from scratch\nTo understand how to build your tokenizer from scratch, we have to dive a little bit more in the 🤗 Tokenizers library and the tokenization pipeline. This pipeline takes several steps:\n\nNormalization: Executes all the initial transformations over the initial input string. For example when you need to lowercase some text, maybe strip it, or even apply one of the common unicode normalization process, you will add a Normalizer.\nPre-tokenization: In charge of splitting the initial input string. That’s the component that decides where and how to pre-segment the origin string. The simplest example would be to simply split on spaces.\nModel: Handles all the sub-token discovery and generation, this is the part that is trainable and really dependent of your input data.\nPost-Processing: Provides advanced construction features to be compatible with some of the Transformers-based SoTA models. For instance, for BERT it would wrap the tokenized sentence around [CLS] and [SEP] tokens.\n\nAnd to go in the other direction:\n\nDecoding: In charge of mapping back a tokenized input to the original string. The decoder is usually chosen according to the PreTokenizer we used previously.\n\nFor the training of the model, the 🤗 Tokenizers library provides a Trainer class that we will use.\nAll of these building blocks can be combined to create working tokenization pipelines. To give you some examples, we will show three full pipelines here: how to replicate GPT-2, BERT and T5 (which will give you an example of BPE, WordPiece and Unigram tokenizer).\n\nWordPiece model like BERT\nLet’s have a look at how we can create a WordPiece tokenizer like the one used for training BERT. The first step is to create a Tokenizer with an empty WordPiece model:\n\nfrom tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer\n\ntokenizer = Tokenizer(models.WordPiece(unl_token=\"[UNK]\"))\n\nThis tokenizer is not ready for training yet. We have to add some preprocessing steps: the normalization (which is optional) and the pre-tokenizer, which will split inputs into the chunks we will call words. The tokens will then be part of those words (but can’t be larger than that).\nIn the case of BERT, the normalization is lowercasing. Since BERT is such a popular model, it has its own normalizer:\n\ntokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\n\nIf you want to customize it, you can use the existing blocks and compose them in a sequence: here for instance we lower case, apply NFD normalization and strip the accents:\n\ntokenizer.normalizer = normalizers.Sequence(\n    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n)\n\nThere is also a BertPreTokenizer we can use directly. It pre-tokenizes using white space and punctuation:\n\ntokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n\nLike for the normalizer, we can combine several pre-tokenizers in a Sequence. If we want to have a quick look at how it preprocesses the inputs, we can call the pre_tokenize_str method:\n\ntokenizer.pre_tokenizer.pre_tokenize_str(\"This is an example!\")\n\nNote that the pre-tokenizer not only split the text into words but keeps the offsets, that is the beginning and start of each of those words inside the original text. This is what will allow the final tokenizer to be able to match each token to the part of the text that it comes from (a feature we use for question answering or token classification tasks).\nWe can now train our tokenizer (the pipeline is not entirely finished but we will need a trained tokenizer to build the post-processor), we use a WordPieceTrainer for that. The key thing to remember is to pass along the special tokens to the trainer, as they won’t be seen in the corpus.\n\nspecial_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\ntrainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)\n\nTo actually train the tokenizer, the method looks like what we used before: we can either pass some text files, or an iterator of batches of texts:\n\ntokenizer.train_from_iterator(batch_iterator(), trainer=trainer)\n\nNow that the tokenizer is trained, we can define the post-processor: we need to add the CLS token at the beginning and the SEP token at the end (for single sentences) or several SEP tokens (for pairs of sentences). We use a TemplateProcessing to do this, which requires to know the IDs of the CLS and SEP token (which is why we waited for the training).\nSo let’s first grab the ids of the two special tokens:\n\ncls_token_id = tokenizer.token_to_id(\"[CLS]\")\nsep_token_id = tokenizer.token_to_id(\"[SEP]\")\nprint(cls_token_id, sep_token_id)\n\nAnd here is how we can build our post processor. We have to indicate in the template how to organize the special tokens with one sentence ($A) or two sentences ($A and $B). The : followed by a number indicates the token type ID to give to each part.\n\ntokenizer.post_processor = processors.TemplateProcessing(\n    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n    special_tokens=[\n        (\"[CLS]\", cls_token_id),\n        (\"[SEP]\", sep_token_id),\n    ],\n)\n\nWe can check we get the expected results by encoding a pair of sentences for instance:\n\nencoding = tokenizer.encode(\"This is one sentence.\", \"With this one we have a pair.\")\n\nWe can look at the tokens to check the special tokens have been inserted in the right places:\n\nencoding.tokens\n\nAnd we can check the token type ids are correct:\n\nencoding.type_ids\n\nThe last piece in this tokenizer is the decoder, we use a WordPiece decoder and indicate the special prefix ##:\n\ntokenizer.decoder = decoders.WordPiece(prefix=\"##\")\n\nNow that our tokenizer is finished, we need to wrap it inside a Transformers object to be able to use it with the Transformers library. More specifically, we have to put it inside the class of tokenizer fast corresponding to the model we want to use, here a BertTokenizerFast:\n\nfrom transformers import BertTokenizerFast\n\nnew_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)\n\nAnd like before, we can use this tokenizer as a normal Transformers tokenizer, and use the save_pretrained or push_to_hub methods.\nIf the tokenizer you are building does not match any class in Transformers because it’s really special, you can wrap it in PreTrainedTokenizerFast.\n\n\nBPE model like GPT-2\nLet’s now have a look at how we can create a BPE tokenizer like the one used for training GPT-2. The first step is to create a Tokenizer with an empty BPE model:\n\ntokenizer = Tokenizer(models.BPE())\n\nLike before, we have to add the optional normalization (not used in the case of GPT-2) and we need to specify a pre-tokenizer before training. In the case of GPT-2, the pre-tokenizer used is a byte level pre-tokenizer:\n\ntokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n\nIf we want to have a quick look at how it preprocesses the inputs, we can call the pre_tokenize_str method:\n\ntokenizer.pre_tokenizer.pre_tokenize_str(\"This is an example!\")\n\nWe used the same default as for GPT-2 for the prefix space, so you can see that each word gets an initial 'Ġ' added at the beginning, except the first one.\nWe can now train our tokenizer! This time we use a BpeTrainer.\n\ntrainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"])\ntokenizer.train_from_iterator(batch_iterator(), trainer=trainer)\n\nTo finish the whole pipeline, we have to include the post-processor and decoder:\n\ntokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\ntokenizer.decoder = decoders.ByteLevel()\n\nAnd like before, we finish by wrapping this in a Transformers tokenizer object:\n\nfrom transformers import GPT2TokenizerFast\n\nnew_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)\n\n\n\nUnigram model like Albert\nLet’s now have a look at how we can create a Unigram tokenizer like the one used for training T5. The first step is to create a Tokenizer with an empty Unigram model:\n\ntokenizer = Tokenizer(models.Unigram())\n\nLike before, we have to add the optional normalization (here some replaces and lower-casing) and we need to specify a pre-tokenizer before training. The pre-tokenizer used is a Metaspace pre-tokenizer: it replaces all spaces by a special character (defaulting to ▁) and then splits on that character.\n\ntokenizer.normalizer = normalizers.Sequence(\n    [normalizers.Replace(\"``\", '\"'), normalizers.Replace(\"''\", '\"'), normalizers.Lowercase()]\n)\ntokenizer.pre_tokenizer = pre_tokenizers.Metaspace()\n\nIf we want to have a quick look at how it preprocesses the inputs, we can call the pre_tokenize_str method:\n\ntokenizer.pre_tokenizer.pre_tokenize_str(\"This is an example!\")\n\nYou can see that each word gets an initial ▁ added at the beginning, as is usually done by sentencepiece.\nWe can now train our tokenizer! This time we use a UnigramTrainer.”We have to explicitely set the unknown token in this trainer otherwise it will forget it afterward.\n\ntrainer = trainers.UnigramTrainer(vocab_size=25000, special_tokens=[\"[CLS]\", \"[SEP]\", \"<unk>\", \"<pad>\", \"[MASK]\"], unk_token=\"<unk>\")\ntokenizer.train_from_iterator(batch_iterator(), trainer=trainer)\n\nTo finish the whole pipeline, we have to include the post-processor and decoder. The post-processor is very similar to what we saw with BERT, the decoder is just Metaspace, like for the pre-tokenizer.\n\ncls_token_id = tokenizer.token_to_id(\"[CLS]\")\nsep_token_id = tokenizer.token_to_id(\"[SEP]\")\n\n\ntokenizer.post_processor = processors.TemplateProcessing(\n    single=\"[CLS]:0 $A:0 [SEP]:0\",\n    pair=\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n    special_tokens=[\n        (\"[CLS]\", cls_token_id),\n        (\"[SEP]\", sep_token_id),\n    ],\n)\ntokenizer.decoder = decoders.Metaspace()\n\nAnd like before, we finish by wrapping this in a Transformers tokenizer object:\n\nfrom transformers import AlbertTokenizerFast\n\nnew_tokenizer = AlbertTokenizerFast(tokenizer_object=tokenizer)"
  },
  {
    "objectID": "lessons/M13_LLMs/tokenizer_training.html#use-your-new-tokenizer-to-train-a-language-model",
    "href": "lessons/M13_LLMs/tokenizer_training.html#use-your-new-tokenizer-to-train-a-language-model",
    "title": "!pip install datasets transformers[sentencepiece]",
    "section": "Use your new tokenizer to train a language model!",
    "text": "Use your new tokenizer to train a language model!\nYou can either use your new tokenizer in the language modeling from scratch notebook [Link to come] or use the --tokenizer_name argument in the language modeling scripts to use it there to train a model from scratch."
  },
  {
    "objectID": "lessons/M90_DispersionPlots/m01.04-austen-kde.html",
    "href": "lessons/M90_DispersionPlots/m01.04-austen-kde.html",
    "title": "Synopsis",
    "section": "",
    "text": "Configuration"
  },
  {
    "objectID": "lessons/M90_DispersionPlots/m01.04-austen-kde.html#import-tables-from-database",
    "href": "lessons/M90_DispersionPlots/m01.04-austen-kde.html#import-tables-from-database",
    "title": "Synopsis",
    "section": "Import tables from database",
    "text": "Import tables from database\n\nV = pd.read_csv(data_dir+'/output/austen-combo-VOCAB.csv').set_index('term_str')\nK0 = pd.read_csv(data_dir+'/output/austen-combo-TOKENS.csv').set_index(OHCO)\n\n\nK = K0.loc[2]\nK = K.reset_index(drop=True)\nK.index.name = 'offset'\n\n\nK.head()\n\n\n\n\n\n  \n    \n      \n      token_str\n      term_str\n    \n    \n      offset\n      \n      \n    \n  \n  \n    \n      0\n      Sir\n      sir\n    \n    \n      1\n      Walter\n      walter\n    \n    \n      2\n      Elliot\n      elliot\n    \n    \n      3\n      of\n      of\n    \n    \n      4\n      Kellynch\n      kellynch"
  },
  {
    "objectID": "lessons/M90_DispersionPlots/m01.04-austen-kde.html#reduce-vocabulary",
    "href": "lessons/M90_DispersionPlots/m01.04-austen-kde.html#reduce-vocabulary",
    "title": "Synopsis",
    "section": "Reduce vocabulary",
    "text": "Reduce vocabulary\n\nstops = set(stopwords.words('english'))\n\n\nlen(stops)\n\n179\n\n\n\nV1 = V[(~V.index.isin(stops)) & (V.n < 1000) & (V.n > 100)]\n\n\nV1.shape\n\n(163, 5)\n\n\n\nV1.sort_values('n', ascending=False).head(10)\n\n\n\n\n\n  \n    \n      \n      n\n      n_chars\n      p\n      i\n      h\n    \n    \n      term_str\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      would\n      870\n      5\n      0.004248\n      7.879013\n      0.033470\n    \n    \n      mrs\n      821\n      3\n      0.004009\n      7.962646\n      0.031920\n    \n    \n      elinor\n      685\n      6\n      0.003345\n      8.223924\n      0.027506\n    \n    \n      said\n      570\n      4\n      0.002783\n      8.489066\n      0.023626\n    \n    \n      one\n      569\n      3\n      0.002778\n      8.491600\n      0.023592\n    \n    \n      marianne\n      566\n      8\n      0.002764\n      8.499226\n      0.023489\n    \n    \n      must\n      511\n      4\n      0.002495\n      8.646705\n      0.021574\n    \n    \n      anne\n      503\n      4\n      0.002456\n      8.669470\n      0.021292\n    \n    \n      much\n      495\n      4\n      0.002417\n      8.692600\n      0.021010\n    \n    \n      every\n      478\n      5\n      0.002334\n      8.743018\n      0.020406\n    \n  \n\n\n\n\n\nK = K[K.term_str.isin(V1.index)]\n\n\nK.head()\n\n\n\n\n\n  \n    \n      \n      token_str\n      term_str\n    \n    \n      offset\n      \n      \n    \n  \n  \n    \n      0\n      Sir\n      sir\n    \n    \n      1\n      Walter\n      walter\n    \n    \n      2\n      Elliot\n      elliot\n    \n    \n      10\n      man\n      man\n    \n    \n      16\n      never\n      never"
  },
  {
    "objectID": "lessons/M90_DispersionPlots/m01.04-austen-kde.html#create-arrays-of-offsets-for-each-term",
    "href": "lessons/M90_DispersionPlots/m01.04-austen-kde.html#create-arrays-of-offsets-for-each-term",
    "title": "Synopsis",
    "section": "Create arrays of offsets for each term",
    "text": "Create arrays of offsets for each term\n\nB = K.reset_index().groupby(['term_str']).offset.apply(lambda x: x.tolist()).to_frame()\n\n\nB['x'] = B.apply(lambda x: np.array(x.offset)[:, np.newaxis], 1)\n\n\nB.head()\n\n\n\n\n\n  \n    \n      \n      offset\n      x\n    \n    \n      term_str\n      \n      \n    \n  \n  \n    \n      acquaintance\n      [692, 1219, 1633, 1891, 2911, 8246, 9276, 1252...\n      [[692], [1219], [1633], [1891], [2911], [8246]...\n    \n    \n      almost\n      [75, 5178, 6696, 7864, 9055, 9327, 10035, 1107...\n      [[75], [5178], [6696], [7864], [9055], [9327],...\n    \n    \n      always\n      [111, 860, 930, 1472, 1606, 3214, 4520, 5296, ...\n      [[111], [860], [930], [1472], [1606], [3214], ...\n    \n    \n      anne\n      [155, 893, 937, 965, 979, 1222, 2424, 2928, 30...\n      [[155], [893], [937], [965], [979], [1222], [2...\n    \n    \n      another\n      [2152, 3680, 4286, 5283, 6957, 12676, 12756, 1...\n      [[2152], [3680], [4286], [5283], [6957], [1267..."
  },
  {
    "objectID": "lessons/M90_DispersionPlots/m01.04-austen-kde.html#get-kde-for-each-term",
    "href": "lessons/M90_DispersionPlots/m01.04-austen-kde.html#get-kde-for-each-term",
    "title": "Synopsis",
    "section": "Get KDE for each term",
    "text": "Get KDE for each term\n\nscale_max = K.index.max() # THIS IS CRUCIAL\nx_axis = np.linspace(0, scale_max, kde_samples)[:, np.newaxis]\nB['kde'] = B.apply(lambda row: KDE(kernel=kde_kernel, bandwidth=kde_bandwidth).fit(row.x), 1)\nB['scores'] = B.apply(lambda row: row.kde.score_samples(x_axis), axis=1)\n# B['scaled'] = B.apply(lambda row: np.exp(row.scores) * (scale_max / kde_samples), axis=1)"
  },
  {
    "objectID": "lessons/M90_DispersionPlots/m01.04-austen-kde.html#visualize-kde-plots",
    "href": "lessons/M90_DispersionPlots/m01.04-austen-kde.html#visualize-kde-plots",
    "title": "Synopsis",
    "section": "Visualize KDE plots",
    "text": "Visualize KDE plots\n\nPLOTS = B.apply(lambda row: pd.Series(np.exp(row.scores) * (scale_max / kde_samples)), axis=1)\n\n\nFIG = dict(figsize=(15, 5))\n\n\nPLOTS.T[['anne','wentworth']].plot(**FIG);\n\n\n\n\n\nPLOTS.T[['anne','walter']].plot(**FIG);\n\n\n\n\n\nPLOTS.T[['wentworth','walter']].plot(**FIG);\n\n\n\n\n\nPLOTS.T[['anne','wentworth','walter']].plot(**FIG);\n\n\n\n\n\nPLOTS.loc['anne'].plot(**FIG)\nPLOTS.loc['walter'].plot(**FIG)\n\n<AxesSubplot: >\n\n\n\n\n\n\nPLOTS.loc['walter'].plot(**FIG)\nPLOTS.loc['wentworth'].plot(**FIG)\n\n<AxesSubplot: >\n\n\n\n\n\n\n# PLOTS.loc['walter'].plot(**FIG)\n# PLOTS.loc['elizabeth'].plot(**FIG)"
  },
  {
    "objectID": "lessons/M90_DispersionPlots/m01.04-austen-kde.html#score-pairs",
    "href": "lessons/M90_DispersionPlots/m01.04-austen-kde.html#score-pairs",
    "title": "Synopsis",
    "section": "Score Pairs",
    "text": "Score Pairs\nWe generate only unique combinations of pairs not permutation, i.e. we treat a,b == b,a.\n\npairs = pd.DataFrame([(x,y) for x in B.index for y in B.index if y > x] , columns=['x','y'])\n\n\npairs.head(10).T"
  },
  {
    "objectID": "lessons/M90_DispersionPlots/m01.04-austen-kde.html#compute-overlap",
    "href": "lessons/M90_DispersionPlots/m01.04-austen-kde.html#compute-overlap",
    "title": "Synopsis",
    "section": "Compute overlap",
    "text": "Compute overlap\nThis takes a while to run.\n\ndef overlap(row):\n    kde1 = PLOTS.loc[row.x]\n    kde2 = PLOTS.loc[row.y]\n    overlap = np.minimum(kde1, kde2)\n    return np.trapz(overlap)\n\n\npairs['overlap'] = pairs.apply(overlap, axis=1)\n\n\ndef paircorr(row):\n    return PLOTS.T[[row.x,row.y]].corr().values[0][1]\n\n\npairs['corr'] = pairs.apply(paircorr, axis=1)"
  },
  {
    "objectID": "lessons/M90_DispersionPlots/m01.04-austen-kde.html#skim-top-pairs",
    "href": "lessons/M90_DispersionPlots/m01.04-austen-kde.html#skim-top-pairs",
    "title": "Synopsis",
    "section": "Skim Top Pairs",
    "text": "Skim Top Pairs\n\npairs.overlap.plot.hist()\n\n\npairs[pairs.overlap > .6 ].sort_values('overlap', ascending=False)\n\n\npairs2 = pairs.copy().rename(columns={'x':'y', 'y':'x'})\npairs3 = pd.concat([pairs, pairs2], sort=True)"
  },
  {
    "objectID": "lessons/M90_DispersionPlots/m01.04-austen-kde.html#see-related-terms-for-top-terms",
    "href": "lessons/M90_DispersionPlots/m01.04-austen-kde.html#see-related-terms-for-top-terms",
    "title": "Synopsis",
    "section": "See related terms for top terms",
    "text": "See related terms for top terms\n\n# TOP_TERMS = V1.iloc[40:60].term_str.tolist()\nTOP_TERMS = 'anne wentworth elliot elizabeth'.split()\n\n\nTOP_TERMS\n\n\nDETAIL = '<table>'\nfor i, term in enumerate(TOP_TERMS):\n    friends = pairs3[pairs3.x == term].sort_values('overlap', ascending=False).head(10)\n    DETAIL += \"<tr><td colspan=1><b>{}. {}</b></td></tr>\".format(i+1, term)\n    for row in friends.reset_index(drop=True)[['y', 'overlap']].values:\n        bar = round(row[1] * 100) * '|'\n        DETAIL += \"<tr><td>{}</td><td style='text-align:left;'>{} ({})</td></tr>\".format(row[0], bar, row[1])\nDETAIL += \"</table>\"\n\n\ndisplay(HTML(DETAIL))"
  },
  {
    "objectID": "lessons/M90_DispersionPlots/m01.04-austen-kde.html#explore-term-correlations",
    "href": "lessons/M90_DispersionPlots/m01.04-austen-kde.html#explore-term-correlations",
    "title": "Synopsis",
    "section": "Explore term correlations",
    "text": "Explore term correlations\n\nCORR = pd.crosstab(pairs3.x, pairs3.y, pairs3.overlap, aggfunc='sum').fillna(1)\n\n\nCORR.head()\n\n\ndef corr_plot_terms(terms, dtm, title='Foo'):\n    plt.figure(figsize = (20,20))\n    print(title)\n    corr = dtm[terms].corr()\n    sns.heatmap(corr, vmax=.3, annot=True, center=0, \n              cmap='RdYlGn',\n              square=True, linewidths=.5, \n              cbar_kws={\"shrink\": .5})\n    plt.show()\n\n\ncorr_plot_terms(TOP_TERMS, PLOTS.T, title='TEST')"
  },
  {
    "objectID": "lessons/M90_DispersionPlots/m01.04-austen-kde.html#export-graphs",
    "href": "lessons/M90_DispersionPlots/m01.04-austen-kde.html#export-graphs",
    "title": "Synopsis",
    "section": "Export Graphs",
    "text": "Export Graphs\n\nimport networkx as nx\nG = nx.Graph()\nedges = pairs[['x','y','overlap']].sort_values('overlap', ascending=False).head(1000).apply(lambda x: (x.x, x.y, x.overlap), axis=1).values\nG.add_weighted_edges_from(edges)\nnx.write_gexf(G, \"{}.gexf\".format('austen'))"
  },
  {
    "objectID": "lessons/M90_DispersionPlots/onehot.html",
    "href": "lessons/M90_DispersionPlots/onehot.html",
    "title": "set size of figure",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nsns.set()\n%matplotlib inline\n\n\nOHCO = \"chap_num para_num sent_num token_num\".split()\n\n\ntokens = pd.read_csv('../MOD02--TextModels/austen-persuasion.csv').set_index(OHCO)\n\n\ntokens\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      token_str\n    \n    \n      chap_num\n      para_num\n      sent_num\n      token_num\n      \n    \n  \n  \n    \n      1\n      1\n      0\n      0\n      Sir\n    \n    \n      1\n      Walter\n    \n    \n      2\n      Elliot\n    \n    \n      3\n      of\n    \n    \n      4\n      Kellynch\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      24\n      14\n      0\n      6\n      of\n    \n    \n      7\n      Persuasion\n    \n    \n      8\n      by\n    \n    \n      9\n      Jane\n    \n    \n      10\n      Austen\n    \n  \n\n88949 rows × 1 columns\n\n\n\n\ntokens['term_str'] = tokens.token_str.str.replace(r'\\W+', '').str.lower()\n\n\nonehot = pd.get_dummies(tokens, columns=['term_str'], prefix='', prefix_sep='', drop_first=True).reset_index(drop=True).iloc[:,1:]\n\n\nonehot['anne'].plot(figsize=(20, 1))\n\n<matplotlib.axes._subplots.AxesSubplot at 0x104563d30>\n\n\n\n\n\n\nonehot[['love','hate', 'joy', 'happiness']].plot(figsize=(20, .5))\n\n<matplotlib.axes._subplots.AxesSubplot at 0x1a1a93cb70>\n\n\n\n\n\n\nX = onehot.reset_index()\n\nplt.figure(figsize=(22,6))\n\n# use horizontal stripplot with x marker size of 5\nsns.stripplot(y='yours', x='index', data=X,\n orient='h', marker='X', color='navy', size=5)\n\n# rotate x tick labels\nplt.xticks(rotation=15)\n\n# remover borders of plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nX\n\n\n\n\n\n  \n    \n      \n      index\n      1\n      15\n      16\n      1760\n      1784\n      1785\n      1787\n      1789\n      1791\n      ...\n      your\n      yours\n      yourself\n      yourselves\n      youth\n      youthful\n      z\n      zeal\n      zealous\n      zealously\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      2\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      3\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      4\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      88944\n      88944\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      88945\n      88945\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      88946\n      88946\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      88947\n      88947\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      88948\n      88948\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n88949 rows × 5760 columns\n\n\n\n\nonehot.reset_index()\n\n\n\n\n\n  \n    \n      \n      index\n      1\n      15\n      16\n      1760\n      1784\n      1785\n      1787\n      1789\n      1791\n      ...\n      your\n      yours\n      yourself\n      yourselves\n      youth\n      youthful\n      z\n      zeal\n      zealous\n      zealously\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      2\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      3\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      4\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      88944\n      88944\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      88945\n      88945\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      88946\n      88946\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      88947\n      88947\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      88948\n      88948\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n88949 rows × 5760 columns\n\n\n\n\ncfg = {'figsize': (20,1)}\n\n\nonehot['anne'].plot(**cfg);\n\n\n\n\n\nonehot['wentworth'].plot(**cfg);\n\n\n\n\n\nonehot.T\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      ...\n      88939\n      88940\n      88941\n      88942\n      88943\n      88944\n      88945\n      88946\n      88947\n      88948\n    \n  \n  \n    \n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      15\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      16\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1760\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1784\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      youthful\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      z\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      zeal\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      zealous\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      zealously\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n5759 rows × 88949 columns\n\n\n\n\nimport re\n\n\nV = [x for x in onehot.columns.tolist() if not re.match('\\d', x)]\n\n\nM = onehot[V].T.sort_values([x for x in range(onehot.shape[0])], ascending=False)"
  },
  {
    "objectID": "lessons/M91_Charrette/CharretteFIguresAndCharacters.html",
    "href": "lessons/M91_Charrette/CharretteFIguresAndCharacters.html",
    "title": "Figures and Characters",
    "section": "",
    "text": "import pandas as pd\nimport sqlite3\n\n\nTOKENS = pd.read_csv('WORDS.csv', sep='\\t').set_index(['line_num','token_num'])\nTOKENS.columns = ['term_id','term_str','punc']\n\n\nTOKENS.head()\n\n\n\n\n\n  \n    \n      \n      \n      term_id\n      term_str\n      punc\n    \n    \n      line_num\n      token_num\n      \n      \n      \n    \n  \n  \n    \n      1\n      1\n      89769\n      Des\n      &#160;\n    \n    \n      2\n      89770\n      que\n      &#160;\n    \n    \n      3\n      89771\n      ma\n      &#160;\n    \n    \n      4\n      89772\n      dame\n      &#160;\n    \n    \n      5\n      89773\n      de\n      &#160;\n    \n  \n\n\n\n\n\nTOKENS.term_str.unstack(fill_value='')\n\n\n\n\n\n  \n    \n      token_num\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n    \n    \n      line_num\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      Des\n      que\n      ma\n      dame\n      de\n      Chanpaigne\n      \n      \n      \n      \n      \n    \n    \n      2\n      Vialt\n      que\n      romans\n      a\n      feire\n      anpraigne\n      \n      \n      \n      \n      \n    \n    \n      3\n      Je\n      l\n      anprendrai\n      mout\n      volentiers\n      \n      \n      \n      \n      \n      \n    \n    \n      4\n      Come\n      cil\n      qui\n      est\n      suens\n      antiers\n      \n      \n      \n      \n      \n    \n    \n      5\n      De\n      quanqu\n      il\n      puet\n      el\n      monde\n      feire\n      \n      \n      \n      \n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      7133\n      Tant\n      en\n      a\n      fet\n      n\n      i\n      vialt\n      plus\n      metre\n      \n      \n    \n    \n      7134\n      Ne\n      moins\n      por\n      le\n      conte\n      malmetre\n      \n      \n      \n      \n      \n    \n    \n      7135\n      Ci\n      faut\n      li\n      ROMANS\n      DE\n      LANCELOT\n      \n      \n      \n      \n      \n    \n    \n      7136\n      DE\n      LA\n      CHARRETE\n      \n      \n      \n      \n      \n      \n      \n      \n    \n    \n      7137\n      LE\n      CHEVALIER\n      DE\n      LA\n      CHARRETTE\n      \n      \n      \n      \n      \n      \n    \n  \n\n7137 rows × 11 columns\n\n\n\n\nFIGURES = pd.read_csv('FIGURES.csv', sep='\\t', \n                      skiprows=1,\n                      names=['fig_genus','fig_species','fig_id','fig_part','term_id'])\\\n    .set_index(['fig_genus','fig_species','fig_part','fig_id'])\n\n\nFIGURES.head()\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      term_id\n    \n    \n      fig_genus\n      fig_species\n      fig_part\n      fig_id\n      \n    \n  \n  \n    \n      AD\n      0\n      A\n      3781\n      89780\n    \n    \n      B\n      3781\n      89783\n    \n    \n      A\n      3782\n      89856\n    \n    \n      B\n      3782\n      89802\n    \n    \n      C\n      3782\n      89814\n    \n  \n\n\n\n\n\nFIGURES.term_id.map(TOKENS.reset_index().set_index('term_id').term_str).to_frame('term_str')\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      term_str\n    \n    \n      fig_genus\n      fig_species\n      fig_part\n      fig_id\n      \n    \n  \n  \n    \n      AD\n      0\n      A\n      3781\n      anpraigne\n    \n    \n      B\n      3781\n      anprendrai\n    \n    \n      A\n      3782\n      losangier\n    \n    \n      B\n      3782\n      losange\n    \n    \n      C\n      3782\n      losenge\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      RR\n      9\n      C\n      5366\n      l\n    \n    \n      5366\n      ait\n    \n    \n      D\n      5366\n      delait\n    \n    \n      C\n      6311\n      lot\n    \n    \n      D\n      6311\n      Lancelot\n    \n  \n\n41833 rows × 1 columns\n\n\n\n\nFTP = pd.read_csv('figuretype-and-person.csv', header=None, names=['figure','person','n']).set_index(['figure','person'])\n\n\nFTP\n\n\n\n\n\n  \n    \n      \n      \n      n\n    \n    \n      figure\n      person\n      \n    \n  \n  \n    \n      AD\n      DEU\n      4\n    \n    \n      IRLANDE\n      1\n    \n    \n      MELEAGANT\n      11\n    \n    \n      PERE\n      1\n    \n    \n      SARRAZIN\n      1\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      RR\n      SELVESTRE\n      1\n    \n    \n      TAULAS DE LA DESERTE\n      1\n    \n    \n      TOLOSE\n      1\n    \n    \n      YSORE\n      1\n    \n    \n      YVAIN\n      1\n    \n  \n\n86 rows × 1 columns\n\n\n\n\nFTPM = FTP.unstack(fill_value=0).T\nFIG_PRIORS = (FTPM.sum() / FTPM.sum().sum())\n# FIG_PRIORS.index = FIG_PRIORS.index.droplevel(0)\nCHAR_PRIORS = (FTPM.sum(1) / FTPM.sum(1).sum())\nCHAR_PRIORS.index = CHAR_PRIORS.index.droplevel(0)\n\n\nFIG_PRIORS.sort_values().plot.barh()\n\n<AxesSubplot:ylabel='figure'>\n\n\n\n\n\n\nCHAR_PRIORS.sort_values(ascending=False).plot.bar(figsize=(15,5))\n\n<AxesSubplot:xlabel='None,person'>\n\n\n\n\n\n\nFTPM.index = FTPM.index.droplevel(0)\n\n\nFTP_JD = FTPM / FTPM.sum().sum()\n\n\nFTP_JD.stack().sort_values(ascending=False).head(10)\n\nperson     figure\nLANCELOT   CH        0.170455\n           RR        0.060606\n           EN        0.060606\nMELEAGANT  EN        0.056818\nGAUVAIN    CH        0.049242\nMELEAGANT  AD        0.041667\nKEU        CH        0.041667\nMELEAGANT  CH        0.037879\nDEU        RR        0.037879\n           EN        0.022727\ndtype: float64\n\n\n\nFTPM.style.background_gradient()\n\n\n            figure        AD        CH        EN        RR                person                                    \n                \n                        ABEL\n                        0\n                        1\n                        0\n                        1\n            \n            \n                        AMIENS\n                        0\n                        0\n                        0\n                        1\n            \n            \n                        ANGLETERRE\n                        0\n                        0\n                        1\n                        0\n            \n            \n                        ARRAGON\n                        0\n                        0\n                        0\n                        1\n            \n            \n                        ARTU\n                        0\n                        3\n                        2\n                        0\n            \n            \n                        ASCANSION\n                        0\n                        0\n                        0\n                        1\n            \n            \n                        BADEMAGU\n                        0\n                        4\n                        1\n                        2\n            \n            \n                        BREIBANÃ‡ON\n                        0\n                        0\n                        0\n                        1\n            \n            \n                        BUCIFAL\n                        0\n                        1\n                        0\n                        0\n            \n            \n                        CARLION\n                        0\n                        0\n                        0\n                        1\n            \n            \n                        CHANPAIGNE\n                        0\n                        0\n                        0\n                        1\n            \n            \n                        CRESTIIEN\n                        0\n                        2\n                        1\n                        0\n            \n            \n                        CRIATOR\n                        0\n                        0\n                        0\n                        1\n            \n            \n                        DEU\n                        4\n                        4\n                        6\n                        10\n            \n            \n                        DONBES\n                        0\n                        1\n                        0\n                        1\n            \n            \n                        ESPAINGNE\n                        0\n                        1\n                        0\n                        1\n            \n            \n                        ESPERIT\n                        0\n                        0\n                        0\n                        1\n            \n            \n                        EVE\n                        0\n                        1\n                        3\n                        0\n            \n            \n                        FORTUNE\n                        0\n                        2\n                        1\n                        0\n            \n            \n                        GANT\n                        0\n                        0\n                        0\n                        1\n            \n            \n                        GAUVAIN\n                        0\n                        13\n                        4\n                        4\n            \n            \n                        GODEFROI\n                        0\n                        1\n                        0\n                        0\n            \n            \n                        GORRE\n                        0\n                        0\n                        0\n                        1\n            \n            \n                        GUENIEVRE\n                        0\n                        1\n                        0\n                        1\n            \n            \n                        IRLANDE\n                        1\n                        0\n                        1\n                        2\n            \n            \n                        JAQUE\n                        0\n                        1\n                        0\n                        1\n            \n            \n                        KEU\n                        0\n                        11\n                        4\n                        0\n            \n            \n                        KEU D'ESTRAUS\n                        0\n                        0\n                        0\n                        1\n            \n            \n                        LAC\n                        0\n                        1\n                        0\n                        0\n            \n            \n                        LANCELOT\n                        0\n                        45\n                        16\n                        16\n            \n            \n                        LANDI\n                        0\n                        0\n                        0\n                        1\n            \n            \n                        LEIGNI\n                        0\n                        1\n                        0\n                        0\n            \n            \n                        LIMOGES\n                        0\n                        0\n                        0\n                        1\n            \n            \n                        LOGRES\n                        0\n                        2\n                        0\n                        3\n            \n            \n                        LONDRES\n                        0\n                        0\n                        0\n                        1\n            \n            \n                        LOOYS\n                        0\n                        0\n                        0\n                        1\n            \n            \n                        MARIES\n                        0\n                        0\n                        0\n                        1\n            \n            \n                        MARTIN\n                        0\n                        1\n                        0\n                        1\n            \n            \n                        MELEAGANT\n                        11\n                        10\n                        15\n                        3\n            \n            \n                        NOAUZ\n                        0\n                        0\n                        0\n                        2\n            \n            \n                        NOEL\n                        0\n                        1\n                        0\n                        0\n            \n            \n                        PANPELUNE\n                        0\n                        1\n                        0\n                        0\n            \n            \n                        PANTECOSTE\n                        0\n                        1\n                        0\n                        0\n            \n            \n                        PEITIERS\n                        0\n                        0\n                        0\n                        1\n            \n            \n                        PERE\n                        1\n                        0\n                        0\n                        2\n            \n            \n                        PILADES\n                        0\n                        0\n                        0\n                        1\n            \n            \n                        PIRAMUS\n                        0\n                        1\n                        0\n                        0\n            \n            \n                        REISON\n                        0\n                        2\n                        0\n                        0\n            \n            \n                        ROBERDIC\n                        0\n                        0\n                        0\n                        1\n            \n            \n                        ROSNE\n                        0\n                        0\n                        0\n                        1\n            \n            \n                        SARRAZIN\n                        1\n                        1\n                        0\n                        0\n            \n            \n                        SELVESTRE\n                        0\n                        0\n                        1\n                        1\n            \n            \n                        TAULAS DE LA DESERTE\n                        0\n                        0\n                        0\n                        1\n            \n            \n                        TESSAILE\n                        0\n                        1\n                        0\n                        0\n            \n            \n                        TOLOSE\n                        0\n                        0\n                        0\n                        1\n            \n            \n                        YSORE\n                        0\n                        0\n                        0\n                        1\n            \n            \n                        YVAIN\n                        0\n                        0\n                        0\n                        1\n            \n    \n\n\n\nA = (FTPM / FTPM.sum())\nB = (FTPM.T / FTPM.T.sum())\nC = A * B\n\n\n# CHAR_PRIORS\n\n\nP_LANCELOT_CH = (B.loc['CH','LANCELOT'] * FIG_PRIORS['CH']) / CHAR_PRIORS['LANCELOT']\n\n\nP_LANCELOT_CH\n\n0.8728284702310675\n\n\n\nA.loc['LANCELOT', 'AD']\n\n0.0\n\n\n\nA.loc['LANCELOT', 'CH']\n\n0.391304347826087\n\n\n\nB.loc['CH','LANCELOT']\n\n0.5844155844155844\n\n\n\\(P(F_{CH}|C_{LANCELOT})\\)\n\nX = FTP.xs('LANCELOT', level=1)\n(X / X.sum()).style.background_gradient()\n\n\n                    n                figure            \n                \n                        CH\n                        0.584416\n            \n            \n                        EN\n                        0.207792\n            \n            \n                        RR\n                        0.207792\n            \n    \n\n\n\nX = FTP.xs('LANCELOT', level=1)\n(X / X.sum()).style.background_gradient()\n\n\n                    n                figure            \n                \n                        CH\n                        0.584416\n            \n            \n                        EN\n                        0.207792\n            \n            \n                        RR\n                        0.207792\n            \n    \n\n\n\ndef get_p_fig_for_char(char_id):\n    X = FTP.xs(char_id, level=1)\n    return (X / X.sum()).sort_values('n', ascending=False).style.background_gradient()\n\n\ndef get_p_char_for_fig(fig_id):\n    X = FTP.loc[fig_id]\n    return (X / X.sum()).sort_values('n', ascending=False).style.background_gradient()    \n\n\nget_p_fig_for_char('LANCELOT')\n\n\n                    n                figure            \n                \n                        CH\n                        0.584416\n            \n            \n                        EN\n                        0.207792\n            \n            \n                        RR\n                        0.207792\n            \n    \n\n\n\nget_p_fig_for_char('MELEAGANT')\n\n\n                    n                figure            \n                \n                        EN\n                        0.384615\n            \n            \n                        AD\n                        0.282051\n            \n            \n                        CH\n                        0.256410\n            \n            \n                        RR\n                        0.076923\n            \n    \n\n\n\nget_p_char_for_fig('CH')\n\n\n                    n                person            \n                \n                        LANCELOT\n                        0.391304\n            \n            \n                        GAUVAIN\n                        0.113043\n            \n            \n                        KEU\n                        0.095652\n            \n            \n                        MELEAGANT\n                        0.086957\n            \n            \n                        DEU\n                        0.034783\n            \n            \n                        BADEMAGU\n                        0.034783\n            \n            \n                        ARTU\n                        0.026087\n            \n            \n                        CRESTIIEN\n                        0.017391\n            \n            \n                        REISON\n                        0.017391\n            \n            \n                        LOGRES\n                        0.017391\n            \n            \n                        FORTUNE\n                        0.017391\n            \n            \n                        SARRAZIN\n                        0.008696\n            \n            \n                        PIRAMUS\n                        0.008696\n            \n            \n                        PANPELUNE\n                        0.008696\n            \n            \n                        NOEL\n                        0.008696\n            \n            \n                        MARTIN\n                        0.008696\n            \n            \n                        PANTECOSTE\n                        0.008696\n            \n            \n                        ABEL\n                        0.008696\n            \n            \n                        LEIGNI\n                        0.008696\n            \n            \n                        LAC\n                        0.008696\n            \n            \n                        JAQUE\n                        0.008696\n            \n            \n                        GUENIEVRE\n                        0.008696\n            \n            \n                        GODEFROI\n                        0.008696\n            \n            \n                        EVE\n                        0.008696\n            \n            \n                        ESPAINGNE\n                        0.008696\n            \n            \n                        DONBES\n                        0.008696\n            \n            \n                        BUCIFAL\n                        0.008696\n            \n            \n                        TESSAILE\n                        0.008696\n            \n    \n\n\n\ndd"
  },
  {
    "objectID": "lessons/M91_Charrette/CharretteTokens.html#corpus",
    "href": "lessons/M91_Charrette/CharretteTokens.html#corpus",
    "title": "Config",
    "section": "Corpus",
    "text": "Corpus\n\nclass Corpus: pass\n\n\ncorpus = Corpus()\n\n\ntables = ['TEXT_WORD','TEXT_GRAMMAR','FIGURE_SEGMENT','FIGURE_TOKEN','FIGURE_TYPE','FIGURE_GENUS']\nfor table in tables:\n    file = f\"{data_in}/{data_prefix}_{table}.csv\"\n    print(file)\n    setattr(corpus, table, pd.read_csv(file, sep='\\t'))\n\n./data_in/figura/ontolige_figura_table_TEXT_WORD.csv\n./data_in/figura/ontolige_figura_table_TEXT_GRAMMAR.csv\n./data_in/figura/ontolige_figura_table_FIGURE_SEGMENT.csv\n./data_in/figura/ontolige_figura_table_FIGURE_TOKEN.csv\n./data_in/figura/ontolige_figura_table_FIGURE_TYPE.csv\n./data_in/figura/ontolige_figura_table_FIGURE_GENUS.csv"
  },
  {
    "objectID": "lessons/M91_Charrette/CharretteTokens.html#tokens",
    "href": "lessons/M91_Charrette/CharretteTokens.html#tokens",
    "title": "Config",
    "section": "Tokens",
    "text": "Tokens\n\nOHCO = ['TEXT_ID','LINE_NUM','TOKEN_NUM']\n\n\nTOKENS = corpus.TEXT_WORD\\\n    .merge(corpus.TEXT_GRAMMAR[['WORD_ID','PART','DFORM']], on='WORD_ID')\\\n    .set_index(OHCO).sort_index()\nTOKENS.index = TOKENS.index.droplevel(0)\nTOKENS = TOKENS[['TOKEN_STR','PUNC','PART','DFORM']]\n\n\nTOKENS\n\n\n\n\n\n  \n    \n      \n      \n      TOKEN_STR\n      PUNC\n      PART\n      DFORM\n    \n    \n      LINE_NUM\n      TOKEN_NUM\n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      1\n      Des\n      &#160;\n      PREPOSITION\n      DES\n    \n    \n      2\n      que\n      &#160;\n      CONJUNCTION\n      QUE\n    \n    \n      3\n      ma\n      &#160;\n      ADJECTIVE\n      MA\n    \n    \n      4\n      dame\n      &#160;\n      NOUN\n      DAME\n    \n    \n      5\n      de\n      &#160;\n      PREPOSITION\n      DE\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      7135\n      5\n      DE\n      &#160;\n      PREPOSITION\n      DE\n    \n    \n      6\n      LANCELOT\n      NaN\n      PROPER\n      LANCELOT\n    \n    \n      7136\n      1\n      DE\n      &#160;\n      PREPOSITION\n      DE\n    \n    \n      2\n      LA\n      &#160;\n      ARTICLE\n      LA\n    \n    \n      3\n      CHARRETE\n      NaN\n      NOUN\n      CHARRETE\n    \n  \n\n43894 rows × 4 columns\n\n\n\n\nTOKENS.DFORM.value_counts().to_frame('n').head(50).plot.bar(figsize=(15,5), rot=45)\n\n<AxesSubplot:>"
  },
  {
    "objectID": "lessons/M91_Charrette/CharretteTokens.html#figures",
    "href": "lessons/M91_Charrette/CharretteTokens.html#figures",
    "title": "Config",
    "section": "Figures",
    "text": "Figures\n\nFIGURES = corpus.FIGURE_SEGMENT\\\n    .merge(corpus.FIGURE_TOKEN, on='FIGURE_TOKEN_ID')\\\n    .merge(corpus.FIGURE_TYPE, on='FIGURE_TYPE_ID')\nFIGURES = FIGURES.set_index(['FIGURE_TOKEN_ID', 'FIGURE_SEGMENT_ID']).sort_index()\nFIGURES = FIGURES[['TITLE', 'OFFSET_START', 'OFFSET_END', 'GENUS', 'SPECIES']]\n\n\nFIGURES\n\n\n\n\n\n  \n    \n      \n      \n      TITLE\n      OFFSET_START\n      OFFSET_END\n      GENUS\n      SPECIES\n    \n    \n      FIGURE_TOKEN_ID\n      FIGURE_SEGMENT_ID\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      351\n      79\n      A1\n      4.01\n      6.06\n      CH\n      2\n    \n    \n      80\n      A2\n      13.01\n      15.05\n      CH\n      2\n    \n    \n      81\n      B1\n      7.01\n      9.05\n      CH\n      2\n    \n    \n      82\n      B2\n      10.01\n      12.07\n      CH\n      2\n    \n    \n      83\n      piv\n      9.05\n      9.05\n      CH\n      2\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      6862\n      8618\n      A1\n      4833.01\n      4833.07\n      CH\n      1A\n    \n    \n      8619\n      A2\n      4838.01\n      4838.07\n      CH\n      1A\n    \n    \n      8620\n      B1\n      4835.01\n      4835.11\n      CH\n      1A\n    \n    \n      8621\n      B2\n      4837.01\n      4837.06\n      CH\n      1A\n    \n    \n      6882\n      8622\n      0\n      188.02\n      188.05\n      OO\n      2\n    \n  \n\n10711 rows × 5 columns"
  },
  {
    "objectID": "lessons/M91_Charrette/CharretteTokens.html#stats",
    "href": "lessons/M91_Charrette/CharretteTokens.html#stats",
    "title": "Config",
    "section": "Stats",
    "text": "Stats\n\nFIGURES.GENUS.value_counts()\n\nRR    3748\nAD    3289\nCH    2202\nEN     795\nOR     543\nOO     134\nName: GENUS, dtype: int64\n\n\n\nTOKENS.PART.value_counts()\n\nVERB           9855\nPRONOUN        7741\nADVERB         6198\nNOUN           5645\nCONJUNCTION    4869\nPREPOSITION    3861\nADJECTIVE      2887\nARTICLE        2248\nPROPER          535\nNUMBER           13\nName: PART, dtype: int64\n\n\n\nTOKENS[TOKENS.PART == 'PROPER'].DFORM.value_counts()\n\nLANCELOT             138\nDEU                  106\nMELEAGANT             61\nGAUVAIN               53\nKEU                   46\n                    ... \nPANTECOSTE             1\nTAULAS                 1\nPEITIERS               1\nCOGUILLANZ             1\nMARIES, LES TROIS      1\nName: DFORM, Length: 82, dtype: int64\n\n\n\nFIGURES[FIGURES.GENUS == 'CH']\n\nFIGURE_TOKEN_ID  FIGURE_SEGMENT_ID\n351              79                      4.01\n                 80                     13.01\n                 81                      7.01\n                 82                     10.01\n                 83                      9.05\n                                       ...   \n3761             5283                 7125.01\n6862             8618                 4833.01\n                 8619                 4838.01\n                 8620                 4835.01\n                 8621                 4837.01\nName: OFFSET_START, Length: 2202, dtype: float64\n\n\n\n# FIGURES['LINE_NUM'] = \nFIGURES.OFFSET_START.astype('int'), ((FIGURES.OFFSET_START - FIGURES.OFFSET_START.astype('int')) * 100).astype('int')\n\n(FIGURE_TOKEN_ID  FIGURE_SEGMENT_ID\n 351              79                      4\n                  80                     13\n                  81                      7\n                  82                     10\n                  83                      9\n                                       ... \n 6862             8618                 4833\n                  8619                 4838\n                  8620                 4835\n                  8621                 4837\n 6882             8622                  188\n Name: OFFSET_START, Length: 10711, dtype: int64,\n FIGURE_TOKEN_ID  FIGURE_SEGMENT_ID\n 351              79                   0\n                  80                   0\n                  81                   0\n                  82                   0\n                  83                   5\n                                      ..\n 6862             8618                 1\n                  8619                 1\n                  8620                 1\n                  8621                 1\n 6882             8622                 2\n Name: OFFSET_START, Length: 10711, dtype: int64)"
  },
  {
    "objectID": "lessons/M92_Helpers/M00_01_VectorizationWithSKLearn.html",
    "href": "lessons/M92_Helpers/M00_01_VectorizationWithSKLearn.html",
    "title": "Metadata",
    "section": "",
    "text": "Set Up\nThis method combines the two.\nLook at the feature token_use_rate $ R = $"
  },
  {
    "objectID": "lessons/M92_Helpers/M00_01_VectorizationWithSKLearn.html#create-dtm",
    "href": "lessons/M92_Helpers/M00_01_VectorizationWithSKLearn.html#create-dtm",
    "title": "Metadata",
    "section": "Create DTM",
    "text": "Create DTM\n\ncount_engine = CountVectorizer(\n    stop_words='english',\n    ngram_range=ngram_range,\n    max_features=n_terms)\n\n\nX = count_engine.fit_transform(DOC.doc_str)\n\n\nX.toarray()\n\narray([[0, 1, 0, ..., 0, 1, 0],\n       [0, 0, 0, ..., 0, 2, 0],\n       [0, 0, 0, ..., 0, 0, 1],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 1, 0],\n       [2, 0, 0, ..., 0, 1, 0]])\n\n\n\n# count_engine.get_feature_names()\n\n\nDTM = pd.DataFrame(X.toarray(), \n                   columns=count_engine.get_feature_names_out(), \n                   index=DOC.index)\n\n\nDTM\n\n\n\n\n\n  \n    \n      \n      \n      abandon\n      abandoned\n      abbess\n      abbey\n      abbot\n      able\n      ablewhite\n      ablewhites\n      abode\n      abroad\n      ...\n      young\n      young ladies\n      young lady\n      young man\n      young men\n      young woman\n      younger\n      youre\n      youth\n      youve\n    \n    \n      book_id\n      chap_id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      adventures\n      1\n      0\n      1\n      0\n      0\n      0\n      3\n      0\n      0\n      0\n      0\n      ...\n      4\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n    \n    \n      2\n      0\n      0\n      0\n      0\n      0\n      5\n      0\n      0\n      0\n      0\n      ...\n      4\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      2\n      0\n    \n    \n      3\n      0\n      0\n      0\n      0\n      0\n      2\n      0\n      0\n      0\n      1\n      ...\n      6\n      0\n      3\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      4\n      0\n      0\n      0\n      0\n      0\n      3\n      0\n      0\n      0\n      1\n      ...\n      24\n      0\n      1\n      5\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      5\n      0\n      0\n      0\n      0\n      0\n      6\n      0\n      0\n      0\n      0\n      ...\n      10\n      0\n      0\n      4\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      udolpho\n      54\n      0\n      0\n      22\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      4\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      3\n      0\n    \n    \n      55\n      0\n      1\n      5\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      56\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      57\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      usher\n      1\n      2\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n  \n\n320 rows × 4000 columns"
  },
  {
    "objectID": "lessons/M92_Helpers/M00_01_VectorizationWithSKLearn.html#get-vocab",
    "href": "lessons/M92_Helpers/M00_01_VectorizationWithSKLearn.html#get-vocab",
    "title": "Metadata",
    "section": "Get VOCAB",
    "text": "Get VOCAB\n\nVOCAB = DTM.sum().to_frame('n')\n\n\nVOCAB.sort_index()\n\n\n\n\n\n  \n    \n      \n      n\n    \n  \n  \n    \n      abandon\n      44\n    \n    \n      abandoned\n      68\n    \n    \n      abbess\n      69\n    \n    \n      abbey\n      114\n    \n    \n      abbot\n      66\n    \n    \n      ...\n      ...\n    \n    \n      young woman\n      34\n    \n    \n      younger\n      59\n    \n    \n      youre\n      127\n    \n    \n      youth\n      219\n    \n    \n      youve\n      61\n    \n  \n\n4000 rows × 1 columns\n\n\n\n\nVOCAB['n_chars'] = VOCAB.apply(lambda x: len(x.name), 1)\nVOCAB['n_tokens'] = VOCAB.apply(lambda x: len(x.name.split()), 1)\n\n\nVOCAB.value_counts('n_tokens')\n\nn_tokens\n1    3718\n2     267\n3      14\n4       1\ndtype: int64\n\n\n\nVOCAB[VOCAB.n_tokens == 2]\n\n\n\n\n\n  \n    \n      \n      n\n      n_chars\n      n_tokens\n    \n  \n  \n    \n      ah said\n      38\n      7\n      2\n    \n    \n      alfred inglethorp\n      52\n      17\n      2\n    \n    \n      annette said\n      29\n      12\n      2\n    \n    \n      baker street\n      59\n      12\n      2\n    \n    \n      baskerville hall\n      30\n      16\n      2\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      young ladies\n      36\n      12\n      2\n    \n    \n      young lady\n      156\n      10\n      2\n    \n    \n      young man\n      165\n      9\n      2\n    \n    \n      young men\n      51\n      9\n      2\n    \n    \n      young woman\n      34\n      11\n      2\n    \n  \n\n267 rows × 3 columns"
  },
  {
    "objectID": "lessons/M92_Helpers/M00_01_VectorizationWithSKLearn.html#create-tfidf",
    "href": "lessons/M92_Helpers/M00_01_VectorizationWithSKLearn.html#create-tfidf",
    "title": "Metadata",
    "section": "Create TFIDF",
    "text": "Create TFIDF\n\n# TfidfTransformer?\n\n\ntfidf_engine = TfidfTransformer(norm='l2', use_idf=True)\n\n\nX1 = tfidf_engine.fit_transform(DTM)\n\n\nTFIDF = pd.DataFrame(X1.toarray(), columns=DTM.columns, index=DTM.index)\n\n\nTFIDF\n\n\n\n\n\n  \n    \n      \n      \n      abandon\n      abandoned\n      abbess\n      abbey\n      abbot\n      able\n      ablewhite\n      ablewhites\n      abode\n      abroad\n      ...\n      young\n      young ladies\n      young lady\n      young man\n      young men\n      young woman\n      younger\n      youre\n      youth\n      youve\n    \n    \n      book_id\n      chap_id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      adventures\n      1\n      0.000000\n      0.011175\n      0.000000\n      0.0\n      0.0\n      0.018096\n      0.0\n      0.0\n      0.0\n      0.000000\n      ...\n      0.018946\n      0.0\n      0.000000\n      0.000000\n      0.012866\n      0.0\n      0.000000\n      0.0\n      0.008457\n      0.000000\n    \n    \n      2\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.025466\n      0.0\n      0.0\n      0.0\n      0.000000\n      ...\n      0.015998\n      0.0\n      0.000000\n      0.007394\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.014282\n      0.000000\n    \n    \n      3\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.012700\n      0.0\n      0.0\n      0.0\n      0.011422\n      ...\n      0.029917\n      0.0\n      0.027653\n      0.000000\n      0.000000\n      0.0\n      0.011261\n      0.0\n      0.000000\n      0.012239\n    \n    \n      4\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.013158\n      0.0\n      0.0\n      0.0\n      0.007889\n      ...\n      0.082655\n      0.0\n      0.006367\n      0.031834\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.006149\n      0.000000\n    \n    \n      5\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.049004\n      0.0\n      0.0\n      0.0\n      0.000000\n      ...\n      0.064134\n      0.0\n      0.000000\n      0.047425\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.000000\n      0.000000\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      udolpho\n      54\n      0.000000\n      0.000000\n      0.287744\n      0.0\n      0.0\n      0.005013\n      0.0\n      0.0\n      0.0\n      0.000000\n      ...\n      0.015747\n      0.0\n      0.000000\n      0.000000\n      0.010694\n      0.0\n      0.000000\n      0.0\n      0.021087\n      0.000000\n    \n    \n      55\n      0.000000\n      0.014358\n      0.101089\n      0.0\n      0.0\n      0.007750\n      0.0\n      0.0\n      0.0\n      0.000000\n      ...\n      0.006085\n      0.0\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.010865\n      0.000000\n    \n    \n      56\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.000000\n      ...\n      0.000000\n      0.0\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.000000\n      0.000000\n    \n    \n      57\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.000000\n      ...\n      0.000000\n      0.0\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.032976\n      0.000000\n    \n    \n      usher\n      1\n      0.037485\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.015968\n      ...\n      0.000000\n      0.0\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.012446\n      0.000000\n    \n  \n\n320 rows × 4000 columns"
  },
  {
    "objectID": "lessons/M92_Helpers/M00_01_VectorizationWithSKLearn.html#add-stats-to-vocab",
    "href": "lessons/M92_Helpers/M00_01_VectorizationWithSKLearn.html#add-stats-to-vocab",
    "title": "Metadata",
    "section": "Add stats to VOCAB",
    "text": "Add stats to VOCAB\n\nVOCAB['tfidf_mean'] = TFIDF.mean()\nVOCAB['df'] = DTM[DTM > 0].count()\nVOCAB['dfidf'] = VOCAB.df * np.log2(len(TFIDF)/VOCAB.df)\n\n\nVOCAB.sort_values('dfidf', ascending=False).head(10)\n\n\n\n\n\n  \n    \n      \n      n\n      n_chars\n      n_tokens\n      tfidf_mean\n      df\n      dfidf\n    \n  \n  \n    \n      curiosity\n      208\n      9\n      1\n      0.005921\n      118\n      169.835635\n    \n    \n      written\n      214\n      7\n      1\n      0.006185\n      118\n      169.835635\n    \n    \n      order\n      227\n      5\n      1\n      0.005313\n      118\n      169.835635\n    \n    \n      reply\n      184\n      5\n      1\n      0.004606\n      118\n      169.835635\n    \n    \n      mentioned\n      180\n      9\n      1\n      0.004824\n      117\n      169.832915\n    \n    \n      perfectly\n      185\n      9\n      1\n      0.004981\n      117\n      169.832915\n    \n    \n      instantly\n      208\n      9\n      1\n      0.005652\n      117\n      169.832915\n    \n    \n      company\n      264\n      7\n      1\n      0.006699\n      119\n      169.826129\n    \n    \n      memory\n      208\n      6\n      1\n      0.006271\n      119\n      169.826129\n    \n    \n      feelings\n      238\n      8\n      1\n      0.009555\n      119\n      169.826129"
  },
  {
    "objectID": "lessons/M92_Helpers/M00_01_VectorizationWithSKLearn.html#create-tfidf-1",
    "href": "lessons/M92_Helpers/M00_01_VectorizationWithSKLearn.html#create-tfidf-1",
    "title": "Metadata",
    "section": "Create TFIDF",
    "text": "Create TFIDF\n\ntfidf_engine2 = TfidfVectorizer(\n    stop_words='english',\n    ngram_range=ngram_range,\n    max_features=n_terms,\n    norm='l2', \n    use_idf=True)\n\n\nX2 = tfidf_engine2.fit_transform(DOC.doc_str)\n\n\nTFIDF2 = pd.DataFrame(X2.toarray(), columns=tfidf_engine2.get_feature_names_out(), index=DTM.index)\n\n\nTFIDF2\n\n\n\n\n\n  \n    \n      \n      \n      abandon\n      abandoned\n      abbess\n      abbey\n      abbot\n      able\n      ablewhite\n      ablewhites\n      abode\n      abroad\n      ...\n      young\n      young ladies\n      young lady\n      young man\n      young men\n      young woman\n      younger\n      youre\n      youth\n      youve\n    \n    \n      book_id\n      chap_id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      adventures\n      1\n      0.000000\n      0.011175\n      0.000000\n      0.0\n      0.0\n      0.018096\n      0.0\n      0.0\n      0.0\n      0.000000\n      ...\n      0.018946\n      0.0\n      0.000000\n      0.000000\n      0.012866\n      0.0\n      0.000000\n      0.0\n      0.008457\n      0.000000\n    \n    \n      2\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.025466\n      0.0\n      0.0\n      0.0\n      0.000000\n      ...\n      0.015998\n      0.0\n      0.000000\n      0.007394\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.014282\n      0.000000\n    \n    \n      3\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.012700\n      0.0\n      0.0\n      0.0\n      0.011422\n      ...\n      0.029917\n      0.0\n      0.027653\n      0.000000\n      0.000000\n      0.0\n      0.011261\n      0.0\n      0.000000\n      0.012239\n    \n    \n      4\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.013158\n      0.0\n      0.0\n      0.0\n      0.007889\n      ...\n      0.082655\n      0.0\n      0.006367\n      0.031834\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.006149\n      0.000000\n    \n    \n      5\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.049004\n      0.0\n      0.0\n      0.0\n      0.000000\n      ...\n      0.064134\n      0.0\n      0.000000\n      0.047425\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.000000\n      0.000000\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      udolpho\n      54\n      0.000000\n      0.000000\n      0.287744\n      0.0\n      0.0\n      0.005013\n      0.0\n      0.0\n      0.0\n      0.000000\n      ...\n      0.015747\n      0.0\n      0.000000\n      0.000000\n      0.010694\n      0.0\n      0.000000\n      0.0\n      0.021087\n      0.000000\n    \n    \n      55\n      0.000000\n      0.014358\n      0.101089\n      0.0\n      0.0\n      0.007750\n      0.0\n      0.0\n      0.0\n      0.000000\n      ...\n      0.006085\n      0.0\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.010865\n      0.000000\n    \n    \n      56\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.000000\n      ...\n      0.000000\n      0.0\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.000000\n      0.000000\n    \n    \n      57\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.000000\n      ...\n      0.000000\n      0.0\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.032976\n      0.000000\n    \n    \n      usher\n      1\n      0.037485\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.015968\n      ...\n      0.000000\n      0.0\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.0\n      0.012446\n      0.000000\n    \n  \n\n320 rows × 4000 columns"
  },
  {
    "objectID": "lessons/M92_Helpers/M00_01_VectorizationWithSKLearn.html#get-vocab-1",
    "href": "lessons/M92_Helpers/M00_01_VectorizationWithSKLearn.html#get-vocab-1",
    "title": "Metadata",
    "section": "Get VOCAB",
    "text": "Get VOCAB\n\nVOCAB2 = TFIDF2.mean().to_frame('tfidf_mean')\n\n\nVOCAB2\n\n\n\n\n\n  \n    \n      \n      tfidf_mean\n    \n  \n  \n    \n      abandon\n      0.001670\n    \n    \n      abandoned\n      0.002396\n    \n    \n      abbess\n      0.003529\n    \n    \n      abbey\n      0.006194\n    \n    \n      abbot\n      0.002089\n    \n    \n      ...\n      ...\n    \n    \n      young woman\n      0.001985\n    \n    \n      younger\n      0.002918\n    \n    \n      youre\n      0.005334\n    \n    \n      youth\n      0.005786\n    \n    \n      youve\n      0.002944\n    \n  \n\n4000 rows × 1 columns"
  },
  {
    "objectID": "lessons/M92_Helpers/M00_01_VectorizationWithSKLearn.html#add-stats-to-vocab-1",
    "href": "lessons/M92_Helpers/M00_01_VectorizationWithSKLearn.html#add-stats-to-vocab-1",
    "title": "Metadata",
    "section": "Add stats to VOCAB",
    "text": "Add stats to VOCAB\n\nVOCAB2['df'] = TFIDF2[TFIDF2 > 0].count()\nVOCAB2['dfidf'] = VOCAB2.df * np.log2(len(TFIDF2)/VOCAB2.df)\n\n\nVOCAB2.sort_values('dfidf', ascending=False).head(10)\n\n\n\n\n\n  \n    \n      \n      tfidf_mean\n      df\n      dfidf\n    \n  \n  \n    \n      curiosity\n      0.005921\n      118\n      169.835635\n    \n    \n      written\n      0.006185\n      118\n      169.835635\n    \n    \n      order\n      0.005313\n      118\n      169.835635\n    \n    \n      reply\n      0.004606\n      118\n      169.835635\n    \n    \n      mentioned\n      0.004824\n      117\n      169.832915\n    \n    \n      perfectly\n      0.004981\n      117\n      169.832915\n    \n    \n      instantly\n      0.005652\n      117\n      169.832915\n    \n    \n      company\n      0.006699\n      119\n      169.826129\n    \n    \n      memory\n      0.006271\n      119\n      169.826129\n    \n    \n      feelings\n      0.009555\n      119\n      169.826129"
  },
  {
    "objectID": "lessons/M92_Helpers/M00_02_SpaCy.html",
    "href": "lessons/M92_Helpers/M00_02_SpaCy.html",
    "title": "Metadata",
    "section": "",
    "text": "Notes"
  },
  {
    "objectID": "lessons/M92_Helpers/M00_02_SpaCy.html#how-to-install",
    "href": "lessons/M92_Helpers/M00_02_SpaCy.html#how-to-install",
    "title": "Metadata",
    "section": "How to install",
    "text": "How to install\n\nconda install -c conda-forge spacy\npython -m spacy download en_core_web_sm"
  },
  {
    "objectID": "lessons/M92_Helpers/M00_02_SpaCy.html#about-spacy",
    "href": "lessons/M92_Helpers/M00_02_SpaCy.html#about-spacy",
    "title": "Metadata",
    "section": "About SpaCy",
    "text": "About SpaCy\n\nMore than a library; it is an entire platform for text processing. It is designed to be integrated into production-level data products.\nDesigned for performance. It uses best of breed tools and can be somewhat opaque.\nA replacement for NLTK, especially for linguistic annonation in the preprocessing stages. It can work with Gensim and SciKit Learn.\nDesigned to be accessed by API, not be dumping to a database – but it can be done.\nShould be installed in its own Python environment.\n\nFor example, do conda create -n spacy and then do conda activate spacy. From there, install SpaCy and everything else you need for your project."
  },
  {
    "objectID": "lessons/M92_Helpers/M00_02_SpaCy.html#spacys-object-model",
    "href": "lessons/M92_Helpers/M00_02_SpaCy.html#spacys-object-model",
    "title": "Metadata",
    "section": "SpaCy’s Object Model",
    "text": "SpaCy’s Object Model\n\nNote: this is not a true data model, but an object model that bundles data with algorithms (methods)."
  },
  {
    "objectID": "lessons/M92_Helpers/M00_02_SpaCy.html#config",
    "href": "lessons/M92_Helpers/M00_02_SpaCy.html#config",
    "title": "Metadata",
    "section": "Config",
    "text": "Config\n\ndata_home = \"../data\"\nlocal_lib = \"../lib\"\ndata_prefix = 'novels'\nOHCO = ['book_id','chap_id','para_num','sent_num','token_num']"
  },
  {
    "objectID": "lessons/M92_Helpers/M00_02_SpaCy.html#import-library",
    "href": "lessons/M92_Helpers/M00_02_SpaCy.html#import-library",
    "title": "Metadata",
    "section": "Import Library",
    "text": "Import Library\n\nimport pandas as pd\nimport numpy as np\nimport tqdm as tqdm\nimport spacy\n\n\nspacy.__version__\n\n'2.3.2'"
  },
  {
    "objectID": "lessons/M92_Helpers/M00_02_SpaCy.html#gather-chaps",
    "href": "lessons/M92_Helpers/M00_02_SpaCy.html#gather-chaps",
    "title": "Metadata",
    "section": "Gather CHAPS",
    "text": "Gather CHAPS\n\nSENTS = gather_docs(CORPUS, 4) # We do this to preserve sentence boundaries in CHAPs\nCHAPS = gather_docs(SENTS, 2, str_col='doc_str', glue='. ')\n\n\nCHAPS\n\n\n\n\n\n  \n    \n      \n      \n      doc_str\n    \n    \n      book_id\n      chap_id\n      \n    \n  \n  \n    \n      adventures\n      1\n      a scandal in bohemia. i. to sherlock holmes sh...\n    \n    \n      2\n      the red headed league. i had called upon my fr...\n    \n    \n      3\n      a case of identity. my dear fellow said sherlo...\n    \n    \n      4\n      the boscombe valley mystery. we were seated at...\n    \n    \n      5\n      the five orange pips. when i glance over my no...\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      udolpho\n      54\n      vi. unnatural deeds do breed unnatural trouble...\n    \n    \n      55\n      vii. but in these cases we still have judgment...\n    \n    \n      56\n      viii. then fresh tears stood on her cheek as d...\n    \n    \n      57\n      ix. now my task is smoothly done i can fly or ...\n    \n    \n      usher\n      1\n      fall of the house of usher. son coeur est un l...\n    \n  \n\n320 rows × 1 columns"
  },
  {
    "objectID": "lessons/M92_Helpers/M00_02_SpaCy.html#load-statistical-models",
    "href": "lessons/M92_Helpers/M00_02_SpaCy.html#load-statistical-models",
    "title": "Metadata",
    "section": "Load Statistical Models",
    "text": "Load Statistical Models\nThese are also called “trained pipelines” in the documentation.\nTrained pipelines for English: * en_core_web_sm: English pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer. * en_core_web_md: English pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer. * en_core_web_lg: English pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer. * en_core_web_trf: English transformer pipeline (roberta-base). Components: transformer, tagger, parser, ner, attribute_ruler, lemmatizer.\n\nSee the docs for more.\n\nspacy.__version__\n\n'2.3.2'\n\n\n\ntrained_pipeline = 'en_core_web_md'\n\n\n# !python -m spacy download {trained_pipeline}\n\n\n# doc = spacy.nlp(doc_str)\n\n\nnlp = spacy.load(trained_pipeline)"
  },
  {
    "objectID": "lessons/M92_Helpers/M00_02_SpaCy.html#generate-annotations",
    "href": "lessons/M92_Helpers/M00_02_SpaCy.html#generate-annotations",
    "title": "Metadata",
    "section": "Generate Annotations",
    "text": "Generate Annotations\n\n# pipleline = [\"tok2vec\", \"tagger\", \"parser\", \"ner\", \"attribute_ruler\", \"lemmatizer\"]\n# disable= [\"attribute_ruler\", \"lemmatizer\", \"parser\"]\ndisable = []\nDOCS = [doc.to_json() for doc in nlp.pipe(CHAPS.doc_str.values, disable=disable)]"
  },
  {
    "objectID": "lessons/M92_Helpers/M00_02_SpaCy.html#convert-to-dataframes",
    "href": "lessons/M92_Helpers/M00_02_SpaCy.html#convert-to-dataframes",
    "title": "Metadata",
    "section": "Convert to DataFrames",
    "text": "Convert to DataFrames\n\nfeatures = list(DOCS[0].keys())\n\n\nfeatures\n\n['text', 'ents', 'sents', 'tokens']\n\n\n\nfeature_data = {f:[] for f in features}\nfor i in range(len(DOCS)):    \n    text = DOCS[i]['text']\n    for feature in features[1:]:\n        df = pd.DataFrame(DOCS[i][feature])\n        df[f'{feature[:-1]}_str'] = df.apply(lambda x: text[x.start:x.end], 1)\n        df['doc_id'] = i\n        feature_data[feature].append(df)\n    \nclass mySpaCy(): pass\nspcy = mySpaCy()\nfor feature in features[1:]:\n    setattr(spcy, feature, pd.concat(feature_data[feature]).rename_axis(f'{feature[:-1]}_id'))\n\n\nspcy.ents\n\n\n\n\n\n  \n    \n      \n      start\n      end\n      label\n      ent_str\n      doc_id\n    \n    \n      ent_id\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      13\n      20\n      GPE\n      bohemia\n      0\n    \n    \n      1\n      22\n      24\n      ORG\n      i.\n      0\n    \n    \n      2\n      28\n      43\n      GPE\n      sherlock holmes\n      0\n    \n    \n      3\n      244\n      255\n      PERSON\n      irene adler\n      0\n    \n    \n      4\n      279\n      282\n      CARDINAL\n      one\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      108\n      39150\n      39158\n      PERSON\n      madeline\n      319\n    \n    \n      109\n      39162\n      39167\n      ORG\n      usher\n      319\n    \n    \n      110\n      39938\n      39946\n      LOC\n      red moon\n      319\n    \n    \n      111\n      40392\n      40400\n      CARDINAL\n      thousand\n      319\n    \n    \n      112\n      40425\n      40434\n      LOC\n      dank tarn\n      319\n    \n  \n\n47895 rows × 5 columns"
  },
  {
    "objectID": "lessons/M92_Helpers/M00_02_SpaCy.html#explore",
    "href": "lessons/M92_Helpers/M00_02_SpaCy.html#explore",
    "title": "Metadata",
    "section": "Explore",
    "text": "Explore\n\nTOKEN\n\nspcy.tokens\n\n\n\n\n\n  \n    \n      \n      id\n      start\n      end\n      pos\n      tag\n      dep\n      head\n      token_str\n      doc_id\n    \n    \n      token_id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0\n      0\n      1\n      DET\n      DT\n      det\n      1\n      a\n      0\n    \n    \n      1\n      1\n      2\n      9\n      NOUN\n      NN\n      ROOT\n      1\n      scandal\n      0\n    \n    \n      2\n      2\n      10\n      12\n      ADP\n      IN\n      prep\n      1\n      in\n      0\n    \n    \n      3\n      3\n      13\n      20\n      PROPN\n      NNP\n      pobj\n      2\n      bohemia\n      0\n    \n    \n      4\n      4\n      20\n      21\n      PUNCT\n      .\n      punct\n      1\n      .\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      7453\n      7453\n      40494\n      40496\n      ADP\n      IN\n      prep\n      7452\n      of\n      319\n    \n    \n      7454\n      7454\n      40497\n      40500\n      DET\n      DT\n      det\n      7455\n      the\n      319\n    \n    \n      7455\n      7455\n      40501\n      40506\n      PROPN\n      NNP\n      pobj\n      7453\n      house\n      319\n    \n    \n      7456\n      7456\n      40507\n      40509\n      ADP\n      IN\n      prep\n      7455\n      of\n      319\n    \n    \n      7457\n      7457\n      40510\n      40515\n      PROPN\n      NNP\n      pobj\n      7456\n      usher\n      319\n    \n  \n\n1588722 rows × 9 columns\n\n\n\n\nCORPUS\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      \n      pos\n      term_str\n    \n    \n      book_id\n      chap_id\n      para_num\n      sent_num\n      token_num\n      \n      \n    \n  \n  \n    \n      secretadversary\n      1\n      0\n      1\n      0\n      DT\n      the\n    \n    \n      1\n      NNP\n      young\n    \n    \n      2\n      NNP\n      adventurers\n    \n    \n      3\n      NNP\n      ltd\n    \n    \n      1\n      0\n      0\n      JJ\n      tommy\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      baskervilles\n      11\n      114\n      1\n      7\n      RBR\n      more\n    \n    \n      8\n      JJ\n      comfortable\n    \n    \n      9\n      IN\n      outside\n    \n    \n      10\n      IN\n      than\n    \n    \n      11\n      NN\n      in\n    \n  \n\n1500417 rows × 2 columns\n\n\n\n\n\nVOCAB\n\nspcy.VOCAB = spcy.tokens.value_counts('token_str').to_frame('n')\n\n\nspcy.VOCAB['max_pos'] = spcy.tokens.value_counts(['token_str','pos']).unstack().idxmax(1)\n\n\nspcy.VOCAB[spcy.VOCAB.max_pos == 'PROPN'].sample(10)\n\n\n\n\n\n  \n    \n      \n      n\n      max_pos\n    \n    \n      token_str\n      \n      \n    \n  \n  \n    \n      theophilus\n      1\n      PROPN\n    \n    \n      raikes\n      10\n      PROPN\n    \n    \n      abramoff\n      1\n      PROPN\n    \n    \n      markham\n      26\n      PROPN\n    \n    \n      frizinghall\n      91\n      PROPN\n    \n    \n      brigham\n      2\n      PROPN\n    \n    \n      journal\n      85\n      PROPN\n    \n    \n      wwwgutenbergorgcontact\n      1\n      PROPN\n    \n    \n      roberts\n      7\n      PROPN\n    \n    \n      carlton\n      3\n      PROPN\n    \n  \n\n\n\n\n\n\nENT\n\nspcy.ents.label.value_counts()\n\nPERSON         24625\nCARDINAL        4707\nTIME            3740\nDATE            3487\nGPE             3259\nORG             2646\nORDINAL         1991\nNORP            1440\nLOC              677\nFAC              558\nQUANTITY         365\nPRODUCT          162\nLANGUAGE          99\nMONEY             46\nEVENT             46\nWORK_OF_ART       39\nLAW                8\nName: label, dtype: int64\n\n\n\nspcy.ents[spcy.ents.label=='PERSON'].sample(10)\n\n\n\n\n\n  \n    \n      \n      start\n      end\n      label\n      ent_str\n      doc_id\n    \n    \n      ent_id\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      80\n      8524\n      8530\n      PERSON\n      julius\n      232\n    \n    \n      128\n      17184\n      17190\n      PERSON\n      howard\n      257\n    \n    \n      0\n      305\n      311\n      PERSON\n      philip\n      189\n    \n    \n      20\n      2853\n      2861\n      PERSON\n      lawrence\n      259\n    \n    \n      115\n      20437\n      20452\n      PERSON\n      sherlock holmes\n      23\n    \n    \n      146\n      21519\n      21525\n      PERSON\n      holmes\n      1\n    \n    \n      156\n      23087\n      23093\n      PERSON\n      holmes\n      7\n    \n    \n      118\n      15819\n      15826\n      PERSON\n      wilkins\n      260\n    \n    \n      51\n      16506\n      16512\n      PERSON\n      buskin\n      194\n    \n    \n      187\n      32382\n      32389\n      PERSON\n      manfred\n      27\n    \n  \n\n\n\n\n\nspcy.ents[spcy.ents.label=='PERSON'].value_counts(['doc_id','ent_str']).unstack().sum().sort_values()\n\nent_str\na dr adams                      1.0\nmanfred prince of otranto       1.0\nmanfred rose                    1.0\nmanfred thou                    1.0\nmanfred thy                     1.0\n                              ...  \nmontoni                       427.0\nholmes                        428.0\nannette                       443.0\ntommy                         507.0\nemily                        1974.0\nLength: 3124, dtype: float64\n\n\n\nspcy.ents[spcy.ents.label=='ORG'].sample(10)\n\n\n\n\n\n  \n    \n      \n      start\n      end\n      label\n      ent_str\n      doc_id\n    \n    \n      ent_id\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      224\n      41163\n      41175\n      ORG\n      westminister\n      63\n    \n    \n      164\n      21049\n      21056\n      ORG\n      journal\n      92\n    \n    \n      2\n      51\n      61\n      ORG\n      hillingham\n      48\n    \n    \n      80\n      16291\n      16310\n      ORG\n      sie nicht verstehen\n      242\n    \n    \n      17\n      1871\n      1884\n      ORG\n      latour claret\n      141\n    \n    \n      190\n      28109\n      28114\n      ORG\n      onlie\n      190\n    \n    \n      471\n      93573\n      93590\n      ORG\n      the court of rome\n      96\n    \n    \n      93\n      17020\n      17031\n      ORG\n      black larch\n      279\n    \n    \n      142\n      21034\n      21041\n      ORG\n      du pont\n      295\n    \n    \n      91\n      18173\n      18182\n      ORG\n      ladyships\n      126\n    \n  \n\n\n\n\n\nspcy.ents[spcy.ents.label=='DATE'].sample(10)\n\n\n\n\n\n  \n    \n      \n      start\n      end\n      label\n      ent_str\n      doc_id\n    \n    \n      ent_id\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      482\n      79206\n      79216\n      DATE\n      four weeks\n      92\n    \n    \n      232\n      36008\n      36015\n      DATE\n      january\n      4\n    \n    \n      25\n      5398\n      5413\n      DATE\n      merry christmas\n      32\n    \n    \n      0\n      0\n      10\n      DATE\n      a few days\n      169\n    \n    \n      178\n      30106\n      30123\n      DATE\n      the next ten days\n      149\n    \n    \n      347\n      50113\n      50121\n      DATE\n      tomorrow\n      149\n    \n    \n      136\n      19575\n      19582\n      DATE\n      one day\n      186\n    \n    \n      87\n      16189\n      16200\n      DATE\n      seventeenth\n      144\n    \n    \n      20\n      3489\n      3495\n      DATE\n      morrow\n      63\n    \n    \n      118\n      20145\n      20153\n      DATE\n      tomorrow\n      160\n    \n  \n\n\n\n\n\n\nSENT\n\nspcy.sents\n\n\n\n\n\n  \n    \n      \n      start\n      end\n      sent_str\n      doc_id\n    \n    \n      sent_id\n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0\n      21\n      a scandal in bohemia.\n      0\n    \n    \n      1\n      22\n      43\n      i. to sherlock holmes\n      0\n    \n    \n      2\n      44\n      68\n      she is always the woman.\n      0\n    \n    \n      3\n      69\n      126\n      i have seldom heard him mention her under any ...\n      0\n    \n    \n      4\n      127\n      190\n      in his eyes she eclipses and predominates the ...\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      303\n      39717\n      39764\n      suddenly there shot along the path a wild light\n      319\n    \n    \n      304\n      39765\n      39885\n      and i turned to see whence a gleam so unusual ...\n      319\n    \n    \n      305\n      39886\n      40123\n      the radiance was that of the full setting and ...\n      319\n    \n    \n      306\n      40124\n      40270\n      while i gazed this fissure rapidly widened the...\n      319\n    \n    \n      307\n      40271\n      40515\n      my brain reeled as i saw the mighty walls rush...\n      319\n    \n  \n\n101403 rows × 4 columns\n\n\n\n\nSENTS\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      doc_str\n    \n    \n      book_id\n      chap_id\n      para_num\n      sent_num\n      \n    \n  \n  \n    \n      adventures\n      1\n      0\n      1\n      a scandal in bohemia\n    \n    \n      1\n      0\n      i\n    \n    \n      2\n      0\n      to sherlock holmes she is always the woman\n    \n    \n      1\n      i have seldom heard him mention her under any ...\n    \n    \n      2\n      in his eyes she eclipses and predominates the ...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      usher\n      1\n      47\n      0\n      from that chamber and from that mansion i fled...\n    \n    \n      1\n      the storm was still abroad in all its wrath as...\n    \n    \n      2\n      suddenly there shot along the path a wild ligh...\n    \n    \n      3\n      the radiance was that of the full setting and ...\n    \n    \n      4\n      while i gazed this fissure rapidly widened the...\n    \n  \n\n84282 rows × 1 columns"
  }
]