{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b15f46-a963-41df-918f-2d28ba2bf627",
   "metadata": {},
   "source": [
    "# LDA with Gensim\n",
    "\n",
    "```yaml\n",
    "Course:    DS 5001 \n",
    "Module:    08 Lab\n",
    "Topic:     LDA with Gensim\n",
    "Author:    R.C. Alvarado\n",
    "Date:      05 March 2024\n",
    "```\n",
    "\n",
    "**Purpose:** Using Gensim to generate topic models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51059c6-192b-4144-92b1-34429a72007c",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090f671b-3226-4d3e-b693-8bed61b7fcc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfd1a939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../../../env.ini')\n",
    "data_home = config['DEFAULT']['data_home']\n",
    "output_dir = config['DEFAULT']['output_dir']\n",
    "local_lib = config['DEFAULT']['local_lib']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99dfd55a-39e9-4cbc-b742-43b94abba488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_prefix = 'austen-melville'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c4de45-a051-4ab7-833f-04fdb8ab4339",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e52f9f39-a99f-48a6-bbc5-ea966ed0d59d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d32f9-7e9f-41a4-a18f-d95497e6c092",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24578ef9-72fd-4406-ad0f-bb2f0bb96b49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TOKENS = pd.read_csv(f\"{output_dir}/{data_prefix}-TOKEN.csv\")\n",
    "OHCO = list(TOKENS.columns[:5])\n",
    "TOKENS = TOKENS.set_index(OHCO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6085ab1d-4008-4171-a259-001ddbead6b7",
   "metadata": {},
   "source": [
    "# Create Gensum Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0d4894-e927-4197-998e-a136013b008d",
   "metadata": {},
   "source": [
    "We convert our TOKENS table into Gensim data structures.\n",
    "\n",
    "Gensim uses lists and dictionaries wrapped in objects to provide various kinds of access.\n",
    "\n",
    "Note that this is a bit of reinventing the wheel and it's not faster than what we've been doing with Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffb1dc8d-8796-482d-87f4-349c9f2bf8ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DOCS = TOKENS.dropna().groupby(OHCO[:3]).term_str.apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82f9a996-82b9-4a23-aa47-cc58c08e7a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "book_id  chap_num  para_num\n",
       "105      1         1           [sir, walter, elliot, of, kellynch, hall, in, ...\n",
       "                   2                                [elliot, of, kellynch, hall]\n",
       "                   3           [walter, elliot, born, march, 1, 1760, married...\n",
       "                   4           [precisely, such, had, the, paragraph, origina...\n",
       "                   5           [then, followed, the, history, and, rise, of, ...\n",
       "                                                     ...                        \n",
       "53861    15        20          [the, innate, disdain, of, regularly, bred, se...\n",
       "                   21          [is, there, any, hard, work, to, be, done, and...\n",
       "                   22          [as, yet, the, intellect, of, the, gee, has, b...\n",
       "                   23          [the, above, account, may, perhaps, among, the...\n",
       "                   24          [thus, much, for, a, general, sketchy, view, o...\n",
       "Name: term_str, Length: 30807, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af52990f-a8ea-42af-8cb8-bf2fd640cc8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab = Dictionary(DOCS.tolist()) \n",
    "vocab.filter_extremes(no_below=20, no_above=0.5)\n",
    "corpus = [vocab.doc2bow(doc) for doc in DOCS.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beeee4e-ec4f-4fcc-84b7-5dbafc816ba7",
   "metadata": {},
   "source": [
    "# Generate LDA\n",
    "\n",
    "Next, we train a model using Gensim's LDA topic modeler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8a6246-2438-4fa8-9d65-3ce22f13037f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "We set some training parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b317782-29ae-44a8-a2f4-6b7f596b6bd2",
   "metadata": {},
   "source": [
    "Regarding these, here are some notes from the Gensim site:\n",
    "\n",
    "> First of all, the elephant in the room: how many topics do I need? There is\n",
    "really no easy answer for this, it will depend on both your data and your\n",
    "application. I have used 10 topics here because I wanted to have a few topics\n",
    "that I could interpret and \"label\", and because that turned out to give me\n",
    "reasonably good results. You might not need to interpret all your topics, so\n",
    "you could use a large number of topics, for example 100.\n",
    "\n",
    "> ``chunksize`` controls how many documents are processed at a time in the\n",
    "training algorithm. Increasing chunksize will speed up training, at least as\n",
    "long as the chunk of documents easily fit into memory. I've set ``chunksize =\n",
    "2000``, which is more than the amount of documents, so I process all the\n",
    "data in one go. Chunksize can however influence the quality of the model, as\n",
    "discussed in Hoffman and co-authors [2], but the difference was not\n",
    "substantial in this case.\n",
    "\n",
    "> ``passes`` controls how often we train the model on the entire corpus.\n",
    "Another word for passes might be \"epochs\". ``iterations`` is somewhat\n",
    "technical, but essentially it controls how often we repeat a particular loop\n",
    "over each document. It is important to set the number of \"passes\" and\n",
    "\"iterations\" high enough.\n",
    "\n",
    "> I suggest the following way to choose iterations and passes. First, enable\n",
    "logging (as described in many Gensim tutorials), and set ``eval_every = 1``\n",
    "in ``LdaModel``. When training the model look for a line in the log that\n",
    "looks something like this::\n",
    "\n",
    ">```2016-06-21 15:40:06,753 - gensim.models.ldamodel - DEBUG - 68/1566 documents converged within 400 iterations```\n",
    "\n",
    "> If you set ``passes = 20`` you will see this line 20 times. Make sure that by\n",
    "the final passes, most of the documents have converged. So you want to choose\n",
    "both passes and iterations to be high enough for this to happen.\n",
    "\n",
    "> We set ``alpha = 'auto'`` and ``eta = 'auto'``. Again this is somewhat\n",
    "technical, but essentially we are automatically learning two parameters in\n",
    "the model that we usually would have to specify explicitly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7354845e-7d4d-4d0f-a30f-6aa7cad45fe0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_topics = 40\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ff2937-01be-4ce0-aa51-d4943764a239",
   "metadata": {
    "tags": []
   },
   "source": [
    "We also make an index to word dictionary, since the model wants it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "644127b3-a167-4f59-8e00-212fca3f8407",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp = vocab[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = vocab.id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220a36a2-5a65-4aa3-b12f-3262cf67c4f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5080570b-f0c0-4c3c-b23d-67b6a77a7685",
   "metadata": {},
   "source": [
    "# Inspect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b15af7-44b5-4ad2-afb2-9bf612b18bae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PHI_gs = pd.DataFrame(model.get_topics(), columns=[v[1] for v in vocab.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbd918c-7bbc-4b15-b5bd-c0f9013e4aca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "THETA_gs = pd.DataFrame([(i, t[0], t[1]) \n",
    "    for i, d in enumerate(model.get_document_topics(corpus)) \n",
    "    for t in d],\n",
    "    columns=['doc_id', 'topic_id', 'topic_weight'])\\\n",
    "    .set_index(['doc_id','topic_id'])\\\n",
    "    .unstack(fill_value=0)\n",
    "THETA_gs.index = DOCS.index\n",
    "THETA_gs.columns = THETA_gs.columns.droplevel(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd182314-a8d9-4c7e-a997-e0074bb99dff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "THETA_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9458e98e-a534-403a-baa9-c9cb307a1695",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "THETA_gs.iloc[7].plot.bar(rot=45, figsize=(10,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7a3740-5ce2-4549-a334-26234648723b",
   "metadata": {},
   "source": [
    "# Topic Coherence \n",
    "\n",
    "> We can compute the topic coherence of each topic. Below we display the\n",
    "average topic coherence and print the topics in order of topic coherence.\n",
    ">\n",
    "> Note that we use the \"Umass\" topic coherence measure here (see\n",
    ":py:func:`gensim.models.ldamodel.LdaModel.top_topics`), Gensim has recently\n",
    "obtained an implementation of the \"AKSW\" topic coherence measure (see\n",
    "accompanying blog post, http://rare-technologies.com/what-is-topic-coherence/).\n",
    ">\n",
    "> If you are familiar with the subject of the articles in this dataset, you can\n",
    "see that the topics below make a lot of sense. However, they are not without\n",
    "flaws. We can see that there is substantial overlap between some topics,\n",
    "others are hard to interpret, and most of them have at least some terms that\n",
    "seem out of place. If you were able to do better, feel free to share your\n",
    "methods on the blog at http://rare-technologies.com/lda-training-tips/ !\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddd469c-c991-47db-9e31-33f90c230fab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_topics = model.top_topics(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca9226b-3e17-4fcf-aaf5-df3d3bc21537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3159cd-c0e7-4402-8659-55ec6a025c11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# top_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e469c1-0a9d-45fc-837c-1d2b02601323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TOPICS = pd.DataFrame([(i, j, topic[1], *reversed(term_pair))\n",
    "  for i, topic in enumerate(top_topics)\n",
    "    for j, term_pair in enumerate(topic[0])], \n",
    "columns=['topic_id', 'term_rank', 'll', 'term_str', 'term_weight']).set_index(['topic_id','term_rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2975881-5007-4451-a329-e0bfe5f42d80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TOPICS.term_str.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd2563a-4f2e-4290-99c2-8aaf0e4e6654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ds5001]",
   "language": "python",
   "name": "conda-env-ds5001-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
