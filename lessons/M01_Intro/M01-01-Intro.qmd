# Introduction

## What is ETA?

Welcome to the world of Exploratory Text Analytics (ETA). This course is designed to delve into the concept of ETA, a term I've coined by amalgamating existing fields: 'text analytics' and 'exploratory data analysis'. The latter, with roots tracing back to the 1960s, merges with the relatively recent term 'text analytics', which itself intersects with age-old disciplines.

ETA stands distinct from conventional text analytics or text mining in several ways. Primarily, it focuses on long-form texts such as novels, essays, newspapers, blogs, and journals. These texts, characterized by their intrinsic structure and integrity, are considered independent units of discourse within broader communicative exchanges.

The essence of ETA lies in uncovering 'latent content' within these texts. This content spans various dimensions, including cultural, psychological, and sociological elements. We delve into concepts like cognitive symbols, values, emotions, events, and what are known as 'collective representations'. This term, coined by the French Sociology School and popularized by Ã‰mile Durkheim, encapsulates ideas shared among people, forming a cornerstone of cultural understanding.

ETA's methodology, primarily unsupervised, aims to facilitate human interpretation of large text corpora. While its techniques align with those in supervised learning, its goal is descriptive rather than explanatory. This approach underscores the importance of human involvement in interpreting vast textual data.

A key aspect of ETA is the recognition that data science, often seen as a blend of computer science, mathematics, and statistics, also requires domain expertise. In the context of ETA, this translates to an understanding of texts not just as ubiquitous entities but as complex and fascinating artifacts. This understanding draws from a myriad of disciplines including linguistics, text criticism, hermeneutics, the philosophy of language, and media studies.

In this course, we commence with an introduction to hermeneutics, the art of interpretation, setting the stage for our exploration of texts. This foundational understanding will guide us in unraveling the layers of meaning within texts and contribute significantly to the field of text analytics. Join me in this intriguing journey as we explore the depths of textual data and uncover the latent narratives within.

## Applications

Let's look at some applications of exploratory text analytics, including very general applications and use cases.

The first of these is the use of exploratory text analytics, or text analytics in general, to extract features from unstructured data to support machine learning. By machine learning, I am including both supervised and unsupervised methods. We'll talk a little more about what those things mean in this context. For example, if you're trying to predict the effect of an ad campaign, say, on sales of a product, among the data that you might have to help you predict this could be textual data in the form of reviews, copy, or content from the ads, Twitter feed responses, and things like that. You could take these texts and extract numeric and categorical features from them, and then use these as features in your DataFrame as part of that predictor variable set to help predict an outcome. This is a very common use case of using these methods.

The second is in the information retrieval space, where you can use these methods to do some very basic tasks. So, let's just pick one here: classification. You can use these methods to develop, say, language models of spam and non-spam email. And then use these to predict whether something is spam or not spam. This is a very broad range of tasks that go much beyond spam detection. You can use these methods to determine whether a wine review is positive or negative, and all sorts of things along those lines.

You can also use these methods to support the work of natural language processing and computational linguistics. Because text analytics, by focusing on text, is focusing on lots and lots of data about actual usage of language, which then forms the basis for the reverse engineering, if you will, or the development of models that generate text, which is really what linguistics is concerned with. And here I specify a few things here where it can help in the development of language models, including models of grammar, syntax, and pragmatics. We're going to go into more detail about how text analytics is related to NLP and computational linguistics.

Finally, there are other uses, but one of the main ones I want to talk about, which is near and dear to my heart, is the use of these methods to extract and interpret cultural and social patterns from texts. This is the broad field of cultural analytics, of which exploratory text analytics is a methodological component, if you will, or can play that role of being a component to cultural analytics. One of the most well-known examples of cultural analytics is a school of thought known as 'culturomics', which was probably the first success story in the field, where it was shown that by using analytical methods on a large corpus of texts, in particular Google Books, you could discover some very interesting patterns, cultural patterns in social patterns in these texts, like how long shame lasts and things like that. We're going to look at some of the work in this field to motivate this class. We're not going to go into the specific methodologies of culturomics in this class, but we're going to go into foundational work that you can use to go in that direction if you wish to.

## Relation to other fields

Let's situate exploratory text analytics within the wider range of fields that preceded and have constituted over time.

Among these related fields, let's look at the first here: Computational linguistics is a field that is very much related to text analytics. That's an older field, probably one of the first to bring computational resources to bear on the study of language. It actually precedes artificial intelligence and is basically concerned with using computers as a tool to study human language. This is essentially linguists using computers to study language as spoken by other human beings and in other populations.

There's also natural language processing, which is closely related to computational linguistics but is a really quite distinct project. Natural language processing is really a branch of artificial intelligence which, as many of you probably know, was founded in 1956 at Dartmouth University at a conference with very high aspirations for using computers to simulate and produce human brain-like programs. So, natural language processing is that sub-field of artificial intelligence which is focused on getting computers to both understand and produce human language, that is, to get computers to be agents of language just as humans are.

The third field is information retrieval, which is related to library science but is a library science of the information age, where the vast corpus of texts that you might find in a library have been digitized or are born digital. The question is, what can you do with a computer to improve access to and representation of all of this textual information? Classic tasks within this space include document summarization, that is to say, take a document and reduce it to its essential propositions or keywords, its most salient terms. The ability to retrieve a document based on a user's interests, finding the needle in the haystack. The ability to index collections effectively, to classify documents into well-organized collections based on their internal contents, that is to say, their language as well as their metadata.

Another field is text mining, which is of more recent vintage and is essentially a set of practices that were developed around the requirement to make sense of texts as they had been accumulating in databases over time. Unstructured data is everywhere: in corporate databases, government databases, social media databases. Text mining was really developed to try to extract value and information from those datasets and was allied pretty much with data mining to do so.

Another field that I'll point out, which you may not be familiar with, is digital humanities. Digital humanities have been around for quite a while, going back to 1949 when an Italian Jesuit priest, Father Busa, met with Watson from IBM on an airplane and decided to use IBM tools to create an index of the collected works of Thomas Aquinas, called the 'Index Thomisticus', which actually did get completed. This field has long been concerned with creating digital collections of primary textual sources. But beyond that, to explore new forms of scholarship, new ways of representing arguments and texts, and many other things - a very broad field.

Now, it has grown into digital humanities, but it's a very important field in this field of exploratory text analytics because it is focused almost exclusively on what I've called long-form text and has a lot to say about how we can think about textuality when represented digitally.

So here is a genealogy, a kind of rough influence diagram that I created to show the relationships between these different endeavors. I have highlighted in green the two most important sorts of antecedents to text mining, which is the most important antecedent to text analytics, and that is DM for data mining and NLP for natural language processing. So, let's look at those ancestral pathways first.

![](images/genealogy-of-eta.png)

Text mining, as a field, is extremely important and emerged in the 1990s when historically, we had accumulated lots and lots of databases in many organizations, primarily due to the combined revolution of the commercialization of the relational model, which begins in the early 1980s, perhaps in the late 1970s, with Oracle and other platforms, and the miniaturization of the computer, the development of the personal computer, and the improvement of computing overall. So, by the 1990s, many organizations were obviously using computers and accumulating data, and the need for text mining emerged. Text mining emerges, by the way, out of the prior practice of data mining, which is taking advantage of data in databases in general. Text mining emerges when it's clear that lots of data in those databases are textual in nature or what is called unstructured data. Data mining, I would argue, gets its influence from relational database theory and design, which in turn gets its influence from databases in general, which have developed quite far in the history of computing.

The other ancestral path is computational linguistics and artificial intelligence, which merge to form natural language processing, which becomes a practice in the 1960s and is hugely influential. So influential, in fact, that people use the term as a cover term for this entire field, but it actually is a subset of it, as we've mentioned. It really covers the work of trying to build computers or computer programs that can speak and understand language, like Alexa, Siri, or Google.

The other field is information retrieval, which we've talked about, which goes back quite far. You could even trace it before the 1950s, with Vannevar Bush's Memex, which we're not going to get into, but if you're interested, go look it up - Google 'Memex'.

And then, we have humanities computing, which I just mentioned. Probably the most significant development in humanities computing that relates to our project is the adoption and development of something called the Text Encoding Initiative in the 1980s and 1990s, which has to do with the development of both a syntax and methodology, as well as a model for understanding what a text is, so that it can be represented formally and then subject to various processes, both analytical and otherwise.

![](images/tm-and-nlp.png)

## Cultural analytics

Let's now look at the concept of cultural analytics as described by Lev Manovich in the reading for this week. The term "cultural analytics" was coined by Lev Manovich around 2004-2005. The term may have existed before he actually coined it back then, but its current usage, and the reason we use it today, has to do with his coinage of it and the work that he has done. I will point out that what Lev Manovich does specifically, as opposed to the broad framework that he describes in the paper, his specific work is very unique and cannot be considered exhaustive of the field of cultural analytics. It's a broad field, which we're going to talk a little more about.

We often refer to cultural analytics as CA. For short, it's the name of an established journal in digital humanities that often goes by the name of CA. The key idea that I want to introduce here is that the concept of cultural analytics, as introduced by Lev Manovich, bridges the digital humanities and a field called social computing, which is adjacent to and related to data science. Remember, digital humanities is one of the ancestors contributing fields to exploratory text analytics. Here, we look at how digital humanities is related to this other field of social computing, and we look at this important, bridging work that Manovich has done to connect the two. In many ways, it's the marriage of digital humanities and social computing, where social computing is broadly conceived to cover the computational approach to the study of culture, text, and other artifacts. It's that bridging work that is the space within which exploratory text analytics emerges and thrives.

He points out that the two fields, digital humanities and social computing, have contrasting methods and kinds of tasks. In particular, they have contrasting styles, or scales rather, in addition to styles. Digital humanists tend to work on smaller collections of historical texts, whereas those involved in social computing tend to work on much larger collections of contemporary texts. But they do have a common focus on textuality, and there is a lot of back and forth and possibility for knowledge transfer. In particular, in this course, we are interested in taking some of the methods from the social computing/data science space and migrating those toward digital humanities. But we're also interested in using digital humanities as our domain knowledge for what textuality is in general, and have that transfer back to the study of contemporary texts in social media, the way that social computing does. The idea here is to combine qualitative and quantitative methods to study culture. That's really what we're interested in.

We talked earlier about the use cases for exploratory text analytics in a previous video, and we talked about how these methods can be applied to a wide variety of standard applications in data science and machine learning. But in this course, the bias is toward the application of these methods to the study of culture. This work continues the work of cultural anthropology, which is my background. Although we're not going to do cultural anthropology per se in this course, it's important to understand that the study of culture proceeds from the field of cultural anthropology, and I'll get into that a bit more in a second.

So, the point here is that the cultural analytics idea that I'm introducing to you, and the essay by Manovich, are really there to provide a frame, a mental frame, for the content and scope of this course. Not so much for the methods, the particular methods described, and we'll get into that also. One way of thinking about this is that we're doing cultural analytics by means of exploratory text analytics. Culture analytics frames the "E" in ETA. Here's a little shout-out, a screenshot of Lev Manovich, who is a really interesting person, and I encourage you to go out on the web and look at his work. He's got some really interesting visualizations out there.

Okay, so a big part of this course depends on the concept of culture. This is not an introduction to anthropology course, but I do want to give you a broad feel for what we're talking about here because it's going to come up a lot when we do these different methods downstream. I'm going to make certain assumptions about the concept of culture that I'd like you to share. First of all, it's important to understand that there are two broad definitions of culture. This is a definitional split that goes back to the 18th-19th centuries.

The first definition is probably one that many of you are familiar with, or probably even mean by culture. This is what the critic Matthew Arnold, a British critic in 1869, called "The Best That has been Thought and Said." This is a very aristocratic view of culture. But even I would point out, in a populous, democratic society such as ours, we still think of culture this way. We still talk about high culture

and low culture. And we also talk about culture, that which is produced and explicitly defined by a sector of society to be consumed, like theatre is culture, art is culture, television is culture. We may call it popular culture, but we think about it as products, as it were, aesthetic products that are meant to be consumed. So even though we don't have the aesthetic definition, we're still pretty much in the same boat as Matthew Arnold, thinking about it as stuff that has been thought and said or presented and created that we want to consume for one reason or another.

But there's another definition, a scientific definition, that Manovich iterates here, and which is shared by many. Which is that basically, culture is everything that's created by everybody. It's a distinctive activity of the human species, and as we're finding out, other species as well, although it seems pretty clear that this is something that human beings have really optimized and something that human beings have really developed as part of adaptation. Cultural anthropology uses this definition. The scientific definition basically is that culture is that which is produced or expressed by everybody, at all times. One definition of anthropology is the study of all people at all times, while culture is equally broad and general as that. But I would point out that what we're talking about when we're talking about culture, in addition to things that within our complex society we tend to think of as the province of certain people who are creatives and who create culture, really it includes much more than artifacts. It includes beliefs and behaviors, and we're talking about beliefs and behaviors of everyone. And in reality, the belief is that the premise in cultural anthropology is that beliefs and behaviors of the kind that we're talking about are essential to the constitution of society itself, that you wouldn't have society without these things.

More specifically, within anthropology, we tend to think about beliefs as being divided between what's called ethos and worldview. Ethos is what you might call the moral framework or the moral sensibility of a society, and worldview you might call the sort of outlook, the world picture they may have, how they think the universe is ordered or constructed, whether it's chaotic or whether it's organized by fate, whether there is a single God, whether there are many gods, or whether there is no God, that's what we would put into worldview. And also, even though this sounds like it's non-modern, the manifestation of ethos in worldview is usually broken down into three areas of what we call symbol, myth, and ritual. And ritual, we can think of broadly as behavior that is motivated by an understanding of the world, and myth we can think of broadly as any kind of narrative or textual or even aesthetic representation of the world, and symbol, we can think of as the thing that's common to both myth and ritual and which expresses ethos and worldview. And these are the dense clusters of meaning that human beings are known to create, in which we're going to be interested in exploring within text to the extent that we can. And the bottom line of all this, literally here in this slide, is that these things that I'm describing â€“ beliefs, behaviors, ethos, worldview, symbol, myth, and ritual â€“ are evident in the form of patterns. And I'm going to give you some definitions of culture to help explain this, and these are classic definitions that are used by lots of anthropologists.

So, the first definition is one that goes back to the origin of what is called Social and Cultural Anthropology in England and France (it's called social anthropology; in the United States, we say cultural anthropology for roughly the same thing). And this is the definition from the English anthropologist E.B. Tylor, who defines culture as "the complex whole which includes," and he's got a lot of things in here, "knowledge, belief, art, morals, law, custom, and any other capabilities and habits acquired by humans as a member of society." So that is the original definition, and it stood the test of time, even though it's a bit of a laundry list, it really does capture the scope of things we're talking about.

A more opinionated, and theoretically driven definition, is a famous one developed by, or promoted by, Ruth Benedict in 1934, the American Anthropologist. And she defined culture as "a consistent pattern of thought and action." Now, her perspective was that cultures could be seen as personalities writ large. And she had a very interesting theory of cultures being classifiable in terms of, say, Apollonian and Dionysian, depending on whether they favored sobriety and purposeful action versus a kind of ecstatic, ritualistic life. And that actually is a distinction that goes back to Friedrich Nietzsche's Genealogy of Morals. And it's actually, quite frankly, not a compelling theory that has stood the test of time. However, the general idea that culture can be thought of as consisting of patterns, and that patterns of thought are related to social patterns and structures, remains valid.

## Machine learning methods

Let's look at the connection between these ideas and the methods of unsupervised learning and pattern discovery. Unsupervised methods are one of two main areas or branches of machine learning, but they're much more than just a set of techniques for clustering. They're also associated with the broader practice of knowledge discovery. I would argue that they're more accurately about pattern discovery, as the discovery of patterns is central to the study of culture as I've just defined it.

![](https://lh7-us.googleusercontent.com/p2gh06qDn5rQLt_uwnomZ6W5Pl7ZkDucntNKWC4ONvIoYQvnVUAo2SDz5ZDiHP7j-jDR-1ie4U9Wq6vGRf02njqmjzrx9IPr_cZqSSpiunmUWucv25NFeMGVQw_lqJpQEwRhkjNrx3_Rc4pSuisbRLIc_A=s2048)

So here's a diagram that shows the general, classical breakdown topology of machine learning. And I really like it because, unlike other similar topologies, it goes into greater detail about what the unsupervised side of things is about. We know about supervised learning. This is, you know, if your data are continuous numeric data, then you would use regression, this is your classic linear models, type stuff from statistics. Or if it's categorical, you would use classification methods, and down underneath that node of this graph is the whole world of methods that very often get equated with data science and machine learning. And here we have things like support vector machines, logistic regression, neural networks, and so forth. But let's put that to one side because we're focused more on unsupervised.

So unsupervised methods are those which, as I've argued and many have, and I'll show you a quote in a second, are focused on knowledge discovery and looking at patterns. And typically you often see only clustering listed under unsupervised methods, techniques for grouping together the points in some kind of vector space using either bottom-up or top-down methods, hierarchical or K-means clustering. But in addition to that, there are at least two other approaches out there. One is what we call dimension reduction, and we're going to go into that, the use of things like principal component analysis to make sense of unsupervised data. And there's also association rules, which I would think of in conjunction with graph-theoretic approaches to working with data. We're not going to go into those as much; we're pretty much going to focus on clustering and dimension reduction type methods.

![](https://lh7-us.googleusercontent.com/j8i9OR57L3IXxMpBFz17ylZE44qdsRNDEXVW_NrV4bt2CXjL75TqNS0p9BuUfsToSI0fuUmW7wYoT6VEjzC9wCT48Qe-N5dU8U9Ql--5dp4OMUONpQcu6lKoumS6W-xVRlnx46oV8J8BP4ZXdJbIQK1WhA=s2048)

Here's a quote from Kevin Murphy's book "Machine Learning: A Probabilistic Perspective", which I think is a great introductory text to machine learning. Here he defines unsupervised learning as having a goal to discover interesting structure in the data. And he also goes so far as to say that unsupervised learning is oftentimes called, or sometimes called, knowledge discovery. This is exactly what we're talking about in this course. This is why we're interested in unsupervised learning because we are definitely interested in looking at interesting structures.

So among the particular unsupervised methods we're going to use in this course, I'll list four.

1.  **Clustering** â€” K-means, hierarchical agglomerative, etc.
2.  **Topic Modeling** â€” PCA, LSI/A, NMF, LDA, etc.
3.  **Word Embedding** â€”Â  SGNS (word2vec), etc.
4.  **Sentiment Analysis** -- dictionary-based methods, etc.

This is a little bit of an apples-and-oranges mixture of categories here. Clustering, for example, is a specific area of unsupervised method that we just talked about, whereas sentiment analysis is a broad sort of charge to figure things out in a particular space, which may use unsupervised methods or other methods. But nevertheless, we're going to treat these all under the category of unsupervised methods.

So we have clustering. We're going to look at principal component analysis. We're going to look at K-means and hierarchical clustering.

We're going to look at topic modeling, pretty much going to focus on Latent Dirichlet Allocation, but Non-Negative Matrix Factorization (NMF) and Latent Semantic Indexing (LSI) we're going to glance at as well.

Word Embedding, which is a way of exploring the meanings of words as evidenced through a corpus of texts, primarily through Word2Vec, but there are other methods as well.

And then there's sentiment analysis, which is the attempt to discover the sentimental valence of a text and the variation of sentiment over time or across the structure of a text, using various methods. Here, I could just point to classical dictionary-based methods and something called VADER, which we're going to look at when we explore this. I'll point out that sentiment analysis is part of a broader field of content analysis that goes back pretty far, actually to the 1950s.

## The data science way

Before getting into that, though, I want to describe what I will call the data science way of doing things, and how we are going to approach the study of text analytics, as this will help guide our approach to the choices that we're going to use.

### The pipeline

First, data science generally consists of a pipeline approach to solving problems that begins with asking a question and ends with creating a product of some kind. I call this pipeline QDAP, for Question, Data, Analytics, and Product, and it consists of four broad phases. So, after the asking of a question, the posing of some sort of problem, something that frames what you're trying to do, there is the hard work of establishing the data. This means everything from discovering data, understanding its structure, to wrangling it and transforming it, doing ETL, putting it into a database, and basically setting the table so that you can perform the next part of the operation, which is to analyze the data using various sophisticated methods. This is the whole array of machine learning tools that we've talked about in a previous video, and something that I think distinguishes data science from other analytical fields is the focus on the product, the deliverable, which can be anything from a database-driven web application to an infographic to a document that is designed to translate the results of analysis and data wrangling into something actionable or that leads to some kind of activity or generally adds value.

### On the shoulders of giants

Another part of data science is that we really take to heart this notion that we stand on the shoulders of giants. We inherit a lot of methods. In this course, we do some coding from scratch, focusing on some core algorithms, but by and large, we are also building on a ton of work that's been done in the area of natural language processing and machine learning. We access this vast store of knowledge through libraries, program libraries that exist and are freely available, and make the work of doing exploratory text analytics a lot easier and indeed possible. Our goal here, pedagogically, is to have an understanding, but not mastery, of the deep logic that informs every algorithm. A true understanding of the depths of information retrieval, natural language processing, computational linguistics, and the other antecedents to text analytics that I described in a previous video, really requires pursuing a Ph.D. in this field. It's quite deep and quite rich. Here, we're learning enough about it to know what we can do with it, to ask the right questions, and to tweak the right parameters when we're working with these tools.

### In and through data

Another principle that's very important is that we think in and through data. We begin this course with an understanding of what I call "data as data." We don't begin necessarily from the mathematical point of view of thinking of an abstract conceptualization or formalization of a problem. We look at the data and try to understand how the algorithms that we use and the formulas that we use interact with that data. To the extent possible, when we're trying to explain the logic behind something, rather than visual representations of abstract spaces, we often look at the data set and try to understand it at that level. I'm a very strong believer in the use of data models, relational models in particular, as a means of exposing the logic of data, so that algorithmic processes that work on that data make much more sense.

### The study of text

Finally, as we already know, given the fact that this course is a course on the study of text, and with there being antecedents to the study of texts that we've already described, ranging from 19th-century text criticism to 21st-century digital humanities, we rely heavily on domain knowledge in informing our work.

### The process

So another way of thinking about the pipeline that we just described is in this way, as a much more granular series of steps through which data flows. After we have framed our question, our area of concern, and know what sort of data we're interested in, we begin with our sources. In the case of text analytics, we have lots of text sources in the world. They may exist in XML databases, corpora of Text Encoding Initiative-marked-up texts, texts from Project Gutenberg, a collection of newspaper articles either in some curated space or perhaps scraped from a website, reviews, wine reviews, book reviews. Sources come from many different formats. The first thing we do is take that data, which is extremely heterogeneous, and convert it into a common format that I call the text-mining corpus format. This is a very basic format that we're going to look at in our next module, which is used by lots of different text analytic tools, such as Scikit-learn, Gensim, and so forth, but we'll get into those.

### The ETA data model

After that, we take that data in its primary form, its most standard form, and convert it into what I call the ETA data model. It is a worked-out, normalized model of the data in the corpus format, but it's much more granular. We have, for example, a table that contains a vocabulary, a table that contains a list of tokens in the text, and so forth. Once we've done that, then we proceed to the step of using natural language processing tools to further annotate and add value to our data, to further augment the database. We do things like look at part of speech, look at stopwords (which I consider to be an NLP process), look at stemming, lemmatization, grammatical dependency, and so forth. From there, we move on to creating various kinds of vector space models, again something I'm going to talk about later, that sets the table, as it were, for us to do all kinds of interesting analytical stuff. There, we apply and generate machine learning models using various tools, both unsupervised and supervised. As I've mentioned before, the emphasis is on unsupervised stuff, but we're also going to look at a couple of supervised things as well. And then finally, we produce what I'm calling visuals, visualizations, and here what I mean are applications that drive what I referred to earlier as human-in-the-loop interpretive processes, where human beings actually look at a reduced representation of both the corpus and the analytical results on that corpus and are able to make interpretive judgments or see things they didn't see before in the text.

### A database mediated process

Another way of looking at this pipeline is as a database-mediated process. I'm a very big fan of leveraging the database as the hub of the pipeline. Here, I have a SQL database listed; it could be another kind of database, but I'm very much a fan of SQL because it allows for the development of principled data models. And here, we see that the pipeline involves a database at every step of the process. After we acquire data and put it into machine learning format, we put it into the database. We also convert it into the ETA format that I described, the model that we're going to go into. And then from there, we can do analytics, and the results of these analyses can then also be put back into the database and augment the appropriate tables therein. And finally, as we move to the product phase, the visualization phase, the communication phase, we have the database there as a foundation on which to build applications. For example, a database-driven web application could use this database and build something using a framework like Django or Flask to stick to the Python world.

## ETA tools

Let's talk about your exploratory text analytics toolkit, by which we mean the basic collection of software tools and choices that we'll be using to run the code that this course is built on.

### Python

Our technology choices, as I've just mentioned, or just sort of hinted at, are going to be framed around Python. There are a lot of great tools in R, and indeed in many other languages; you could even go back to Perl and do a lot of this stuff. The focus here will be on Python, however, in particular Python 3. We'll probably be using Python 3.6, but I'm not going to be using any of the newer functions that make a difference within the Python 3 series. We're going to be using Anaconda as our distribution. Anaconda is a group that packages Python along with a whole bunch of libraries and other tools, like integrated development environments, and so forth, and you download it to your system, and it sets up an environment for you. It sets up some tools. It comes with a nice user interface called Anaconda Navigator that you can use to pick and choose tools, environments, and libraries. It's very useful and pretty much of a standard in the field, something along the lines of RStudio for R, although that's comparing apples and oranges a little bit.

### Pandas

Next, we're going to be moving up the stack, as it were. We're going to be using Pandas a lot. Pandas is a DataFrame-oriented library that allows Python to do things that one might be used to doing in R. And we're going to be doing things in a kind of Pandas way in this class because we want to think about our text data as data, that is to say, in a data-centric perspective, where the set of data is the unit of analysis, and the way we process the data, we think of as a set of tabular and matrix operations, as opposed to, for example, thinking about the algorithmic processes that we perform as being for loops that work on individual lines of things. We'll get into that, and what that means specifically, but we're taking a data-centric view here. And, of course, this means using other libraries, such as NumPy (numeric Python), which is focused on linear algebra; SciPy, or scientific Python, which provides a lot of libraries, such as statistics library and so forth; and other libraries as well.